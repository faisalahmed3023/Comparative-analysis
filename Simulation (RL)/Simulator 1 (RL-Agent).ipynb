{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6e3Zj9ddwDv"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def check_folder_exist(fpath):\n",
        "    if os.path.exists(fpath):\n",
        "        print(\"dir \\\"\" + fpath + \"\\\" existed\")\n",
        "    else:\n",
        "        try:\n",
        "            os.mkdir(fpath)\n",
        "        except:\n",
        "            print(\"error when creating \\\"\" + fpath + \"\\\"\")\n",
        "\n",
        "def setup_path(fpath, is_dir = True):\n",
        "    dirs = [p for p in fpath.split(\"/\")]\n",
        "    curP = \"\"\n",
        "    dirs = dirs[:-1] if not is_dir else dirs\n",
        "    for p in dirs:\n",
        "        curP += p\n",
        "        check_folder_exist(curP)\n",
        "        curP += \"/\"\n",
        "\n",
        "\n",
        "# Data Related            \n",
        "def repeat_n_core(df, user_col_id, item_col_id, n_core, user_counts, item_counts):\n",
        "    '''\n",
        "    Iterative n_core filter\n",
        "\n",
        "    @input:\n",
        "    - df: [UserID, ItemID, ...]\n",
        "    - n_core: number of core\n",
        "    - user_counts: {uid: frequency}\n",
        "    - item_counts: {iid: frequency}\n",
        "    '''\n",
        "    print(\"N-core is set to [5,100]\")\n",
        "    n_core = min(max(n_core, 5),100) # 5 <= n_core <= 100\n",
        "    print(\"Filtering \" + str(n_core) + \"-core data\")\n",
        "    iteration = 0\n",
        "    lastNRemove = len(df)  # the number of removed record\n",
        "    proposedData = df.values\n",
        "    originalSize = len(df)\n",
        "\n",
        "    # each iteration, count number of records that need to delete\n",
        "    while lastNRemove != 0:\n",
        "        iteration += 1\n",
        "        print(\"Iteration \" + str(iteration))\n",
        "        changeNum = 0\n",
        "        newData = []\n",
        "        for row in tqdm(proposedData):\n",
        "            user, item = row[user_col_id], row[item_col_id]\n",
        "            if user_counts[user] < n_core or item_counts[item] < n_core:\n",
        "                user_counts[user] -= 1\n",
        "                item_counts[item] -= 1\n",
        "                changeNum += 1\n",
        "            else:\n",
        "                newData.append(row)\n",
        "        proposedData = newData\n",
        "        print(\"Number of removed record: \" + str(changeNum))\n",
        "        if changeNum > lastNRemove + 10000:\n",
        "            print(\"Not converging, will use original data\")\n",
        "            break\n",
        "        else:\n",
        "            lastNRemove = changeNum\n",
        "    print(\"Size change: \" + str(originalSize) + \" --> \" + str(len(proposedData)))\n",
        "    return pd.DataFrame(proposedData, columns=df.columns)\n",
        "\n",
        "def run_multicore(df, user_key = \"user_id\", item_key = \"item_id\", n_core = 10, auto_core = False, filter_rate = 0.2):\n",
        "    '''\n",
        "    @input:\n",
        "    - df: pd.DataFrame, col:[UserID,ItemID,...]\n",
        "    - n_core: number of core\n",
        "    - auto_core: automatically find n_core, set to True will ignore n_core\n",
        "    - filter_rate: proportion of removal for user/item, require auto_core = True\n",
        "    '''\n",
        "    print(f\"Filter {n_core if not auto_core else 'auto'}-core data.\")\n",
        "    uCounts = df[user_key].value_counts().to_dict() # {user_id: count}\n",
        "    iCounts = df[item_key].value_counts().to_dict() # {item_id: count}\n",
        "\n",
        "    # automatically find n_core based on filter rate\n",
        "    if auto_core:\n",
        "        print(\"Automatically find n_core that filter \" + str(100*filter_rate) + \"% of user/item\")\n",
        "\n",
        "        nCoreCounts = dict() # {n_core: [#user, #item]}\n",
        "        for v,c in iCounts.items():\n",
        "            if c not in nCoreCounts:\n",
        "                nCoreCounts[c] = [0,1]\n",
        "            else:\n",
        "                nCoreCounts[c][1] += 1\n",
        "        for u,c in uCounts.items():\n",
        "            if c not in nCoreCounts:\n",
        "                nCoreCounts[c] = [1,0]\n",
        "            else:\n",
        "                nCoreCounts[c][0] += 1\n",
        "\n",
        "        # find n_core for: filtered data < filter_rate * length(data)\n",
        "        userToRemove = 0 # number of user records to remove\n",
        "        itemToRemove = 0 # number of item records to remove\n",
        "        for c,counts in sorted(nCoreCounts.items()):\n",
        "            userToRemove += counts[0] * c # #user * #core\n",
        "            itemToRemove += counts[1] * c # #item * #core\n",
        "            if userToRemove > filter_rate * len(df) or itemToRemove > filter_rate * len(df):\n",
        "                n_core = c\n",
        "                print(\"Autocore = \" + str(n_core))\n",
        "                break\n",
        "    else:\n",
        "        print(\"n_core = \" + str(n_core))\n",
        "\n",
        "    return repeat_n_core(df, 0, 1, n_core, uCounts, iCounts)\n",
        "\n",
        "def padding_and_clip(sequence, max_len, padding_direction = 'left'):\n",
        "    if len(sequence) < max_len:\n",
        "        sequence = [0] * (max_len - len(sequence)) + sequence if padding_direction == 'left' else sequence + [0] * (max_len - len(sequence))\n",
        "    sequence = sequence[-max_len:] if padding_direction == 'left' else sequence[:max_len]\n",
        "    return sequence\n",
        "\n",
        "def get_onehot_vocab(meta_df, features):\n",
        "    print('build vocab for onehot features')\n",
        "    vocab = {}\n",
        "    for f in tqdm(features):\n",
        "        value_list = list(meta_df[f].unique())\n",
        "        vocab[f] = {}\n",
        "        for i,v in enumerate(value_list):\n",
        "            onehot_vec = np.zeros(len(value_list))\n",
        "            onehot_vec[i] = 1\n",
        "            vocab[f][v] = onehot_vec\n",
        "    return vocab\n",
        "\n",
        "def get_multihot_vocab(meta_df, features, sep = ','):\n",
        "    print('build vocab for multihot features:')\n",
        "    vocab = {}\n",
        "    for f in features:\n",
        "        print(f'\\t{f}')\n",
        "        ID_freq = {}\n",
        "        for row in tqdm(meta_df[f]):\n",
        "            IDs = str(row).split(sep)\n",
        "            for ID in IDs:\n",
        "                if ID not in ID_freq:\n",
        "                    ID_freq[ID] = 1\n",
        "                else:\n",
        "                    ID_freq[ID] += 1\n",
        "        v_list = list(ID_freq.keys())\n",
        "        vocab[f] = {}\n",
        "        for i,v in enumerate(v_list):\n",
        "            onehot_vec = np.zeros(len(v_list))\n",
        "            onehot_vec[i] = 1\n",
        "            vocab[f][v] = onehot_vec\n",
        "    return vocab\n",
        "\n",
        "def get_ID_vocab(meta_df, features):\n",
        "    print('build vocab for encoded ID features')\n",
        "    vocab = {}\n",
        "    for f in tqdm(features):\n",
        "        value_list = list(meta_df[f].unique())\n",
        "        vocab[f] = {v:i+1 for i,v in enumerate(value_list)}\n",
        "    return vocab\n",
        "\n",
        "def get_multiID_vocab(meta_df, features, sep = ','):\n",
        "    print('build vocab for encoded ID features')\n",
        "    vocab = {}\n",
        "    for f in features:\n",
        "        print(f'\\t{f}:')\n",
        "        ID_freq = {}\n",
        "        for row in tqdm(meta_df[f]):\n",
        "            IDs = str(row).split(sep)\n",
        "            for ID in IDs:\n",
        "                if ID not in ID_freq:\n",
        "                    ID_freq[ID] = 1\n",
        "                else:\n",
        "                    ID_freq[ID] += 1\n",
        "        v_list = list(ID_freq.keys())\n",
        "        vocab[f] = {v:i+1 for i,v in enumerate(v_list)}\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def show_batch(batch):\n",
        "    for k, batch in batch.items():\n",
        "        if torch.is_tensor(batch):\n",
        "            print(f\"{k}: size {batch.shape}, \\n\\tfirst 5 {batch[:5]}\")\n",
        "        else:\n",
        "            print(f\"{k}: {batch}\")\n",
        "\n",
        "\n",
        "def wrap_batch(batch, device):\n",
        "    '''\n",
        "    Build feed_dict from batch data and move data to device\n",
        "    '''\n",
        "    for k,val in batch.items():\n",
        "        if type(val).__module__ == np.__name__:\n",
        "            batch[k] = torch.from_numpy(val)\n",
        "        elif torch.is_tensor(val):\n",
        "            batch[k] = val\n",
        "        elif type(val) is list:\n",
        "            batch[k] = torch.tensor(val)\n",
        "        else:\n",
        "            continue\n",
        "        if batch[k].type() == \"torch.DoubleTensor\":\n",
        "            batch[k] = batch[k].float()\n",
        "        batch[k] = batch[k].to(device)\n",
        "    return batch\n",
        "\n",
        "\n",
        "\n",
        "# Model Related \n",
        "\n",
        "def init_weights(m):\n",
        "    if 'Linear' in str(type(m)):\n",
        "#         nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "        nn.init.xavier_normal_(m.weight, gain=1.)\n",
        "        if m.bias is not None:\n",
        "            nn.init.normal_(m.bias, mean=0.0, std=0.01)\n",
        "    elif 'Embedding' in str(type(m)):\n",
        "#         nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "        nn.init.xavier_normal_(m.weight, gain=1.0)\n",
        "        print(\"embedding: \" + str(m.weight.data))\n",
        "        with torch.no_grad():\n",
        "            m.weight[m.padding_idx].fill_(0.)\n",
        "    elif 'ModuleDict' in str(type(m)):\n",
        "        for param in module.values():\n",
        "            nn.init.xavier_normal_(param.weight, gain=1.)\n",
        "            with torch.no_grad():\n",
        "                param.weight[param.padding_idx].fill_(0.)\n",
        "\n",
        "\n",
        "def get_regularization(*modules):\n",
        "    reg = 0\n",
        "    for m in modules:\n",
        "        for p in m.parameters():\n",
        "            reg = torch.mean(p * p) + reg\n",
        "    return reg\n",
        "\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - tau) + param.data * tau\n",
        "        )\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "def sample_categorical_action(action_prob, candidate_ids, slate_size, with_replacement = True,\n",
        "                              batch_wise = False, return_idx = False):\n",
        "    '''\n",
        "    @input:\n",
        "    - action_prob: (B, L)\n",
        "    - candidate_ids: (B, L) or (1, L)\n",
        "    - slate_size: K\n",
        "    - with_replacement: sample with replacement\n",
        "    - batch_wise: do batch wise candidate selection\n",
        "    '''\n",
        "    if with_replacement:\n",
        "        indices = Categorical(action_prob).sample(sample_shape = (slate_size,))\n",
        "        indices = torch.transpose(indices, 0, 1)\n",
        "    else:\n",
        "        indices = torch.cat([torch.multinomial(prob, slate_size, replacement = False).view(1,-1) \\\n",
        "                             for prob in action_prob], dim = 0)\n",
        "    action = torch.gather(candidate_ids,1,indices) if batch_wise else candidate_ids[indices]\n",
        "    if return_idx:\n",
        "        return action.detach(), indices.detach()\n",
        "    else:\n",
        "        return action.detach()\n",
        "\n",
        "\n",
        "\n",
        "# Learning               \n",
        "\n",
        "class LinearScheduler(object):\n",
        "    '''\n",
        "    Code used in DQN: https://github.com/dxyang/DQN_pytorch/blob/master/utils/schedules.py\n",
        "    '''\n",
        "\n",
        "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
        "        self.schedule_timesteps = schedule_timesteps\n",
        "        self.final_p            = final_p\n",
        "        self.initial_p          = initial_p\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        fraction  = min(float(t) / self.schedule_timesteps, 1.0)\n",
        "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
        "\n",
        "\n",
        "class SinScheduler(object):\n",
        "    '''\n",
        "    Code used in DQN: https://github.com/dxyang/DQN_pytorch/blob/master/utils/schedules.py\n",
        "    '''\n",
        "\n",
        "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
        "        self.schedule_timesteps = schedule_timesteps\n",
        "        self.final_p            = final_p\n",
        "        self.initial_p          = initial_p\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        fraction  = np.sin(min(float(t) / self.schedule_timesteps, 1.0) * np.pi * 0.5)\n",
        "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQCaUWnXXJBI",
        "outputId": "d4481da4-c35c-4d79-8b9b-151615a48aae"
      },
      "outputs": [],
      "source": [
        "music_rating = pd.read_table('movie_rating_kuaisim.csv', sep = ',')\n",
        "music_profile = pd.read_table('movie_profiles_kuaisim.csv', sep = ',')\n",
        "user_profile = pd.read_table('profiles.csv', sep = ',')\n",
        "items = list(music_profile['item_id'].unique())\n",
        "check_items = list(music_rating['item_id'].unique())\n",
        "check_items = pd.Series(check_items).sort_values().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3hkx-iReXOh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def worker_init_func(worker_id):\n",
        "    worker_info = data.get_worker_info()\n",
        "    worker_info.dataset.worker_id = worker_id\n",
        "\n",
        "#############################################################################\n",
        "#                              Dataset Class                                #\n",
        "#############################################################################\n",
        "\n",
        "class BaseReader(Dataset):\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_data_args(parser):\n",
        "        parser.add_argument('--train_file', type=str, required=True,\n",
        "                            help='train data file_path')\n",
        "        parser.add_argument('--val_file', type=str, default='',\n",
        "                            help='val data file_path')\n",
        "        parser.add_argument('--test_file', type=str, default='',\n",
        "                            help='test data file_path')\n",
        "        parser.add_argument('--n_worker', type=int, default=4,\n",
        "                            help='number of worker for dataset loader')\n",
        "        parser.add_argument('--data_separator', type=str, default='\\t',\n",
        "                            help='separator of csv file')\n",
        "        return parser\n",
        "\n",
        "    def log(self):\n",
        "        print(\"Reader params:\")\n",
        "        print(f\"\\tn_worker: {self.n_worker}\")\n",
        "        for k,v in self.get_statistics().items():\n",
        "            print(f\"\\t{k}: {v}\")\n",
        "\n",
        "    def __init__(self, args):\n",
        "        '''\n",
        "        - phase: one of [\"train\", \"val\", \"test\"]\n",
        "        - data: {phase: pd.DataFrame}\n",
        "        - data_fields: {field_name: (field_type, field_var)}\n",
        "        - data_vocab: {field_name: {value: index}}\n",
        "        '''\n",
        "        self.phase = \"train\"\n",
        "        self.n_worker = args.n_worker\n",
        "        self._read_data(args)\n",
        "\n",
        "    def _read_data(self, args):\n",
        "        self.data = dict()\n",
        "        print(f\"Loading data files\", end = '\\r')\n",
        "        self.data['train'] = pd.read_table(args.train_file, sep = args.data_separator)\n",
        "        self.data['val'] = pd.read_table(args.val_file, sep = args.data_separator) \\\n",
        "                                if len(args.val_file) > 0 else self.data['train']\n",
        "        self.data['test'] = pd.read_table(args.test_file, sep = args.data_separator) \\\n",
        "                                if len(args.test_file) > 0 else self.data['val']\n",
        "\n",
        "    def get_statistics(self):\n",
        "        return {'length': len(self)}\n",
        "\n",
        "    def set_phase(self, phase):\n",
        "        assert phase in [\"train\", \"val\", \"test\"]\n",
        "        self.phase = phase\n",
        "\n",
        "    def get_train_dataset(self):\n",
        "        self.set_phase(\"train\")\n",
        "        return self\n",
        "\n",
        "    def get_eval_dataset(self, phase = 'val'):\n",
        "        self.set_phase(phase)\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[self.phase])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZHPAyDne1oo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "class KRMBSeqReader(BaseReader):\n",
        "    '''\n",
        "    KuaiRand Multi-Behavior Data Reader\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_data_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - user_meta_file\n",
        "        - item_meta_file\n",
        "        - max_hist_seq_len\n",
        "        - val_holdout_per_user\n",
        "        - test_holdout_per_user\n",
        "        - meta_file_sep\n",
        "        - from BaseReader:\n",
        "            - train_file\n",
        "            - val_file\n",
        "            - test_file\n",
        "            - n_worker\n",
        "        '''\n",
        "        parser = BaseReader.parse_data_args(parser)\n",
        "        parser.add_argument('--user_meta_file', type=str, required=True,\n",
        "                            help='user raw feature file_path')\n",
        "        parser.add_argument('--item_meta_file', type=str, required=True,\n",
        "                            help='item raw feature file_path')\n",
        "        parser.add_argument('--max_hist_seq_len', type=int, default=100,\n",
        "                            help='maximum history length in the sample')\n",
        "        parser.add_argument('--val_holdout_per_user', type=int, default=5,\n",
        "                            help='number of holdout records for val set')\n",
        "        parser.add_argument('--test_holdout_per_user', type=int, default=5,\n",
        "                            help='number of holdout records for test set')\n",
        "        parser.add_argument('--meta_file_sep', type=str, default=',',\n",
        "                            help='separater of user/item meta csv file')\n",
        "        return parser\n",
        "\n",
        "    def log(self):\n",
        "        super().log()\n",
        "        print(f\"\\tval_holdout_per_user: {self.val_holdout_per_user}\")\n",
        "        print(f\"\\ttest_holdout_per_user: {self.test_holdout_per_user}\")\n",
        "\n",
        "    def __init__(self, args):\n",
        "        '''\n",
        "        - max_hist_seq_len\n",
        "        - val_holdout_per_user\n",
        "        - test_holdout_per_user\n",
        "        - from BaseReader:\n",
        "            - phase\n",
        "            - n_worker\n",
        "        '''\n",
        "        print(\"initiate KuaiRandMultiBehaior sequence reader\")\n",
        "        self.max_hist_seq_len = args.max_hist_seq_len\n",
        "        self.val_holdout_per_user = args.val_holdout_per_user\n",
        "        self.test_holdout_per_user = args.test_holdout_per_user\n",
        "        super().__init__(args)\n",
        "\n",
        "    def _read_data(self, args):\n",
        "        '''\n",
        "        - log_data: pd.DataFrame\n",
        "        - data: {'train': [row_id], 'val': [row_id], 'test': [row_id]}\n",
        "        - users: [user_id]\n",
        "        - user_id_vocab: {user_id: encoded_user_id}\n",
        "        - user_meta: {user_id: {feature_name: feature_value}}\n",
        "        - user_vocab: {feature_name: {feature_value: one-hot vector}}\n",
        "        - selected_user_features\n",
        "        - items: [item_id]\n",
        "        - item_id_vocab: {item_id: encoded_item_id}\n",
        "        - item_meta: {item_id: {feature_name: feature_value}}\n",
        "        - item_vocab: {feature_name: {feature_value: one-hot vector}}\n",
        "        - selected_item_features: [feature_name]\n",
        "        - padding_item_meta: {feature_name: 0}\n",
        "        - user_history: {uid: [row_id]}\n",
        "        - response_list: [response_type]\n",
        "        - padding_response: {response_type: 0}\n",
        "        -\n",
        "        '''\n",
        "\n",
        "        # read data_file\n",
        "        print(f\"Loading data files\")\n",
        "        self.log_data = pd.read_table(args.train_file, sep = args.data_separator)\n",
        "\n",
        "        print(\"Load item meta data\")\n",
        "        item_meta_file = pd.read_csv(args.item_meta_file, sep = args.meta_file_sep)\n",
        "        self.item_meta = item_meta_file.set_index('item_id').to_dict('index')\n",
        "        print(\"Load user meta data\")\n",
        "        user_meta_file = pd.read_csv(args.user_meta_file, sep = args.meta_file_sep)\n",
        "        self.user_meta = user_meta_file.set_index('user_id').to_dict('index')\n",
        "\n",
        "        # user list, item list, user history\n",
        "        self.users = list(self.log_data['user_id'].unique())\n",
        "        self.itemns = list(self.log_data['item_id'].unique()) #changed this line for not sort items\n",
        "        self.items = pd.Series(self.itemns).sort_values().tolist() #change this line for sort items\n",
        "        self.user_history = {uid: list(self.log_data[self.log_data['user_id'] == uid].index) for uid in self.users}\n",
        "\n",
        "        # id reindex\n",
        "        self.user_id_vocab = {uid: i+1 for i,uid in enumerate(self.users)}\n",
        "        self.item_id_vocab = {iid: i+1 for i,iid in enumerate(self.items)}\n",
        "\n",
        "        # selected meta features\n",
        "        self.selected_item_features = ['title',\t'genres']\n",
        "        self.selected_user_features = ['name',\t'gender',\t'age']\n",
        "\n",
        "        # meta feature vocabulary, {feature_name: {feature_value: one-hot/multi-hot vector}}\n",
        "        self.user_vocab = get_onehot_vocab(user_meta_file, self.selected_user_features)\n",
        "        self.item_vocab = get_onehot_vocab(item_meta_file, self.selected_item_features[:-1])\n",
        "        self.item_vocab.update(get_multihot_vocab(item_meta_file, ['genres']))\n",
        "        self.padding_item_meta = {f: np.zeros_like(list(v_dict.values())[0]) \\\n",
        "                                  for f, v_dict in self.item_vocab.items()}\n",
        "\n",
        "        # response meta\n",
        "        self.response_list = ['is_like','is_follow','is_comment','is_forward','is_hate']\n",
        "        self.response_dim = len(self.response_list)\n",
        "        self.padding_response = {resp: 0. for i,resp in enumerate(self.response_list)}\n",
        "        self.response_neg_sample_rate = self.get_response_weights()\n",
        "\n",
        "        # {'train': [row_id], 'val': [row_id], 'test': [row_id]}\n",
        "        self.data = self._sequence_holdout(args)\n",
        "\n",
        "    def _sequence_holdout(self, args):\n",
        "        '''\n",
        "        Holdout validation and test set from log_data\n",
        "        '''\n",
        "        print(f\"sequence holdout for users (-1, {args.val_holdout_per_user}, {args.test_holdout_per_user})\")\n",
        "        if args.val_holdout_per_user == 0 and args.test_holdout_per_user == 0:\n",
        "            return {\"train\": self.log_data.index, \"val\": [], \"test\": []}\n",
        "        data = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        for u in tqdm(self.users):\n",
        "            sub_df = self.log_data[self.log_data['user_id'] == u]\n",
        "            n_train = len(sub_df) - args.val_holdout_per_user - args.test_holdout_per_user\n",
        "            if n_train < 0.6 * len(sub_df):\n",
        "                continue\n",
        "            data['train'].append(list(sub_df.index[:n_train]))\n",
        "            data['val'].append(list(sub_df.index[n_train:n_train+args.val_holdout_per_user]))\n",
        "            data['test'].append(list(sub_df.index[-args.test_holdout_per_user:]))\n",
        "        for k,v in data.items():\n",
        "            data[k] = np.concatenate(v)\n",
        "        return data\n",
        "\n",
        "    def get_response_weights(self):\n",
        "        ratio = {}\n",
        "        for f in self.response_list:\n",
        "            counts = self.log_data[f].value_counts()\n",
        "            ratio[f] = float(counts[1]) / counts[0]\n",
        "        ratio['is_hate'] *= -1\n",
        "        return ratio\n",
        "\n",
        "\n",
        "    ###########################\n",
        "    #        Iterator         #\n",
        "    ###########################\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        sample getter\n",
        "\n",
        "        train batch after collate:\n",
        "        {\n",
        "            'user_id': (B,)\n",
        "            'item_id': (B,)\n",
        "            'is_click', 'long_view', ...: (B,)\n",
        "            'uf_{feature}': (B,F_dim(feature)), user features\n",
        "            'if_{feature}': (B,F_dim(feature)), item features\n",
        "            'history': (B,max_H)\n",
        "            'history_length': (B,)\n",
        "            'history_if_{feature}': (B, max_H, F_dim(feature))\n",
        "            'history_{response}': (B, max_H)\n",
        "            'loss_weight': (B, n_response)\n",
        "        }\n",
        "        '''\n",
        "        row_id = self.data[self.phase][idx]\n",
        "        row = self.log_data.iloc[row_id]\n",
        "\n",
        "        user_id = row['user_id'] # raw user ID\n",
        "        item_id = row['item_id'] # raw item ID\n",
        "\n",
        "        # user, item, responses\n",
        "        record = {\n",
        "            'user_id': self.user_id_vocab[row['user_id']], # encoded user ID\n",
        "            'item_id': self.item_id_vocab[row['item_id']], # encoded item ID\n",
        "        }\n",
        "        for _,f in enumerate(self.response_list):\n",
        "            record[f] = row[f]\n",
        "        loss_weight = np.array([1. if record[f] == 1 \\\n",
        "                                    else -self.response_neg_sample_rate[f] if f == 'is_hate' \\\n",
        "                                        else self.response_neg_sample_rate[f]\\\n",
        "                                for i,f in enumerate(self.response_list)])\n",
        "        record[\"loss_weight\"] = loss_weight\n",
        "\n",
        "        # meta features\n",
        "        user_meta = self.get_user_meta_data(user_id)\n",
        "        record.update(user_meta)\n",
        "        item_meta = self.get_item_meta_data(item_id)\n",
        "        record.update(item_meta)\n",
        "\n",
        "        # history features (max_H,)\n",
        "        H_rowIDs = [rid for rid in self.user_history[user_id] if rid < row_id][-self.max_hist_seq_len:]\n",
        "        history, hist_length, hist_meta, hist_response = self.get_user_history(H_rowIDs)\n",
        "        record['history'] = np.array(history)\n",
        "        record['history_length'] = hist_length\n",
        "        # hist_meta keys are already like 'history_if_title', 'history_if_genres'\n",
        "        record.update(hist_meta)\n",
        "        # hist_response keys are already like 'history_is_like', ...\n",
        "        record.update(hist_response)\n",
        "        # for f,v in hist_meta.items():\n",
        "        #     record[f'history_{f}'] = v\n",
        "        # for f,v in hist_response.items():\n",
        "        #     record[f'history_{f}'] = v\n",
        "\n",
        "        return record\n",
        "\n",
        "    def get_user_meta_data(self, user_id):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_id: raw user ID\n",
        "        @output:\n",
        "        - user_meta_record: {'uf_{feature_name}: one-hot vector'}\n",
        "        '''\n",
        "        user_feature_dict = self.user_meta[user_id]\n",
        "        user_meta_record = {f'uf_{f}': self.user_vocab[f][user_feature_dict[f]]\\\n",
        "                            for f in self.selected_user_features}\n",
        "        return user_meta_record\n",
        "\n",
        "    def get_item_meta_data(self, item_id):\n",
        "        item_feature_dict = self.item_meta[item_id]\n",
        "\n",
        "        # Exclude genres (multi-hot handled separately)\n",
        "        onehot_feats = [f for f in self.selected_item_features if f != \"genres\" and f in self.item_vocab]\n",
        "\n",
        "        item_meta_record = {\n",
        "        f'if_{f}': self.item_vocab[f][item_feature_dict[f]]\n",
        "        for f in onehot_feats\n",
        "        }\n",
        "\n",
        "        # Multi-hot encoding for genres\n",
        "        genres_val = item_feature_dict.get(\"genres\", \"\")\n",
        "        tags = [t.strip() for t in genres_val.split(\",\") if t.strip()]\n",
        "        if tags:\n",
        "           vecs = [self.item_vocab[\"genres\"][t] for t in tags if t in self.item_vocab[\"genres\"]]\n",
        "           if vecs:\n",
        "              item_meta_record[\"if_genres\"] = np.sum(vecs, axis=0)\n",
        "           else:\n",
        "              item_meta_record[\"if_genres\"] = np.zeros_like(next(iter(self.item_vocab[\"genres\"].values())))\n",
        "        else:\n",
        "           item_meta_record[\"if_genres\"] = np.zeros_like(next(iter(self.item_vocab[\"genres\"].values())))\n",
        "\n",
        "        return item_meta_record\n",
        "\n",
        "\n",
        "    def get_user_history(self, H_rowIDs):\n",
        "        L = len(H_rowIDs)\n",
        "        if L == 0:\n",
        "           history = [0] * self.max_hist_seq_len\n",
        "\n",
        "           # Build history feature arrays only for features we actually embed:\n",
        "           # that's every key in self.item_vocab (e.g., 'title' one-hot and 'genres' multi-hot)\n",
        "           hist_meta = {}\n",
        "           for f, v_dict in self.item_vocab.items():\n",
        "              pad_vec = np.zeros_like(next(iter(v_dict.values())))\n",
        "              hist_meta[f'history_if_{f}'] = np.tile(pad_vec, (self.max_hist_seq_len, 1))\n",
        "\n",
        "           history_response = {\n",
        "            f'history_{resp}': np.zeros(self.max_hist_seq_len, dtype=float)\n",
        "            for resp in self.response_list\n",
        "           }\n",
        "           return history, 0, hist_meta, history_response\n",
        "\n",
        "        # L > 0\n",
        "        H = self.log_data.iloc[H_rowIDs]\n",
        "        item_ids = [self.item_id_vocab[iid] for iid in H['item_id']]\n",
        "        history = padding_and_clip(item_ids, self.max_hist_seq_len)\n",
        "\n",
        "        # build per-item meta then pad to (max_H, feat_dim)\n",
        "        meta_list = [self.get_item_meta_data(iid) for iid in H['item_id']]\n",
        "\n",
        "        hist_meta = {}\n",
        "        for f in self.item_vocab.keys():  # e.g., 'title', 'genres'\n",
        "            feat_stack = np.stack([m[f'if_{f}'] for m in meta_list], axis=0)  # (L, feat_dim)\n",
        "            pad_rows = self.max_hist_seq_len - L\n",
        "            if pad_rows > 0:\n",
        "               pad_vec = np.zeros_like(feat_stack[0])\n",
        "               pad = np.tile(pad_vec, (pad_rows, 1))                         # (pad_rows, feat_dim)\n",
        "               feat_stack = np.concatenate([pad, feat_stack], axis=0)        # (max_H, feat_dim)\n",
        "            hist_meta[f'history_if_{f}'] = feat_stack\n",
        "\n",
        "        history_response = {}\n",
        "        for resp in self.response_list:\n",
        "           arr = np.array(H[resp], dtype=float)            # (L,)\n",
        "           pad_rows = self.max_hist_seq_len - L\n",
        "           if pad_rows > 0:\n",
        "              pad = np.zeros(pad_rows, dtype=float)\n",
        "              arr = np.concatenate([pad, arr], axis=0)    # (max_H,)\n",
        "           history_response[f'history_{resp}'] = arr\n",
        "\n",
        "        return history, L, hist_meta, history_response\n",
        "\n",
        "\n",
        "    def get_statistics(self):\n",
        "        '''\n",
        "        - n_user\n",
        "        - n_item\n",
        "        - s_parsity\n",
        "        - from BaseReader:\n",
        "            - length\n",
        "            - fields\n",
        "        '''\n",
        "        stats = {}\n",
        "        stats[\"raw_data_size\"] = len(self.log_data)\n",
        "        stats[\"data_size\"] = [len(self.data['train']), len(self.data['val']), len(self.data['test'])]\n",
        "        stats[\"n_user\"] = len(self.users)\n",
        "        stats[\"n_item\"] = len(self.items)\n",
        "        stats[\"max_seq_len\"] = self.max_hist_seq_len\n",
        "        stats[\"user_features\"] = self.selected_user_features\n",
        "        stats[\"user_feature_dims\"] = {f: len(list(v_dict.values())[0]) for f, v_dict in self.user_vocab.items()}\n",
        "        stats[\"item_features\"] = self.selected_item_features\n",
        "        stats[\"item_feature_dims\"] = {f: len(list(v_dict.values())[0]) for f, v_dict in self.item_vocab.items()}\n",
        "        stats[\"feedback_type\"] = self.response_list\n",
        "        stats[\"feedback_size\"] = self.response_dim\n",
        "        stats[\"feedback_negative_sample_rate\"] = self.response_neg_sample_rate\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny32h1WkfKR7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "\n",
        "    #############################\n",
        "    #     Optional Overwrite    #\n",
        "    #############################\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        parser.add_argument('--model_path', type=str, default='',\n",
        "                            help='Model save path.')\n",
        "        parser.add_argument('--loss', type=str, default='bce',\n",
        "                            help='loss type')\n",
        "        parser.add_argument('--l2_coef', type=float, default=0.,\n",
        "                            help='coefficient of regularization term')\n",
        "        return parser\n",
        "\n",
        "    def log(self):\n",
        "        print(\"Model params\")\n",
        "        print(\"\\tmodel_path = \" + str(self.model_path))\n",
        "        print(\"\\tloss_type = \" + str(self.loss_type))\n",
        "        print(\"\\tl2_coef = \" + str(self.l2_coef))\n",
        "        print(\"\\tdevice = \" + str(self.device))\n",
        "\n",
        "    def __init__(self, *input_args):\n",
        "        args, reader_stats, device = input_args\n",
        "        # super(BaseModel, self).__init__()\n",
        "        nn.Module.__init__(self)\n",
        "        self.display_name = \"BaseModel\"\n",
        "        self.reader_stats = reader_stats\n",
        "        self.model_path = args.model_path\n",
        "        self.loss_type = args.loss\n",
        "        self.l2_coef = args.l2_coef\n",
        "        self.no_reg = 0. < args.l2_coef < 1.\n",
        "        self.device = device\n",
        "\n",
        "        self._define_params(args, reader_stats)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def get_regularization(self, *modules):\n",
        "        return get_regularization(*modules)\n",
        "\n",
        "    def show_params(self):\n",
        "        print(f\"All parameters for {self.display_name}========================\")\n",
        "        idx = 0\n",
        "        all_params = []\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                # try:\n",
        "                param_shape = list(param.size())\n",
        "                print(\" var {:3}: {:15} {}\".format(idx, str(param_shape), name))\n",
        "                num_params = 1\n",
        "                if (len(param_shape) > 1):\n",
        "                    for p in param_shape:\n",
        "                        if (p > 0):\n",
        "                            num_params = num_params * int(p)\n",
        "                    all_params.append(num_params)\n",
        "                elif len(param_shape) == 1:\n",
        "                    all_params.append(param_shape[0])\n",
        "                else:\n",
        "                    all_params.append(1)\n",
        "                idx += 1\n",
        "        num_fixed_params = np.sum(all_params)\n",
        "        idx = 0\n",
        "        all_params = []\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # try:\n",
        "                param_shape = list(param.size())\n",
        "                print(\" var {:3}: {:15} {}\".format(idx, str(param_shape), name))\n",
        "                num_params = 1\n",
        "                if (len(param_shape) > 1):\n",
        "                    for p in param_shape:\n",
        "                        if (p > 0):\n",
        "                            num_params = num_params * int(p)\n",
        "                    all_params.append(num_params)\n",
        "                elif len(param_shape) == 1:\n",
        "                    all_params.append(param_shape[0])\n",
        "                else:\n",
        "                    all_params.append(1)\n",
        "                idx += 1\n",
        "        num_params = np.sum(all_params)\n",
        "        print(\"Total number of trainable params {}\".format(num_params))\n",
        "        print(\"Total number of fixed params {}\".format(num_fixed_params))\n",
        "\n",
        "    def do_forward_and_loss(self, feed_dict: dict) -> dict:\n",
        "        '''\n",
        "        Called during training\n",
        "        '''\n",
        "        out_dict = self.forward(feed_dict)\n",
        "        return self.get_loss(feed_dict, out_dict)\n",
        "\n",
        "\n",
        "    def forward(self, feed_dict: dict, return_prob=True) -> dict:\n",
        "        out_dict = self.get_forward(feed_dict)\n",
        "        # only add probs if this model actually produced \"preds\"\n",
        "        if return_prob and \"preds\" in out_dict:\n",
        "            out_dict[\"probs\"] = self.sigmoid(out_dict[\"preds\"])\n",
        "        return out_dict\n",
        "\n",
        "    def wrap_batch(self, batch):\n",
        "        '''\n",
        "        Build feed_dict from batch data and move data to self.device\n",
        "        '''\n",
        "        for k,val in batch.items():\n",
        "            if type(val).__module__ == np.__name__:\n",
        "                batch[k] = torch.from_numpy(val)\n",
        "            elif torch.is_tensor(val):\n",
        "                batch[k] = val\n",
        "            elif type(val) is list:\n",
        "                batch[k] = torch.tensor(val)\n",
        "            else:\n",
        "                continue\n",
        "            if batch[k].type() == \"torch.DoubleTensor\":\n",
        "                batch[k] = batch[k].float()\n",
        "            batch[k] = batch[k].to(self.device)\n",
        "        return batch\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save({\n",
        "            \"model_state_dict\": self.state_dict(),\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"reader_stats\": self.reader_stats\n",
        "        }, self.model_path + \".checkpoint\")\n",
        "        print(\"Model (checkpoint) saved to \" + self.model_path + \".checkpoint\")\n",
        "\n",
        "\n",
        "    def load_from_checkpoint(self, model_path='', with_optimizer=True):\n",
        "        if len(model_path) == 0:\n",
        "            model_path = self.model_path\n",
        "        ckpt_file = model_path + \".checkpoint\"\n",
        "        print(\"Load (checkpoint) from\", ckpt_file)\n",
        "\n",
        "        try:\n",
        "            # First try: safest way (PyTorch 2.6+ default)\n",
        "            checkpoint = torch.load(ckpt_file, map_location=self.device, weights_only=True)\n",
        "        except TypeError:\n",
        "            # PyTorch < 2.6 doesn't know weights_only\n",
        "            checkpoint = torch.load(ckpt_file, map_location=self.device)\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ weights_only=True failed:\", e)\n",
        "            print(\"Retrying with weights_only=False (trusted checkpoint)...\")\n",
        "            checkpoint = torch.load(ckpt_file, map_location=self.device, weights_only=False)\n",
        "\n",
        "        # Restore fields\n",
        "        if \"reader_stats\" in checkpoint:\n",
        "            self.reader_stats = checkpoint[\"reader_stats\"]\n",
        "            print(\"reader_stats loaded:\", self.reader_stats)\n",
        "\n",
        "        if \"model_state_dict\" in checkpoint:\n",
        "            self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        else:\n",
        "            raise KeyError(\"No 'model_state_dict' in checkpoint!\")\n",
        "\n",
        "        if with_optimizer and \"optimizer_state_dict\" in checkpoint:\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        self.model_path = model_path\n",
        "        print(\"Checkpoint loaded successfully ✅\")\n",
        "\n",
        "    def actions_before_train(self, info):  # e.g. initialization\n",
        "        pass\n",
        "\n",
        "    def actions_after_train(self, info):  # e.g. compression\n",
        "        pass\n",
        "\n",
        "    def actions_before_epoch(self, info): # e.g. expectation update\n",
        "        pass\n",
        "\n",
        "    def actions_after_epoch(self, info): # e.g. prunning\n",
        "        pass\n",
        "\n",
        "    #############################\n",
        "    #   Require Implementation  #\n",
        "    #############################\n",
        "\n",
        "    def _define_params(self, args, reader_stats):  # the model components and parameters\n",
        "        pass\n",
        "\n",
        "    def get_forward(self, feed_dict: dict) -> dict:  # the forward function\n",
        "        pass\n",
        "\n",
        "    def get_loss(self, feed_dict: dict, out_dict: dict):  # the loss function\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGwnmys8fOtO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dims, out_dim = 1, dropout_rate = 0., do_batch_norm = True):\n",
        "        super(DNN, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        layers = []\n",
        "\n",
        "        # hidden layers\n",
        "        for hidden_dim in hidden_dims:\n",
        "            linear_layer = nn.Linear(in_dim, hidden_dim)\n",
        "            # torch.nn.init.xavier_uniform_(linear_layer.weight, gain=nn.init.calculate_gain('relu'))\n",
        "            layers.append(linear_layer)\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout_rate > 0:\n",
        "                layers.append(nn.Dropout(dropout_rate))\n",
        "            if do_batch_norm:\n",
        "#                 layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "                layers.append(nn.LayerNorm([hidden_dim]))\n",
        "\n",
        "        # prediction layer\n",
        "        last_layer = nn.Linear(in_dim, out_dim)\n",
        "        layers.append(last_layer)\n",
        "        # torch.nn.init.xavier_uniform_(last_layer.weight, gain=1.0)\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        @input:\n",
        "            `inputs`, [bsz, in_dim]\n",
        "        @output:\n",
        "            `logit`, [bsz, out_dim]\n",
        "        \"\"\"\n",
        "        inputs = inputs.view(-1, self.in_dim)\n",
        "        logit = self.layers(inputs)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCB4_q6RfSAN"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import axes, axis\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "class KRMBUserResponse(BaseModel):\n",
        "    '''\n",
        "    KuaiRand Multi-Behavior user response model\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - user_latent_dim\n",
        "        - item_latent_dim\n",
        "        - enc_dim\n",
        "        - attn_n_head\n",
        "        - transformer_d_forward\n",
        "        - transformer_n_layer\n",
        "        - scorer_hidden_dims\n",
        "        - dropout_rate\n",
        "        - from BaseModel:\n",
        "            - model_path\n",
        "            - loss\n",
        "            - l2_coef\n",
        "        '''\n",
        "        parser = BaseModel.parse_model_args(parser)\n",
        "\n",
        "        parser.add_argument('--user_latent_dim', type=int, default=16,\n",
        "                            help='user latent embedding size')\n",
        "        parser.add_argument('--item_latent_dim', type=int, default=16,\n",
        "                            help='item latent embedding size')\n",
        "        parser.add_argument('--enc_dim', type=int, default=32,\n",
        "                            help='item encoding size')\n",
        "        parser.add_argument('--attn_n_head', type=int, default=4,\n",
        "                            help='number of attention heads in transformer')\n",
        "        parser.add_argument('--transformer_d_forward', type=int, default=64,\n",
        "                            help='forward layer dimension in transformer')\n",
        "        parser.add_argument('--transformer_n_layer', type=int, default=2,\n",
        "                            help='number of encoder layers in transformer')\n",
        "        parser.add_argument('--state_hidden_dims', type=int, nargs='+', default=[128],\n",
        "                            help='hidden dimensions')\n",
        "        parser.add_argument('--scorer_hidden_dims', type=int, nargs='+', default=[128],\n",
        "                            help='hidden dimensions')\n",
        "        parser.add_argument('--dropout_rate', type=float, default=0.1,\n",
        "                            help='dropout rate in deep layers')\n",
        "        return parser\n",
        "\n",
        "    def log(self):\n",
        "        print(\"KRMBUserResponse params:\")\n",
        "        print(f\"\\tuser_latent_dim: {self.user_latent_dim}\")\n",
        "        print(f\"\\titem_latent_dim: {self.item_latent_dim}\")\n",
        "        print(f\"\\tenc_dim: {self.enc_dim}\")\n",
        "        print(f\"\\tattn_n_head: {self.attn_n_head}\")\n",
        "        print(f\"\\tscorer_hidden_dims: {self.scorer_hidden_dims}\")\n",
        "        print(f\"\\tdropout_rate: {self.dropout_rate}\")\n",
        "        print(f\"\\tstate_dim: {self.state_dim}\")\n",
        "        super().log()\n",
        "\n",
        "    def __init__(self, args, reader_stats, device):\n",
        "        self.user_latent_dim = args.user_latent_dim\n",
        "        self.item_latent_dim = args.item_latent_dim\n",
        "        self.enc_dim = args.enc_dim\n",
        "        self.attn_n_head = args.attn_n_head\n",
        "        self.scorer_hidden_dims = args.scorer_hidden_dims\n",
        "        self.dropout_rate = args.dropout_rate\n",
        "        super().__init__(args, reader_stats, device)\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "        self.state_dim = 3*args.enc_dim\n",
        "\n",
        "    def to(self, device):\n",
        "        new_self = super(KRMBUserResponse, self).to(device)\n",
        "        new_self.attn_mask = new_self.attn_mask.to(device)\n",
        "        new_self.pos_emb_getter = new_self.pos_emb_getter.to(device)\n",
        "        new_self.behavior_weight = new_self.behavior_weight.to(device)\n",
        "        return new_self\n",
        "\n",
        "    def _define_params(self, args, reader_stats):\n",
        "        stats = reader_stats\n",
        "\n",
        "        self.user_feature_dims = stats['user_feature_dims'] # {feature_name: dim}\n",
        "        self.item_feature_dims = stats['item_feature_dims'] # {feature_name: dim}\n",
        "\n",
        "        # user embedding\n",
        "        self.uIDEmb = nn.Embedding(stats['n_user']+1, args.user_latent_dim)\n",
        "        self.uFeatureEmb = {}\n",
        "        for f,dim in self.user_feature_dims.items():\n",
        "            embedding_module = nn.Linear(dim, args.user_latent_dim)\n",
        "            self.add_module(f'UFEmb_{f}', embedding_module)\n",
        "            self.uFeatureEmb[f] = embedding_module\n",
        "\n",
        "        # item embedding\n",
        "        self.iIDEmb = nn.Embedding(stats['n_item']+1, args.item_latent_dim)\n",
        "        self.iFeatureEmb = {}\n",
        "        for f,dim in self.item_feature_dims.items():\n",
        "            embedding_module = nn.Linear(dim, args.item_latent_dim)\n",
        "            self.add_module(f'IFEmb_{f}', embedding_module)\n",
        "            self.iFeatureEmb[f] = embedding_module\n",
        "\n",
        "        # feedback embedding\n",
        "        self.feedback_types = stats['feedback_type']\n",
        "        self.feedback_dim = stats['feedback_size']\n",
        "        self.xtr_dim = 2*self.feedback_dim\n",
        "        self.feedbackEncoder = nn.Linear(self.feedback_dim, args.enc_dim)\n",
        "        self.set_behavior_hyper_weight(torch.ones(self.feedback_dim))\n",
        "\n",
        "        # item embedding kernel encoder\n",
        "        self.itemEmbNorm = nn.LayerNorm(args.item_latent_dim)\n",
        "        self.userEmbNorm = nn.LayerNorm(args.user_latent_dim)\n",
        "        self.itemFeatureKernel = nn.Linear(args.item_latent_dim, args.enc_dim)\n",
        "        self.userFeatureKernel = nn.Linear(args.user_latent_dim, args.enc_dim)\n",
        "        self.encDropout = nn.Dropout(self.dropout_rate)\n",
        "        self.encNorm = nn.LayerNorm(args.enc_dim)\n",
        "\n",
        "        # positional embedding\n",
        "        self.max_len = stats['max_seq_len']\n",
        "        self.posEmb = nn.Embedding(self.max_len, args.enc_dim)\n",
        "        self.pos_emb_getter = torch.arange(self.max_len, dtype = torch.long)\n",
        "        self.attn_mask = ~torch.tril(torch.ones((self.max_len,self.max_len), dtype=torch.bool))\n",
        "\n",
        "        # sequence encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=2*args.enc_dim, dim_feedforward = args.transformer_d_forward,\n",
        "                                                   nhead=args.attn_n_head, dropout = args.dropout_rate,\n",
        "                                                   batch_first = True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=args.transformer_n_layer)\n",
        "\n",
        "        # DNN state encoder\n",
        "        self.stateNorm = nn.LayerNorm(args.enc_dim)\n",
        "\n",
        "        # DNN scorer\n",
        "        self.scorer_hidden_dims = args.scorer_hidden_dims\n",
        "        self.scorer = DNN(3*args.enc_dim, args.state_hidden_dims, self.feedback_dim * args.enc_dim,\n",
        "                          dropout_rate = args.dropout_rate, do_batch_norm = True)\n",
        "\n",
        "    def set_behavior_hyper_weight(self, weight):\n",
        "        self.behavior_weight = weight.view(-1)\n",
        "        assert len(self.behavior_weight) == self.feedback_dim\n",
        "\n",
        "    def get_forward(self, feed_dict: dict):\n",
        "        '''\n",
        "        This is used during simulator training\n",
        "        When serving as a simulator, it calls encode_state() + get_pointwise_score()\n",
        "        @input:\n",
        "        - feed_dict: {\n",
        "            'user_id': (B,)\n",
        "            'uf_{feature_name}': (B,feature_dim), the user features\n",
        "            'item_id': (B,), the target item\n",
        "            'if_{feature_name}': (B,feature_dim), the target item features\n",
        "            'history': (B,max_H)\n",
        "            'history_if_{feature_name}': (B,max_H,feature_dim), the history item features\n",
        "            ... (irrelevant input)\n",
        "        }\n",
        "        @output:\n",
        "        - out_dict: {'preds': (B,-1,n_feedback), 'reg': scalar}\n",
        "        '''\n",
        "        B = feed_dict['user_id'].shape[0]\n",
        "\n",
        "        # target item\n",
        "        # (B, -1, enc_dim)\n",
        "        item_enc, item_reg = self.get_item_encoding(feed_dict['item_id'],\n",
        "                                          {k[3:]:v for k,v in feed_dict.items() if k[:3] == 'if_'}, B)\n",
        "\n",
        "\n",
        "        # (B, -1, 1, enc_dim)\n",
        "        item_enc = item_enc.view(B,-1,1,self.enc_dim)\n",
        "\n",
        "\n",
        "        # user encoding\n",
        "        state_encoder_output = self.encode_state(feed_dict, B)\n",
        "        # (B, 1, 3*enc_dim)\n",
        "        user_state = state_encoder_output['state'].view(B,1,3*self.enc_dim)\n",
        "        # (B, -1, n_feedback), (B, -1, n_feedback)\n",
        "        behavior_scores, point_scores = self.get_pointwise_scores(user_state, item_enc, B)\n",
        "\n",
        "\n",
        "        # regularization terms\n",
        "        reg = self.get_regularization(self.feedbackEncoder,\n",
        "                                      self.itemFeatureKernel, self.userFeatureKernel,\n",
        "                                      self.posEmb, self.transformer, self.scorer)\n",
        "#         for v in self.uFeatureEmb.values():\n",
        "#             reg += self.get_regularization(v)\n",
        "#         for v in self.iFeatureEmb.values():\n",
        "#             reg += self.get_regularization(v)\n",
        "        reg = reg + state_encoder_output['reg'] + item_reg\n",
        "        return {'preds': behavior_scores, 'state': user_state, 'reg': reg}\n",
        "#         output_dict['reg'] = reg\n",
        "#         return output_dict\n",
        "\n",
        "    def encode_state(self, feed_dict, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: {\n",
        "            'user_id': (B,)\n",
        "            'uf_{feature_name}': (B,feature_dim), the user features\n",
        "            'history': (B,max_H)\n",
        "            'history_if_{feature_name}': (B,max_H,feature_dim), the history item features\n",
        "            ... (irrelevant input)\n",
        "        }\n",
        "        - B: batch size\n",
        "        @output:\n",
        "        - out_dict:{\n",
        "            'out_seq': (B,max_H,2*enc_dim)\n",
        "            'state': (B,n_feedback*enc_dim)\n",
        "            'reg': scalar\n",
        "        }\n",
        "        '''\n",
        "        # user history\n",
        "        # (B, max_H, enc_dim)\n",
        "        history_enc, history_reg = self.get_item_encoding(feed_dict['history'],\n",
        "                                             {f:feed_dict[f'history_if_{f}'] for f in self.iFeatureEmb}, B)\n",
        "        history_enc = history_enc.view(B, self.max_len, self.enc_dim)\n",
        "        # (1, max_H, enc_dim)\n",
        "        pos_emb = self.posEmb(self.pos_emb_getter).view(1,self.max_len,self.enc_dim)\n",
        "        # (B, max_H, enc_dim)\n",
        "        seq_enc_feat = self.encNorm(self.encDropout(history_enc + pos_emb))\n",
        "        # (B, max_H, enc_dim)\n",
        "        feedback_emb = self.get_response_embedding(feed_dict, B)\n",
        "        # (B, max_H, 2*enc_dim)\n",
        "        seq_enc = torch.cat((seq_enc_feat, feedback_emb), dim = -1)\n",
        "        # (B, max_H, 2*enc_dim)\n",
        "        output_seq = self.transformer(seq_enc, mask = self.attn_mask)\n",
        "        # (B, 2*enc_dim)\n",
        "        hist_enc = output_seq[:,-1,:].view(B,2*self.enc_dim)\n",
        "        # user features\n",
        "        # (B, enc_dim), scalar\n",
        "        user_enc, user_reg = self.get_user_encoding(feed_dict['user_id'],\n",
        "                                          {k[3:]:v for k,v in feed_dict.items() if k[:3] == 'uf_'}, B)\n",
        "        # (B, enc_dim)\n",
        "        user_enc = self.encNorm(self.encDropout(user_enc)).view(B,self.enc_dim)\n",
        "        # (B, 3*enc_dim)\n",
        "        state = torch.cat([hist_enc,user_enc], 1)\n",
        "        return {'output_seq': output_seq, 'state': state, 'reg': user_reg + history_reg}\n",
        "\n",
        "    def get_user_encoding(self, user_ids, user_features, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_ids: (B,), encoded user id\n",
        "        - user_features: {'uf_{feature_name}': (B, feature_dim)}\n",
        "        '''\n",
        "        # (B, 1, u_latent_dim)\n",
        "        user_id_emb = self.uIDEmb(user_ids).view(B,1,self.user_latent_dim)\n",
        "        # [(B, 1, u_latent_dim)] * n_user_feature\n",
        "        user_feature_emb = [user_id_emb]\n",
        "        for f,fEmbModule in self.uFeatureEmb.items():\n",
        "            user_feature_emb.append(fEmbModule(user_features[f]).view(B,1,self.user_latent_dim))\n",
        "        # (B, n_user_feature+1, u_latent_dim)\n",
        "        combined_user_emb = torch.cat(user_feature_emb, 1)\n",
        "        combined_user_emb = self.userEmbNorm(combined_user_emb)\n",
        "        # (B, enc_dim)\n",
        "        encoding = self.userFeatureKernel(combined_user_emb).sum(1)\n",
        "        # regularization\n",
        "        reg = torch.mean(user_id_emb * user_id_emb)\n",
        "        return encoding, reg\n",
        "\n",
        "    def get_item_encoding(self, item_ids, item_features, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - item_ids: (B,) or (B,H), encoded item id\n",
        "        - item_features: {'if_{feature_name}': (B,feature_dim) or (B,H,feature_dim)}\n",
        "        '''\n",
        "        # (B, 1, i_latent_dim) or (B, H, i_latent_dim)\n",
        "        item_id_emb = self.iIDEmb(item_ids).view(B,-1,self.item_latent_dim)\n",
        "        L = item_id_emb.shape[1]\n",
        "        # [(B, 1, i_latent_dim)] * n_item_feature or [(B, H, i_latent_dim)] * n_item_feature\n",
        "        item_feature_emb = [item_id_emb]\n",
        "        for f,fEmbModule in self.iFeatureEmb.items():\n",
        "            f_dim = self.item_feature_dims[f]\n",
        "            item_feature_emb.append(fEmbModule(item_features[f].view(B,L,f_dim)).view(B,-1,self.item_latent_dim))\n",
        "        # (B, 1, n_item_feature+1, i_latent_dim) or (B, H, n_item_feature+1, i_latent_dim)\n",
        "        combined_item_emb = torch.cat(item_feature_emb, -1).view(B, L, -1, self.item_latent_dim)\n",
        "        combined_item_emb = self.itemEmbNorm(combined_item_emb)\n",
        "        # (B, 1, enc_dim) or (B, H, enc_dim)\n",
        "        encoding = self.itemFeatureKernel(combined_item_emb).sum(2)\n",
        "        encoding = encoding.view(B, -1, self.enc_dim)\n",
        "        encoding = self.encNorm(encoding)\n",
        "        # regularization\n",
        "        reg = torch.mean(item_id_emb * item_id_emb)\n",
        "        return encoding, reg\n",
        "\n",
        "    def get_response_embedding(self, feed_dict, B):\n",
        "        resp_list = []\n",
        "        for f in self.feedback_types:\n",
        "            # (B, max_H)\n",
        "            resp = feed_dict[f'history_{f}'].view(B, self.max_len)\n",
        "            resp_list.append(resp)\n",
        "        # (B, max_H, n_feedback)\n",
        "        combined_resp = torch.cat(resp_list, -1).view(B,self.max_len,self.feedback_dim)\n",
        "        # (B, max_H, i_latent_dim)\n",
        "        resp_emb = self.feedbackEncoder(combined_resp)\n",
        "        return resp_emb\n",
        "\n",
        "    def get_loss(self, feed_dict: dict, out_dict: dict):\n",
        "        \"\"\"\n",
        "        @input:\n",
        "        - feed_dict: {...}\n",
        "        - out_dict: {\"preds\":, \"reg\":}\n",
        "\n",
        "        Loss terms implemented:\n",
        "        - BCE\n",
        "        \"\"\"\n",
        "        B = feed_dict['user_id'].shape[0]\n",
        "        # (B, -1, n_feedback)\n",
        "        preds = out_dict['preds'].view(B,-1,self.feedback_dim)\n",
        "        # [(B, -1, 1)] * n_feedback\n",
        "        targets = {f:feed_dict[f].view(B,-1).to(torch.float) for f in self.feedback_types}\n",
        "        # (B, -1, n_feedback)\n",
        "        loss_weight = feed_dict['loss_weight'].view(B,-1,self.feedback_dim)\n",
        "\n",
        "        if self.loss_type == 'bce':\n",
        "            behavior_loss = {}\n",
        "            loss = 0\n",
        "            for i,fb in enumerate(self.feedback_types):\n",
        "                if self.behavior_weight[i] == 0:\n",
        "                    continue\n",
        "                Y = targets[fb].view(-1)\n",
        "                P = preds[:,:,i].view(-1)\n",
        "                W = loss_weight[:,:,i].view(-1)\n",
        "                # (B*L,)\n",
        "                point_loss = self.bce_loss(self.sigmoid(P), Y)\n",
        "                behavior_loss[fb] = torch.mean(point_loss).item()\n",
        "                point_loss = torch.mean(point_loss * W)\n",
        "                point_loss = torch.mean(point_loss)\n",
        "                loss = self.behavior_weight[i] * point_loss + loss\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "        out_dict['loss'] = loss + self.l2_coef * out_dict['reg']\n",
        "        out_dict['behavior_loss'] = behavior_loss\n",
        "        return out_dict\n",
        "\n",
        "\n",
        "    def get_pointwise_scores(self, user_state, item_enc, B):\n",
        "        '''\n",
        "        Get user-item pointwise interaction scores\n",
        "        @input:\n",
        "        - user_state: (B, state_dim)\n",
        "        - item_enc: (B, -1, 1, enc_dim) for batch-wise candidates or (1, -1, 1, enc_dim) for universal candidates\n",
        "        - B: batch size\n",
        "        @output:\n",
        "        - behavior_scores: (B, -1, n_feedback)\n",
        "        '''\n",
        "        # scoring\n",
        "        # (B, 1, n_feedback, enc_dim)\n",
        "        behavior_attn = self.scorer(user_state).view(B,1,self.feedback_dim,self.enc_dim)\n",
        "        # (B, 1, n_feedback, enc_dim)\n",
        "        behavior_attn = self.stateNorm(behavior_attn)\n",
        "        # (B, -1, n_feedback)\n",
        "        point_scores = (behavior_attn * item_enc).mean(dim = -1).view(B,-1,self.feedback_dim)\n",
        "        return point_scores, torch.mean(point_scores, dim = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxp-rYCUfbsE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def do_eval(model, reader, args):\n",
        "    reader.set_phase(\"val\")\n",
        "\n",
        "    eval_loader = DataLoader(reader, batch_size=1, shuffle=False, pin_memory=True, num_workers=1, persistent_workers=True)\n",
        "\n",
        "\n",
        "    val_report = {'loss': [], 'auc': {}}\n",
        "    Y_dict = {f: [] for f in model.feedback_types}\n",
        "    P_dict = {f: [] for f in model.feedback_types}\n",
        "    pbar = tqdm(total = len(reader))\n",
        "    with torch.no_grad():\n",
        "        for i, batch_data in enumerate(eval_loader):\n",
        "            wrapped_batch = wrap_batch(batch_data, device = args.device)\n",
        "            out_dict = model.do_forward_and_loss(wrapped_batch)\n",
        "            loss = out_dict['loss']\n",
        "            val_report['loss'].append(loss.item())\n",
        "            for j,f in enumerate(model.feedback_types):\n",
        "                Y_dict[f].append(wrapped_batch[f].view(-1).detach().cpu().numpy())\n",
        "                P_dict[f].append(out_dict['preds'][:,:,j].view(-1).detach().cpu().numpy())\n",
        "            pbar.update(args.batch_size)\n",
        "    val_report['loss'] = (np.mean(val_report['loss']), np.min(val_report['loss']), np.max(val_report['loss']))\n",
        "    for f in model.feedback_types:\n",
        "        val_report['auc'][f] = roc_auc_score(np.concatenate(Y_dict[f]),\n",
        "                                             np.concatenate(P_dict[f]))\n",
        "    pbar.close()\n",
        "    return val_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-FaRam6B3xU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Any, Dict, Optional, List, Tuple\n",
        "from yacs.config import CfgNode\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "# logger\n",
        "def set_logger(log_file, name=\"default\"):\n",
        "    \"\"\"\n",
        "    Set logger.\n",
        "    Args:\n",
        "        log_file (str): log file path\n",
        "        name (str): logger name\n",
        "    \"\"\"\n",
        "\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    formatter = logging.Formatter(\n",
        "        \"%(asctime)s - %(levelname)s - %(module)s - %(funcName)s - %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    )\n",
        "    print(formatter.format(logging.LogRecord(\n",
        "    name='root',\n",
        "    level=logging.INFO,\n",
        "    pathname=None,\n",
        "    lineno=None,\n",
        "    msg='Test message',\n",
        "    args=None,\n",
        "    exc_info=None,\n",
        "    )))\n",
        "    output_folder = \"output\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Create the 'log' folder if it doesn't exist\n",
        "    log_folder = os.path.join(output_folder, \"log\")\n",
        "    if not os.path.exists(log_folder):\n",
        "        os.makedirs(log_folder)\n",
        "\n",
        "    # Create the 'message' folder if it doesn't exist\n",
        "    message_folder = os.path.join(output_folder, \"message\")\n",
        "    if not os.path.exists(message_folder):\n",
        "        os.makedirs(message_folder)\n",
        "    log_file = os.path.join(log_folder, log_file)\n",
        "    handler = logging.FileHandler(log_file, mode=\"w\")\n",
        "    handler.setLevel(logging.INFO)\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.handlers = []\n",
        "    logger.addHandler(handler)\n",
        "    print(logger)\n",
        "    return logger\n",
        "\n",
        "def load_cfg(cfg_file: str, new_allowed: bool = True) -> CfgNode:\n",
        "    \"\"\"\n",
        "    Load config from file.\n",
        "    Args:\n",
        "        cfg_file (str): config file path\n",
        "        new_allowed (bool): whether to allow new keys in config\n",
        "    \"\"\"\n",
        "    with open(cfg_file, \"r\") as fi:\n",
        "        cfg = CfgNode.load_cfg(fi)\n",
        "    cfg.set_new_allowed(new_allowed)\n",
        "    return cfg\n",
        "\n",
        "def add_variable_to_config(cfg: CfgNode, name: str, value: Any) -> CfgNode:\n",
        "    \"\"\"\n",
        "    Add variable to config.\n",
        "    Args:\n",
        "        cfg (CfgNode): config\n",
        "        name (str): variable name\n",
        "        value (Any): variable value\n",
        "    \"\"\"\n",
        "    cfg.defrost()\n",
        "    cfg[name] = value\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "def ensure_dir(dir_path):\n",
        "    \"\"\"\n",
        "    Make sure the directory exists, if it does not exist, create it\n",
        "    Args:\n",
        "        dir_path (str): The directory path.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def calculate_entropy(movie_types):\n",
        "    type_freq = {}\n",
        "    for movie_type in movie_types:\n",
        "        if movie_type in type_freq:\n",
        "            type_freq[movie_type] += 1\n",
        "        else:\n",
        "            type_freq[movie_type] = 1\n",
        "\n",
        "    total_movies = len(movie_types)\n",
        "\n",
        "    entropy = 0\n",
        "    for key in type_freq:\n",
        "        prob = type_freq[key] / total_movies\n",
        "        entropy -= prob * math.log2(prob)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def get_entropy(inters, data):\n",
        "    genres = data.get_genres_by_id(inters)\n",
        "    entropy = calculate_entropy(genres)\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxtTP_RpB7yG"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Data:\n",
        "    \"\"\"\n",
        "    Data class for loading data from local files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.items = {}\n",
        "        self.users = {}\n",
        "        self.db = None\n",
        "        self.tot_relationship_num = 0\n",
        "        self.netwerk_density = 0.0\n",
        "        self.role_id = -1\n",
        "        self.interrating = {}\n",
        "        self.user_ratings = {}\n",
        "        self.item_ratings = {}\n",
        "        self.load_items(config[\"item_path\"])\n",
        "        self.load_users(config[\"user_path\"])\n",
        "        self.load_interactions_rating(config[\"rating_path\"])\n",
        "\n",
        "    def load_items(self, file_path):\n",
        "        \"\"\"\n",
        "        Load items from local file.\n",
        "        \"\"\"\n",
        "        with open(file_path, \"r\", newline=\"\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            next(reader)  # Skip the header line\n",
        "            for row in reader:\n",
        "                item_id, title, genre, description = row\n",
        "                self.items[int(item_id)] = {\n",
        "                    \"name\": title.strip(),\n",
        "                    \"genre\": genre,\n",
        "                    \"description\": description.strip(),\n",
        "                    \"inter_cnt\": 0,\n",
        "                    \"mention_cnt\": 0,\n",
        "                }\n",
        "\n",
        "    def load_users(self, file_path):\n",
        "        \"\"\"\n",
        "        Load users from local file.\n",
        "        \"\"\"\n",
        "        cnt = 1\n",
        "        with open(file_path, \"r\", newline=\"\") as file:\n",
        "            reader = csv.reader(file)\n",
        "            next(reader)  # Skip the header line\n",
        "            for row in reader:\n",
        "                # print(len(row), row)\n",
        "                user_id, name, gender, age, traits, status, interest, feature = row\n",
        "                # user_id, name, gender, age, status, pos, neg = row\n",
        "                self.users[cnt] = {\n",
        "                    \"name\": name,\n",
        "                    \"gender\": gender,\n",
        "                    \"age\": int(age),\n",
        "                    \"traits\": traits,\n",
        "                    \"status\": status,\n",
        "                    \"interest\": interest,\n",
        "                    \"feature\": feature,\n",
        "                }\n",
        "                cnt += 1\n",
        "\n",
        "\n",
        "    def load_interactions_rating(self, file_path):\n",
        "      \"\"\"\n",
        "      Load user-item interactions (with rating) from local file.\n",
        "      Stores in self.interrating as a dict:\n",
        "      {user_id: [(item_id, rating), ...], ...}\n",
        "      \"\"\"\n",
        "      with open(file_path, \"r\", newline=\"\") as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader)  # Skip the header line\n",
        "        for row in reader:\n",
        "            user_id, item_id, rating = row\n",
        "            user_id = int(user_id)\n",
        "            item_id = int(item_id)\n",
        "            rating = int(rating)\n",
        "            if user_id not in self.interrating:\n",
        "                self.interrating[user_id] = []\n",
        "            self.interrating[user_id].append((item_id, rating))\n",
        "\n",
        "            if user_id not in self.user_ratings:\n",
        "                self.user_ratings[user_id] = []\n",
        "            self.user_ratings[user_id].append(rating)\n",
        "\n",
        "            # Store item rating\n",
        "            if item_id not in self.item_ratings:\n",
        "                self.item_ratings[item_id] = []\n",
        "            self.item_ratings[item_id].append(rating)\n",
        "\n",
        "      # Compute and store average historical rating for each user\n",
        "      self.user_avg_rating = {uid: sum(ratings)/len(ratings) for uid, ratings in self.user_ratings.items() if ratings}\n",
        "\n",
        "      # Compute and store average historical rating for each item\n",
        "      self.item_avg_rating = {iid: sum(ratings)/len(ratings) for iid, ratings in self.item_ratings.items() if ratings}\n",
        "\n",
        "\n",
        "    def get_full_items(self):\n",
        "        return list(self.items.keys())\n",
        "\n",
        "    def get_inter_popular_items(self):\n",
        "        \"\"\"\n",
        "        Get the most popular items based on the number of interactions.\n",
        "        \"\"\"\n",
        "        ids = sorted(\n",
        "            self.items.keys(), key=lambda x: self.items[x][\"inter_cnt\"], reverse=True\n",
        "        )[:3]\n",
        "        return self.get_item_names(ids)\n",
        "\n",
        "    def add_inter_cnt(self, item_names):\n",
        "        item_ids = self.get_item_ids(item_names)\n",
        "        print(\"item ids:\", item_ids)\n",
        "        for item_id in item_ids:\n",
        "            self.items[item_id][\"inter_cnt\"] += 1\n",
        "\n",
        "    def add_mention_cnt(self, item_names):\n",
        "        item_ids = self.get_item_ids(item_names)\n",
        "        for item_id in item_ids:\n",
        "            self.items[item_id][\"mention_cnt\"] += 1\n",
        "\n",
        "    def get_mention_popular_items(self):\n",
        "        \"\"\"\n",
        "        Get the most popular items based on the number of mentions.\n",
        "        \"\"\"\n",
        "        ids = sorted(\n",
        "            self.items.keys(), key=lambda x: self.items[x][\"mention_cnt\"], reverse=True\n",
        "        )[:3]\n",
        "        return self.get_item_names(ids)\n",
        "\n",
        "    def get_item_names(self, item_ids):\n",
        "        return [\"<\" + self.items[item_id][\"name\"] + \">\" for item_id in item_ids]\n",
        "\n",
        "    def get_item_ids(self, item_names):\n",
        "        item_ids = []\n",
        "        for item in item_names:\n",
        "            for item_id, item_info in self.items.items():\n",
        "                if item_info[\"name\"] in item:\n",
        "                    item_ids.append(item_id)\n",
        "                    break\n",
        "        return item_ids\n",
        "\n",
        "    def get_item_ids_exact(self, item_names):\n",
        "        \"\"\"\n",
        "        Get item ids from item names.\n",
        "        I coundn't find any difference with the get_item_ids(item_names) function\n",
        "        \"\"\"\n",
        "        item_ids = []\n",
        "        for item in item_names:\n",
        "            for item_id, item_info in self.items.items():\n",
        "                if item_info[\"name\"] == item:\n",
        "                    item_ids.append(item_id)\n",
        "                    break\n",
        "        return item_ids\n",
        "\n",
        "    def get_full_users(self):\n",
        "        return list(self.users.keys())\n",
        "\n",
        "    def get_user_profile(self, user_id):\n",
        "        \"\"\"\n",
        "        Return the user profile as a formatted string for the given user_id.\n",
        "        \"\"\"\n",
        "        user = self.users.get(user_id)\n",
        "        if not user:\n",
        "           return f\"User ID {user_id} not found.\"\n",
        "\n",
        "        profile = (\n",
        "        f\"User ID: {user_id};; Name: {user['name']}\\n\"\n",
        "        f\"Gender: {user['gender']};; Age: {user['age']};; Status: {user['status']}\\n\"\n",
        "        f\"Traits: {user['traits']};; Interest: {user['interest']};; Feature: {user['feature']}\\n\")\n",
        "        return profile\n",
        "\n",
        "    def get_user_names(self, user_ids):\n",
        "        return [self.users[user_id][\"name\"] for user_id in user_ids]\n",
        "\n",
        "    def get_user_ids(self, user_names):\n",
        "        user_ids = []\n",
        "        for user in user_names:\n",
        "            for user_id, user_info in self.users.items():\n",
        "                if user_info[\"name\"] == user:\n",
        "                    user_ids.append(user_id)\n",
        "                    break\n",
        "        return user_ids\n",
        "\n",
        "    def get_user_num(self):\n",
        "        \"\"\"\n",
        "        Return the number of users.\n",
        "        \"\"\"\n",
        "        return len(self.users.keys())\n",
        "\n",
        "    def get_item_num(self):\n",
        "        \"\"\"\n",
        "        Return the number of items.\n",
        "        \"\"\"\n",
        "        return len(self.items.keys())\n",
        "\n",
        "    def search_items(self, item, k=50):\n",
        "        \"\"\"\n",
        "        Search similar items from faiss db.\n",
        "        Args:\n",
        "            item: str, item name\n",
        "            k: int, number of similar items to return\n",
        "        \"\"\"\n",
        "        docs = self.db.similarity_search(item, k)\n",
        "        item_names = [doc.page_content for doc in docs]\n",
        "        return item_names\n",
        "\n",
        "\n",
        "    def get_item_description_by_id(self, item_ids):\n",
        "        \"\"\"\n",
        "        Get description of items by item id.\n",
        "        \"\"\"\n",
        "        return [self.items[item_id][\"description\"] for item_id in item_ids]\n",
        "\n",
        "    def get_item_description_by_name(self, item_names):\n",
        "        \"\"\"\n",
        "        Get description of items by item name.\n",
        "        \"\"\"\n",
        "        item_descriptions = []\n",
        "        for item in item_names:\n",
        "            found = False\n",
        "            for item_id, item_info in self.items.items():\n",
        "                if item_info[\"name\"] == item.strip(\" <>\"):\n",
        "                    item_descriptions.append(item_info[\"description\"])\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                item_descriptions.append(\"\")\n",
        "        return item_descriptions\n",
        "\n",
        "    def get_genres_by_id(self, item_ids):\n",
        "        \"\"\"\n",
        "        Get genre of items by item id.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            genre\n",
        "            for item_id in item_ids\n",
        "            for genre in self.items[item_id][\"genre\"].split('|')\n",
        "        ]\n",
        "\n",
        "    def hit_at_k(self, ground_truth, predicted, k):\n",
        "        \"\"\"Return 1 if any of the top-k predicted are relevant, else 0.\"\"\"\n",
        "        return int(bool(set(ground_truth) & set(predicted[:k])))\n",
        "\n",
        "    def ndcg_at_k(self, ground_truth, predicted, k):\n",
        "        \"\"\"Compute NDCG@k for a single user.\"\"\"\n",
        "        def dcg(rel):\n",
        "          return np.sum([(2**r - 1) / np.log2(i + 2) for i, r in enumerate(rel)])\n",
        "\n",
        "        rel = [1 if item in ground_truth else 0 for item in predicted[:k]]\n",
        "        ideal_rel = sorted([1]*min(len(ground_truth), k) + [0]*(k - min(len(ground_truth), k)), reverse=True)\n",
        "        dcg_score = dcg(rel)\n",
        "        idcg_score = dcg(ideal_rel)\n",
        "        return dcg_score / idcg_score if idcg_score > 0 else 0.0\n",
        "\n",
        "    def mse(self, ground_truth_ratings, predicted_ratings, items=None):\n",
        "        \"\"\"Compute MSE for ratings (on items in both sets).\"\"\"\n",
        "        if items is None:\n",
        "           items = set(ground_truth_ratings.keys()) & set(predicted_ratings.keys())\n",
        "        else:\n",
        "           items = set(items) & set(ground_truth_ratings.keys()) & set(predicted_ratings.keys())\n",
        "        if not items:\n",
        "           return np.nan\n",
        "        errors = [(ground_truth_ratings[i] - predicted_ratings[i]) ** 2 for i in items]\n",
        "        return np.mean(errors)\n",
        "\n",
        "    def rmse(self, ground_truth_ratings, predicted_ratings, items=None):\n",
        "        \"\"\"Compute RMSE for ratings.\"\"\"\n",
        "        return np.sqrt(self.mse(ground_truth_ratings, predicted_ratings, items))\n",
        "\n",
        "    def safe_log(self, x):\n",
        "        \"\"\"Numerically safe log.\"\"\"\n",
        "        return math.log(max(x, 1e-15))\n",
        "\n",
        "    def ordered_probit_probs(self, pred_int, K, taus=None):\n",
        "        \"\"\"\n",
        "        Compute ordered probit class probabilities for a predicted integer rating.\n",
        "\n",
        "        pred_int : int\n",
        "            The predicted integer rating (e.g., 1..K).\n",
        "        K : int\n",
        "            Number of rating classes (e.g., 5 for 1–5 stars).\n",
        "        taus : list or array, optional\n",
        "            Thresholds separating the ordered categories.\n",
        "            If None, uses equally spaced thresholds [1.5, 2.5, ..., K-0.5].\n",
        "        \"\"\"\n",
        "\n",
        "        if taus is None:\n",
        "           taus = np.array([1.5 + i for i in range(K-1)])  # default thresholds\n",
        "\n",
        "        assert len(taus) == K-1\n",
        "\n",
        "        def Phi(z):\n",
        "            return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))  # Normal CDF\n",
        "\n",
        "        probs = []\n",
        "        for k in range(1, K+1):\n",
        "           if k == 1:\n",
        "              lower = -np.inf\n",
        "              upper = taus[0]\n",
        "           elif k == K:\n",
        "              lower = taus[-1]\n",
        "              upper = np.inf\n",
        "           else:\n",
        "              lower = taus[k-2]\n",
        "              upper = taus[k-1]\n",
        "           p_lower = 0.0 if lower == -np.inf else Phi((lower - pred_int))\n",
        "           p_upper = 1.0 if upper == np.inf else Phi((upper - pred_int))\n",
        "           probs.append(max(p_upper - p_lower, 1e-15))\n",
        "\n",
        "        probs = np.array(probs)\n",
        "        probs /= probs.sum()  # normalize\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4clcMqiWCDWH"
      },
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "    \"\"\"Base class for all models.\"\"\"\n",
        "\n",
        "    def __init__(self, config, n_users, n_items):\n",
        "        self.config = config\n",
        "        self.items = None\n",
        "\n",
        "    def get_full_sort_items(self, user_id, *args, **kwargs):\n",
        "        \"\"\"Get a list of sorted items for a given user.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _sort_full_items(self, user_id, *args, **kwargs):\n",
        "        \"\"\"Sort a list of items for a given user.\"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgd5aqhOCIwY"
      },
      "outputs": [],
      "source": [
        "from typing import Union, List, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MF(BaseModel, nn.Module):\n",
        "    def __init__(self, config, n_users, n_items):\n",
        "        BaseModel.__init__(self, config, n_users, n_items)\n",
        "        nn.Module.__init__(self)\n",
        "        self.config = config\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        torch.manual_seed(config['seed'])\n",
        "        # define layers and loss\n",
        "        self.user_embedding = nn.Embedding(self.n_users+1, self.embedding_size)\n",
        "        self.item_embedding = nn.Embedding(self.n_items+1, self.embedding_size)\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        \"\"\"Predicts the rating of a user for an item.\"\"\"\n",
        "        user_embed = self.user_embedding(user)\n",
        "        item_embed = self.item_embedding(item)\n",
        "\n",
        "        # Dot product between user and item embeddings to predict rating\n",
        "        predicted_rating = (user_embed * item_embed).sum(1)\n",
        "\n",
        "        return predicted_rating\n",
        "\n",
        "    def get_full_sort_items(self, user, items):\n",
        "        \"\"\"Get a list of sorted items for a given user.\"\"\"\n",
        "        predicted_ratings = self.forward(user, items)\n",
        "        sorted_items = self._sort_full_items(user, predicted_ratings, items)\n",
        "        return sorted_items.tolist()\n",
        "\n",
        "    def _sort_full_items(self, user, predicted_ratings, items):\n",
        "        \"\"\"Sort items based on their predicted ratings.\"\"\"\n",
        "        # Sort items based on ratings in descending order and return item indices\n",
        "        _, sorted_indices = torch.sort(predicted_ratings, descending=True)\n",
        "        return items[sorted_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m27wxhbiCNgd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple, Iterable, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LightGCN(BaseModel, nn.Module):\n",
        "    \"\"\"\n",
        "    LightGCN for recommendation.\n",
        "    Reference: He et al., \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        n_users: int,\n",
        "        n_items: int,\n",
        "        interactions: Iterable[Tuple[int, int]],\n",
        "        n_layers: int = 3,\n",
        "        embedding_dim: int = 64,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        interactions: iterable of (user_id, item_id) pairs (duplicates okay, handled as multi-edges of weight 1)\n",
        "        \"\"\"\n",
        "        BaseModel.__init__(self, config, n_users, n_items)\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        torch.manual_seed(config.get(\"seed\", 2023))\n",
        "\n",
        "        # Embeddings\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "        # Build normalized adjacency once\n",
        "        self.Graph = self._build_normalized_adj(interactions).to(self.device)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    # ---------- Graph utilities ----------\n",
        "    def _build_normalized_adj(self, interactions: Iterable[Tuple[int, int]]) -> torch.sparse.FloatTensor:\n",
        "        \"\"\"\n",
        "        Build the symmetrically normalized adjacency matrix:\n",
        "            A_hat = D^{-1/2} * A * D^{-1/2}\n",
        "        for the bipartite user-item graph where nodes = users + items\n",
        "        Size: (n_nodes, n_nodes) with n_nodes = n_users + n_items\n",
        "        \"\"\"\n",
        "        n_nodes = self.n_users + self.n_items\n",
        "\n",
        "        # Collect COO edges (user <-> item, undirected)\n",
        "        rows = []\n",
        "        cols = []\n",
        "        vals = []\n",
        "\n",
        "        for u, i, *rest in interactions:\n",
        "            # Allow (u,i) or (u,i,rating) — rating ignored here (implicit 1)\n",
        "            if u < 0 or u >= self.n_users or i < 0 or i >= self.n_items:\n",
        "                continue\n",
        "            i_offset = self.n_users + i  # item node index in unified graph\n",
        "            # u -> i\n",
        "            rows.append(u); cols.append(i_offset); vals.append(1.0)\n",
        "            # i -> u\n",
        "            rows.append(i_offset); cols.append(u); vals.append(1.0)\n",
        "\n",
        "        if len(rows) == 0:\n",
        "            # Empty graph fallback (identity to avoid NaNs)\n",
        "            indices = torch.arange(n_nodes, dtype=torch.long)\n",
        "            indices = torch.stack([indices, indices], dim=0)\n",
        "            values = torch.ones(n_nodes, dtype=torch.float32)\n",
        "            return torch.sparse_coo_tensor(indices, values, (n_nodes, n_nodes)).coalesce()\n",
        "\n",
        "        indices = torch.tensor([rows, cols], dtype=torch.long)\n",
        "        values = torch.tensor(vals, dtype=torch.float32)\n",
        "        A = torch.sparse_coo_tensor(indices, values, (n_nodes, n_nodes)).coalesce()\n",
        "\n",
        "        # Degree vector d = sum of rows\n",
        "        deg = torch.sparse.sum(A, dim=1).to_dense()  # (n_nodes,)\n",
        "        # Avoid divide-by-zero\n",
        "        deg = torch.clamp(deg, min=1e-12)\n",
        "        d_inv_sqrt = torch.pow(deg, -0.5)\n",
        "\n",
        "        # Normalize values: for each edge (i,j), val *= d^-1/2[i] * d^-1/2[j]\n",
        "        row, col = A.indices()\n",
        "        norm_vals = A.values() * d_inv_sqrt[row] * d_inv_sqrt[col]\n",
        "\n",
        "        A_hat = torch.sparse_coo_tensor(A.indices(), norm_vals, A.size()).coalesce()\n",
        "        return A_hat\n",
        "\n",
        "    # ---------- Embedding propagation ----------\n",
        "    def propagate(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Perform K-layer LightGCN propagation and return final user & item embeddings.\n",
        "        E^(0) = concat([U, I])  -> shape (n_users + n_items, d)\n",
        "        E_final = 1/(K+1) * sum_{k=0..K} E^(k)\n",
        "        \"\"\"\n",
        "        E_u0 = self.user_embedding.weight\n",
        "        E_i0 = self.item_embedding.weight\n",
        "        E0 = torch.cat([E_u0, E_i0], dim=0)  # (n_users + n_items, d)\n",
        "\n",
        "        all_layers = [E0]\n",
        "        x = E0\n",
        "        for _ in range(self.n_layers):\n",
        "            x = torch.sparse.mm(self.Graph, x)  # LightGCN propagation (no weights, no nonlinearity)\n",
        "            all_layers.append(x)\n",
        "\n",
        "        E = torch.stack(all_layers, dim=0).mean(dim=0)  # layer-wise average\n",
        "        Eu, Ei = torch.split(E, [self.n_users, self.n_items], dim=0)\n",
        "        return Eu, Ei\n",
        "\n",
        "    # ---------- Scoring ----------\n",
        "    def forward(self, users: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict scores for (users, items). Supports:\n",
        "          - users: (B,), items: (B,)  -> elementwise scores\n",
        "          - users: (B,), items: (N,)  -> broadcast users for all N items (returns (B,N))\n",
        "        \"\"\"\n",
        "        users = users.to(self.device)\n",
        "        items = items.to(self.device)\n",
        "\n",
        "        Eu, Ei = self.propagate()\n",
        "\n",
        "        if users.dim() == 1 and items.dim() == 1 and users.shape[0] == items.shape[0]:\n",
        "            u_emb = Eu[users]                   # (B, d)\n",
        "            i_emb = Ei[items]                   # (B, d)\n",
        "            return (u_emb * i_emb).sum(dim=1)   # (B,)\n",
        "\n",
        "        # Broadcast to (B, N)\n",
        "        u_emb = Eu[users]                       # (B, d)\n",
        "        i_emb = Ei[items]                       # (N, d)\n",
        "        scores = u_emb @ i_emb.t()              # (B, N)\n",
        "        return scores\n",
        "\n",
        "    def predict(self, user_ids: List[int], item_ids: Optional[List[int]] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convenience method to get scores as numpy.\n",
        "        - If item_ids is None: scores for all items for each user -> shape (B, n_items)\n",
        "        - Else: scores for user_ids x item_ids -> (B, len(item_ids))\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            u = torch.tensor(user_ids, dtype=torch.long, device=self.device)\n",
        "            if item_ids is None:\n",
        "                items = torch.arange(self.n_items, dtype=torch.long, device=self.device)\n",
        "                scores = self.forward(u, items)        # (B, n_items)\n",
        "            else:\n",
        "                items = torch.tensor(item_ids, dtype=torch.long, device=self.device)\n",
        "                scores = self.forward(u, items)        # (B, len(item_ids))\n",
        "        return scores.detach().cpu().numpy()\n",
        "\n",
        "    # ---------- Ranking API (BaseModel) ----------\n",
        "    def get_full_sort_items(self, user_id: int, seen_items: Optional[Iterable[int]] = None, top_k: Optional[int] = None) -> List[int]:\n",
        "        \"\"\"\n",
        "        Rank all items for a given user (descending by predicted score).\n",
        "        Optionally drop previously seen items.\n",
        "        \"\"\"\n",
        "        scores = self.predict([user_id], None).ravel()  # (n_items,)\n",
        "        if seen_items is not None:\n",
        "            # push seen items to bottom\n",
        "            seen_items = [i for i in seen_items if 0 <= i < self.n_items]\n",
        "            scores[np.array(seen_items, dtype=np.int64)] = -np.inf\n",
        "\n",
        "        order = np.argsort(-scores)  # descending\n",
        "        if top_k is not None:\n",
        "            order = order[:top_k]\n",
        "        return order.tolist()\n",
        "\n",
        "    def _sort_full_items(self, user_id: int, predicted_ratings: torch.Tensor, items: torch.Tensor):\n",
        "        # Not used here; keeping for BaseModel compatibility\n",
        "        _, idx = torch.sort(predicted_ratings, descending=True)\n",
        "        return items[idx]\n",
        "\n",
        "\n",
        "class BPRLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Pairwise Bayesian Personalized Ranking loss with L2 regularization on embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, reg: float = 1e-4):\n",
        "        super().__init__()\n",
        "        self.reg = reg\n",
        "\n",
        "    def forward(self, u_emb, pos_emb, neg_emb):\n",
        "        pos_scores = (u_emb * pos_emb).sum(dim=1)\n",
        "        neg_scores = (u_emb * neg_emb).sum(dim=1)\n",
        "        loss = -F.logsigmoid(pos_scores - neg_scores).mean()\n",
        "        reg = (u_emb.norm(2).pow(2) + pos_emb.norm(2).pow(2) + neg_emb.norm(2).pow(2)) / u_emb.shape[0]\n",
        "        return loss + self.reg * reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeNDA-kaCfrF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional, Tuple, Iterable, List, Dict\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that yields dense user vectors (torch.float32).\n",
        "    Accepts a scipy.sparse CSR matrix or a numpy array of shape (n_users, n_items).\n",
        "    \"\"\"\n",
        "    def __init__(self, user_item_matrix):\n",
        "        # Accept CSR matrix or numpy array\n",
        "        if hasattr(user_item_matrix, \"toarray\") and hasattr(user_item_matrix, \"tocsr\"):\n",
        "            self.mat = user_item_matrix.tocsr()\n",
        "            self.is_sparse = True\n",
        "            self.n_users, self.n_items = self.mat.shape\n",
        "        else:\n",
        "            self.mat = np.asarray(user_item_matrix, dtype=np.float32)\n",
        "            self.is_sparse = False\n",
        "            self.n_users, self.n_items = self.mat.shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_users\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_sparse:\n",
        "            row = self.mat.getrow(idx).toarray().astype(np.float32).squeeze(0)\n",
        "            return torch.from_numpy(row)\n",
        "        else:\n",
        "            return torch.from_numpy(self.mat[idx].astype(np.float32))\n",
        "\n",
        "\n",
        "class MultiVAE(BaseModel, nn.Module):\n",
        "    \"\"\"\n",
        "    MultiVAE model (variational autoencoder for collaborative filtering).\n",
        "    - encoder: two-layer MLP -> mu, logvar\n",
        "    - decoder: linear mapping from latent z to item logits\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: dict,\n",
        "        n_users: int,\n",
        "        n_items: int,\n",
        "        hidden_dim: int = 600,\n",
        "        latent_dim: int = 200,\n",
        "        dropout=0.5,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        BaseModel.__init__(self, config, n_users, n_items)\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "\n",
        "        # model dims\n",
        "        self.hidden_dim = config.get(\"vae_hidden_dim\", hidden_dim)\n",
        "        self.latent_dim = config.get(\"vae_latent_dim\", latent_dim)\n",
        "        self.dropout = config.get(\"vae_dropout\", dropout)\n",
        "\n",
        "        # encoder\n",
        "        self.encoder_fc1 = nn.Linear(n_items, self.hidden_dim)\n",
        "        self.encoder_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.mu_layer = nn.Linear(self.hidden_dim, self.latent_dim)\n",
        "        self.logvar_layer = nn.Linear(self.hidden_dim, self.latent_dim)\n",
        "\n",
        "        # decoder (linear + bias to produce logits over items)\n",
        "        self.decoder = nn.Linear(self.latent_dim, n_items, bias=True)\n",
        "\n",
        "        self.act = nn.Tanh()\n",
        "        self.drop = nn.Dropout(self.dropout)\n",
        "\n",
        "        # initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    # ---------- VAE ops ----------\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        x: (B, n_items) input user vectors\n",
        "        returns mu, logvar (each (B, latent_dim))\n",
        "        \"\"\"\n",
        "        h = self.drop(self.act(self.encoder_fc1(x)))\n",
        "        h = self.drop(self.act(self.encoder_fc2(h)))\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns logits for items: (B, n_items)\n",
        "        \"\"\"\n",
        "        logits = self.decoder(z)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sample: bool = True) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        One forward pass:\n",
        "        - encodes x -> mu, logvar\n",
        "        - draws z (reparameterization)\n",
        "        - decodes logits\n",
        "        Returns: logits, mu, logvar\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        if self.training and sample:\n",
        "            z = self.reparameterize(mu, logvar)\n",
        "        else:\n",
        "            z = mu  # use mean for deterministic inference\n",
        "        logits = self.decode(z)\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    # ---------- Loss / ELBO ----------\n",
        "    def loss_function(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        input_batch: torch.Tensor,\n",
        "        mu: torch.Tensor,\n",
        "        logvar: torch.Tensor,\n",
        "        anneal: float = 1.0,\n",
        "    ) -> Tuple[torch.Tensor, float, float]:\n",
        "        \"\"\"\n",
        "        Multinomial likelihood version used in MultiVAE:\n",
        "        recon_loss = - sum_j x_j * log_softmax(logits)_j\n",
        "\n",
        "        KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
        "        total_loss = recon_loss + anneal * KL\n",
        "        Returns (loss, recon_loss_scalar, kl_scalar)\n",
        "        \"\"\"\n",
        "        # reconstruction: log softmax and weighted by counts\n",
        "        log_softmax = F.log_softmax(logits, dim=1)  # (B, n_items)\n",
        "        # input_batch may be counts (0/1 or counts). multiply and sum per sample\n",
        "        recon = -torch.sum(log_softmax * input_batch, dim=1)  # (B,)\n",
        "        recon_loss = torch.mean(recon)\n",
        "\n",
        "        # KL\n",
        "        kl = -0.5 * torch.sum(1.0 + logvar - mu.pow(2) - logvar.exp(), dim=1)  # (B,)\n",
        "        kl_loss = torch.mean(kl)\n",
        "\n",
        "        loss = recon_loss + anneal * kl_loss\n",
        "        return loss, recon_loss.item(), kl_loss.item()\n",
        "\n",
        "\n",
        "    # ---------- Embeddings helper (for ranking similar to LightGCN propagate) ----------\n",
        "    def get_user_latent(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Return deterministic latent mu for x (no sampling).\n",
        "        x: (B, n_items)\n",
        "        returns mu: (B, latent_dim)\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        return mu\n",
        "\n",
        "    # ---------- Prediction / Ranking (BaseModel methods) ----------\n",
        "    def predict(self, user_vectors: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict item scores for a batch of user vectors.\n",
        "        user_vectors: torch.Tensor (B, n_items) on cpu or device\n",
        "        returns: numpy array (B, n_items) of logits (higher = more recommended)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            user_vectors = user_vectors.to(self.device)\n",
        "            logits, mu, logvar = self.forward(user_vectors, sample=False)\n",
        "            # Return logits (no softmax) so they can be sorted. move to cpu\n",
        "            return logits.detach().cpu().numpy()\n",
        "\n",
        "    def get_full_sort_items(self, user_id: int, user_item_matrix, seen_items: Optional[Iterable[int]] = None, top_k: Optional[int] = None) -> List[int]:\n",
        "        \"\"\"\n",
        "        Rank all items for a given user.\n",
        "        - user_item_matrix: the full user-item matrix (csr or numpy) to extract the user's vector\n",
        "        - seen_items: optional iterable to mask (push to bottom)\n",
        "        \"\"\"\n",
        "        # build user vector\n",
        "        if hasattr(user_item_matrix, \"getrow\"):\n",
        "            vec = user_item_matrix.getrow(user_id).toarray().astype(np.float32)\n",
        "            user_vec = torch.from_numpy(vec).to(self.device)\n",
        "        else:\n",
        "            user_vec = torch.from_numpy(np.asarray(user_item_matrix[user_id], dtype=np.float32)).to(self.device)\n",
        "\n",
        "        scores = self.predict(user_vec.unsqueeze(0)).ravel()  # (n_items,)\n",
        "\n",
        "        if seen_items is not None:\n",
        "            seen = [i for i in seen_items if 0 <= i < self.n_items]\n",
        "            scores[np.array(seen, dtype=np.int64)] = -np.inf\n",
        "\n",
        "        order = np.argsort(-scores)\n",
        "        if top_k is not None:\n",
        "            order = order[:top_k]\n",
        "        return order.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK1nGxngClJ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class FMModel(BaseModel, nn.Module):\n",
        "    def __init__(self, config, n_users, n_items, n_factors=16):\n",
        "        BaseModel.__init__(self, config, n_users, n_items)\n",
        "        nn.Module.__init__(self)\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "\n",
        "        # Linear terms\n",
        "        self.user_bias = nn.Embedding(n_users, 1)\n",
        "        self.item_bias = nn.Embedding(n_items, 1)\n",
        "\n",
        "        # Factorization embeddings\n",
        "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
        "        self.item_embedding = nn.Embedding(n_items, n_factors)\n",
        "\n",
        "        # Global bias\n",
        "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        # Init\n",
        "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
        "        nn.init.zeros_(self.user_bias.weight)\n",
        "        nn.init.zeros_(self.item_bias.weight)\n",
        "\n",
        "    def forward(self, user_ids, item_ids):\n",
        "        user_vec = self.user_embedding(user_ids)\n",
        "        item_vec = self.item_embedding(item_ids)\n",
        "\n",
        "        linear_terms = self.user_bias(user_ids) + self.item_bias(item_ids)\n",
        "        interaction = torch.sum(user_vec * item_vec, dim=1, keepdim=True)\n",
        "\n",
        "        score = self.global_bias + linear_terms + interaction\n",
        "        return score.view(-1)\n",
        "\n",
        "    def get_full_sort_items(self, user_id):\n",
        "        device = next(self.parameters()).device\n",
        "        user_id_tensor = torch.tensor([user_id], device=device)\n",
        "        item_ids = torch.arange(self.n_items, device=device)\n",
        "        user_id_expand = user_id_tensor.expand(self.n_items)\n",
        "        scores = self.forward(user_id_expand, item_ids)\n",
        "        return torch.argsort(scores, descending=True).tolist()\n",
        "\n",
        "    def _sort_full_items(self, user_id):\n",
        "        return self.get_full_sort_items(user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf9s2A4CCp5T"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "# ------------------------------\n",
        "# Utility: set all seeds\n",
        "# ------------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def sasrec_pointwise_step(model, batch, device, logit_clip=20.0):\n",
        "    users, seqs, pos_items, neg_items, mask = batch\n",
        "    seqs, pos_items, neg_items = seqs.to(device), pos_items.to(device), neg_items.to(device)\n",
        "    mask = mask.to(device).bool()\n",
        "\n",
        "    seq_out = model(seqs)\n",
        "    item_emb = model.item_embedding.weight\n",
        "\n",
        "    pos_logits = torch.sum(seq_out * item_emb[pos_items], dim=-1).clamp(-logit_clip, logit_clip)\n",
        "    neg_logits = torch.sum(seq_out * item_emb[neg_items], dim=-1).clamp(-logit_clip, logit_clip)\n",
        "\n",
        "    valid_mask = (pos_items > 0) & mask\n",
        "    if valid_mask.sum() == 0:\n",
        "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
        "\n",
        "    loss_pos = F.binary_cross_entropy_with_logits(pos_logits[valid_mask], torch.ones_like(pos_logits[valid_mask]))\n",
        "    loss_neg = F.binary_cross_entropy_with_logits(neg_logits[valid_mask], torch.zeros_like(neg_logits[valid_mask]))\n",
        "\n",
        "    loss = loss_pos + loss_neg\n",
        "\n",
        "    # L2 reg, skip padding embedding\n",
        "    if model.l2_emb > 0:\n",
        "        loss += model.l2_emb * (item_emb[1:].norm(p=2) ** 2) / 2\n",
        "\n",
        "    return loss\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset & Collate\n",
        "# ------------------------------\n",
        "class SASRecDataset(Dataset):\n",
        "    \"\"\"Builds sequences for SASRec training.\n",
        "\n",
        "    Args:\n",
        "        user2items: dict mapping user_id -> list of interacted item ids (in time order)\n",
        "        n_items: total number of items (max ID)\n",
        "        max_seq_len: truncate/pad sequences to this length\n",
        "        min_seq_len: smallest effective length (>=2 to have a next item)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 user2items: Dict[int, List[int]],\n",
        "                 n_items: int,\n",
        "                 max_seq_len: int = 50,\n",
        "                 min_seq_len: int = 2):\n",
        "        self.user2items = user2items\n",
        "        self.users = [u for u, seq in user2items.items() if len(seq) >= min_seq_len]\n",
        "        self.n_items = n_items\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        user = self.users[idx]\n",
        "        full = self.user2items[user]\n",
        "        # Truncate to latest max_seq_len\n",
        "        seq = full[-self.max_seq_len:]\n",
        "        return user, seq\n",
        "\n",
        "\n",
        "def _pad_sequence(seq: list, max_len: int) -> list:\n",
        "    \"\"\"Left-pad sequence with 0 to max_len.\"\"\"\n",
        "    seq = seq[-max_len:]\n",
        "    return [0] * (max_len - len(seq)) + seq\n",
        "\n",
        "def _build_pos_items(seq: list) -> list:\n",
        "    \"\"\"Next-item targets, last position padded with 0.\"\"\"\n",
        "    return seq[1:] + [0]\n",
        "\n",
        "\n",
        "def _sample_negatives(seq: list, n_items: int) -> list:\n",
        "    \"\"\"Sample negatives for each position, avoiding seq items.\"\"\"\n",
        "    user_set = set(seq)\n",
        "    negatives = []\n",
        "    for _ in range(len(seq)):\n",
        "        neg = random.randint(1, n_items - 1)  # avoid 0\n",
        "        while neg in user_set:\n",
        "            neg = random.randint(1, n_items - 1)\n",
        "        negatives.append(neg)\n",
        "    return negatives\n",
        "\n",
        "\n",
        "def sasrec_collate(batch, n_items: int, max_seq_len: int):\n",
        "    users, seqs = zip(*batch)\n",
        "    seqs = [_pad_sequence(s, max_seq_len) for s in seqs]\n",
        "\n",
        "    pos_items = [_build_pos_items(s) for s in seqs]\n",
        "    neg_items = [_sample_negatives(s, n_items) for s in seqs]\n",
        "    mask = [[1 if x != 0 else 0 for x in s] for s in seqs]\n",
        "\n",
        "    return (\n",
        "        torch.tensor(users, dtype=torch.long),\n",
        "        torch.tensor(seqs, dtype=torch.long),\n",
        "        torch.tensor(pos_items, dtype=torch.long),\n",
        "        torch.tensor(neg_items, dtype=torch.long),\n",
        "        torch.tensor(mask, dtype=torch.float),\n",
        "    )\n",
        "\n",
        "\n",
        "def build_user2items(train_data):\n",
        "    user2items = defaultdict(list)\n",
        "    for u, i, r in sorted(train_data, key=lambda x: (x[0], x[2])):\n",
        "        user2items[u].append(i)\n",
        "    return user2items\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Model\n",
        "# ------------------------------\n",
        "class SASRec(nn.Module, BaseModel):\n",
        "    def __init__(self, config, n_users: int, n_items: int):\n",
        "        nn.Module.__init__(self)  # initialize nn.Module\n",
        "        BaseModel.__init__(self, config, n_users, n_items)  # keep BaseModel logic\n",
        "\n",
        "        self.n_items = n_items\n",
        "        self.hidden_units = int(config.get(\"hidden_units\", 128))\n",
        "        self.max_seq_len = int(config.get(\"max_seq_len\", 50))\n",
        "        self.num_heads = int(config.get(\"num_heads\", 2))\n",
        "        self.num_blocks = int(config.get(\"num_blocks\", 2))\n",
        "        self.dropout_rate = float(config.get(\"dropout_rate\", 0.2))\n",
        "        self.l2_emb = float(config.get(\"l2_emb\", 0.0))\n",
        "\n",
        "        # Embeddings\n",
        "        self.item_embedding = nn.Embedding(n_items + 1, self.hidden_units, padding_idx=0)\n",
        "        self.position_embedding = nn.Embedding(self.max_seq_len, self.hidden_units)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=self.hidden_units,\n",
        "                nhead=self.num_heads,\n",
        "                dim_feedforward=self.hidden_units * 4,\n",
        "                dropout=self.dropout_rate,\n",
        "                activation=\"gelu\",\n",
        "                batch_first=True,\n",
        "            ) for _ in range(self.num_blocks)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(self.hidden_units)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.normal_(self.item_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.position_embedding.weight, std=0.02)\n",
        "\n",
        "    def _causal_mask(self, L: int, device=None):\n",
        "        # True means masked in PyTorch\n",
        "        mask = torch.triu(torch.ones(L, L, device=device), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, item_seq: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode sequence safely, with padding & causal masks.\n",
        "        Args:\n",
        "            item_seq: (B, L) padded with 0 on left\n",
        "        Returns:\n",
        "            seq_out: (B, L, H)\n",
        "        \"\"\"\n",
        "        B, L = item_seq.shape\n",
        "        device = item_seq.device\n",
        "        pos_ids = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
        "\n",
        "        # Embeddings\n",
        "        x = self.item_embedding(item_seq) + self.position_embedding(pos_ids)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        key_padding_mask = (item_seq == 0)  # True = pad\n",
        "        attn_mask = self._causal_mask(L, device=device)  # True = masked\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(\n",
        "            x,\n",
        "            src_mask=attn_mask,\n",
        "            src_key_padding_mask=key_padding_mask\n",
        "            )\n",
        "            # Safety: replace NaN/inf\n",
        "            if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "               x = torch.nan_to_num(x, nan=0.0, posinf=1e4, neginf=-1e4)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_full_sort_items(self, user_id, user_seq: torch.Tensor):\n",
        "        \"\"\"Return sorted item IDs by score for a single user.\n",
        "        Args:\n",
        "            user_seq: (1, L)\n",
        "        Returns:\n",
        "            torch.Tensor of sorted item ids (desc)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        seq_out = self.forward(user_seq)[:, -1, :]  # (1, H)\n",
        "        all_item_emb = self.item_embedding.weight  # (n_items+1, H)\n",
        "        scores = torch.matmul(seq_out, all_item_emb.t()).squeeze(0)  # (n_items+1,)\n",
        "        # Avoid recommending padding id 0\n",
        "        scores[0] = -1e9\n",
        "        return torch.argsort(scores, descending=True)\n",
        "\n",
        "    def _sort_full_items(self, user_id, *args, **kwargs):\n",
        "        raise NotImplementedError(\"Use get_full_sort_items(user_id, user_seq)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WExetJsyCutt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import spearmanr\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class Recommender:\n",
        "    \"\"\"\n",
        "    Recommender System class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, logger, data):\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.page_size = config[\"page_size\"]\n",
        "        self.items_per_page = config[\"items_per_page\"]\n",
        "        self.random_k = config[\"rec_random_k\"]\n",
        "        self.train_data = []\n",
        "        self.n_layers = 3\n",
        "        self.embedding_dim = 4\n",
        "        if config[\"rec_model\"] == \"MF\":\n",
        "           self.model = MF(config, self.data.get_user_num(), self.data.get_item_num())\n",
        "        elif config[\"rec_model\"] == \"LightGCN\":\n",
        "           self.model = LightGCN(config, self.data.get_user_num(), self.data.get_item_num(), self.train_data, n_layers=self.n_layers, embedding_dim=self.embedding_dim)\n",
        "        elif config[\"rec_model\"] == \"SASRec\":\n",
        "           self.model = SASRec(config, self.data.get_user_num(), self.data.get_item_num())\n",
        "        elif config[\"rec_model\"] == \"MultiVAE\":\n",
        "           self.model = MultiVAE(config, self.data.get_user_num(), self.data.get_item_num())\n",
        "        elif config[\"rec_model\"] == \"FM\":\n",
        "           self.model = FMModel(config, self.data.get_user_num(), self.data.get_item_num())\n",
        "        else:\n",
        "           raise ValueError(f\"Unknown model: {config['rec_model']}\")\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        if config[\"rec_model\"] != \"FM\":\n",
        "           self.optimizer = optim.Adam(self.model.parameters(), lr=config[\"lr\"], weight_decay=1e-5)\n",
        "\n",
        "        self.epoch_num = config[\"epoch_num\"]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.record = {}\n",
        "        self.round_record = {}\n",
        "        self.positive = {}\n",
        "        self.interaction_dict = {}\n",
        "        self.inter_df = None\n",
        "        self.inter_num = 0\n",
        "        for user in self.data.get_full_users():\n",
        "            self.record[user] = []\n",
        "            self.positive[user] = []\n",
        "            self.round_record[user] = []\n",
        "        self.user_data = {\n",
        "            \"user\": [],\n",
        "            \"N_expose\": [],\n",
        "            \"N_view\": [],\n",
        "            \"N_like\": [],\n",
        "            \"N_exit\": [],\n",
        "            \"S_sat\": []\n",
        "            }\n",
        "        self.rating_feeling = {\n",
        "            \"User\": [],\n",
        "            \"Rating\": [],\n",
        "            \"Feelings\": []\n",
        "        }\n",
        "\n",
        "    def sample_bpr_triples(self,\n",
        "                           user_pos_items: List[List[int]],\n",
        "                           n_items: int,\n",
        "                           batch_size: int,\n",
        "                           device: torch.device,\n",
        "                           ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        user_pos_items: list of lists; for each user u, a list of items they've interacted with (positive)\n",
        "        returns (users, pos_items, neg_items) tensors of length batch_size\n",
        "        \"\"\"\n",
        "        users = []\n",
        "        pos = []\n",
        "        neg = []\n",
        "        for _ in range(batch_size):\n",
        "           # sample a user with at least one positive\n",
        "           while True:\n",
        "               u = np.random.randint(0, len(user_pos_items))\n",
        "               if len(user_pos_items[u]) > 0:\n",
        "                  break\n",
        "\n",
        "           i = np.random.choice(user_pos_items[u])\n",
        "           # sample a negative item\n",
        "\n",
        "           while True:\n",
        "               j = np.random.randint(0, n_items)\n",
        "               if j not in user_pos_items[u]:\n",
        "                  break\n",
        "\n",
        "           users.append(u)\n",
        "           pos.append(i)\n",
        "           neg.append(j)\n",
        "\n",
        "        return (\n",
        "        torch.tensor(users, dtype=torch.long, device=self.device),\n",
        "        torch.tensor(pos, dtype=torch.long, device=self.device),\n",
        "        torch.tensor(neg, dtype=torch.long, device=self.device),\n",
        "        )\n",
        "\n",
        "    def train_lightgcn_bpr(self,\n",
        "        reg: float = 1e-4,\n",
        "        log: bool = True):\n",
        "        \"\"\"\n",
        "        Train LightGCN with BPR loss.\n",
        "        - train_interactions/val_interactions can be (u,i) or (u,i,rating>0) tuples.\n",
        "        - If ckpt_path is provided, saves the best (by simple val recall proxy) state dict.\n",
        "        \"\"\"\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config[\"lr\"])\n",
        "        self.criterion = BPRLoss(reg=reg)\n",
        "\n",
        "        # Build user -> positives list\n",
        "        user_pos = [[] for _ in range(self.model.n_users)]\n",
        "        for u, i, *rest in self.train_data:\n",
        "            if 0 <= u < self.model.n_users and 0 <= i < self.model.n_items:\n",
        "               user_pos[u].append(i)\n",
        "\n",
        "        # Basic validation proxy: count of positives ranked in top-10 (cheap & optional)\n",
        "        def quick_val_topk_hits(k: int = 10) -> float:\n",
        "            if self.val_data is None:\n",
        "               return -1.0\n",
        "            # build val positives per user\n",
        "            val_pos = [[] for _ in range(self.model.n_users)]\n",
        "            for u, i, *rest in self.val_data:\n",
        "               if 0 <= u < self.model.n_users and 0 <= i < self.model.n_items:\n",
        "                   val_pos[u].append(i)\n",
        "\n",
        "            hits = 0\n",
        "            total = 0\n",
        "\n",
        "            for u in range(self.model.n_users):\n",
        "               if not val_pos[u]:\n",
        "                  continue\n",
        "               recs = self.model.get_full_sort_items(u, seen_items=set(user_pos[u]), top_k=k)\n",
        "               s = set(val_pos[u])\n",
        "               hits += len([r for r in recs if r in s])\n",
        "               total += min(k, len(s))\n",
        "            return hits / total if total > 0 else -1.0\n",
        "\n",
        "        best_metric = -math.inf\n",
        "\n",
        "        # Build full checkpoint file path\n",
        "        ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_lightGCN_model.pth\")\n",
        "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "        for epoch in range(1, self.epoch_num + 1):\n",
        "            self.model.train()\n",
        "\n",
        "            # One epoch of mini-batch BPR\n",
        "            n_steps = max(1, sum(len(v) for v in user_pos) // max(1, config['batch_size']))\n",
        "            losses = []\n",
        "            for _ in range(n_steps):\n",
        "                users, pos_items, neg_items = self.sample_bpr_triples(user_pos, self.model.n_items, config['batch_size'], self.device)\n",
        "                Eu, Ei = self.model.propagate()\n",
        "                u_emb = Eu[users]\n",
        "                p_emb = Ei[pos_items]\n",
        "                n_emb = Ei[neg_items]\n",
        "                loss = self.criterion(u_emb, p_emb, n_emb)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # quick validation metric\n",
        "            metric = quick_val_topk_hits(k=10)\n",
        "            if log:\n",
        "               print(f\"[Epoch {epoch:3d}] BPR Loss: {np.mean(losses):.4f} | Val@10: {metric:.4f}\")\n",
        "\n",
        "            # Save checkpoint if validation improves\n",
        "            if metric > best_metric:\n",
        "               best_metric = metric\n",
        "               torch.save({\n",
        "                   \"epoch\": self.epoch_num + 1,\n",
        "                   \"model_state_dict\": self.model.state_dict(),\n",
        "                   \"n_users\": self.data.get_user_num(),\n",
        "                   \"n_items\": self.data.get_item_num(),\n",
        "                   \"n_layers\": self.n_layers,\n",
        "                   \"embedding_dim\": self.embedding_dim,\n",
        "                   \"metric\": metric,\n",
        "                   }, ckpt_file)\n",
        "               self.logger.info(f\"Best model updated at epoch {epoch+1}, saved to {ckpt_file}\")\n",
        "\n",
        "        # Load best (optional)\n",
        "        if ckpt_file is not None and best_metric > -math.inf:\n",
        "           # At the end, reload the best weights for inference\n",
        "           checkpoint = torch.load(ckpt_file)\n",
        "           self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    def bce_sampled_loss(self, seq_out: torch.Tensor,\n",
        "        pos_items: torch.Tensor,\n",
        "        neg_items: torch.Tensor,\n",
        "        item_embedding: nn.Embedding,\n",
        "        mask: torch.Tensor,\n",
        "        l2_emb: float = 0.0) -> torch.Tensor:\n",
        "        \"\"\"Binary cross-entropy on sampled positives/negatives per position.\n",
        "        Args:\n",
        "            seq_out: (B, L, H)\n",
        "            pos_items: (B, L) next-item ids (0 where no target)\n",
        "            neg_items: (B, L) sampled negatives (0 where no target)\n",
        "            item_embedding: embedding module (to fetch item vectors)\n",
        "            mask: (B, L) boolean, True where a target exists (i.e., pos_items > 0)\n",
        "            l2_emb: weight decay on item embeddings (regularizes pos/neg lookups)\n",
        "        \"\"\"\n",
        "        B, L, H = seq_out.shape\n",
        "\n",
        "        pos_vecs = item_embedding(pos_items)  # (B, L, H)\n",
        "        neg_vecs = item_embedding(neg_items)  # (B, L, H)\n",
        "\n",
        "        pos_logits = (seq_out * pos_vecs).sum(-1)  # (B, L)\n",
        "        neg_logits = (seq_out * neg_vecs).sum(-1)  # (B, L)\n",
        "\n",
        "        # Targets: pos -> 1, neg -> 0\n",
        "        pos_loss = F.binary_cross_entropy_with_logits(pos_logits[mask], torch.ones_like(pos_logits[mask]))\n",
        "        neg_loss = F.binary_cross_entropy_with_logits(neg_logits[mask], torch.zeros_like(neg_logits[mask]))\n",
        "        loss = pos_loss + neg_loss\n",
        "\n",
        "        if l2_emb > 0:\n",
        "           reg = (pos_vecs[mask].pow(2).sum() + neg_vecs[mask].pow(2).sum()) / mask.sum().clamp_min(1)\n",
        "           loss = loss + l2_emb * reg\n",
        "        return loss\n",
        "\n",
        "    def train_sasrec(self, grad_clip=1.0, logit_clip=20.0):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Build user->items dict and dataset\n",
        "        user2items = build_user2items(self.train_data)\n",
        "        n_items_global = int(self.data.get_item_num())\n",
        "        max_seq_len = self.model.max_seq_len\n",
        "\n",
        "        train_dataset = SASRecDataset(user2items, n_items=n_items_global, max_seq_len=max_seq_len)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda batch: sasrec_collate(batch, n_items=n_items_global, max_seq_len=max_seq_len)\n",
        "            )\n",
        "\n",
        "        # Validation loader\n",
        "        val_user2items = build_user2items(self.val_data)\n",
        "        val_dataset = SASRecDataset(val_user2items, n_items=n_items_global, max_seq_len=max_seq_len)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda batch: sasrec_collate(batch, n_items=n_items_global, max_seq_len=max_seq_len)\n",
        "            )\n",
        "\n",
        "        # Checkpoint setup\n",
        "        ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_SASRec_model.pth\")\n",
        "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
        "        best_metric = -float(\"inf\")\n",
        "\n",
        "        for epoch in range(1, self.epoch_num + 1):\n",
        "           self.model.train()\n",
        "           running = 0.0\n",
        "           n_steps = 0\n",
        "\n",
        "           for users, seqs, pos, neg, umask in train_loader:\n",
        "              users, seqs, pos, neg, umask = (\n",
        "                users.to(self.device),\n",
        "                seqs.to(self.device),\n",
        "                pos.to(self.device),\n",
        "                neg.to(self.device),\n",
        "                umask.to(self.device).bool(),  # ensure bool type\n",
        "                )\n",
        "\n",
        "              # Compute mask dynamically\n",
        "              mask = pos > 0\n",
        "              mask = mask.bool()\n",
        "              if mask.sum() == 0:\n",
        "                 continue  # skip batch with no valid positions\n",
        "\n",
        "              # Compute stable loss\n",
        "              loss = sasrec_pointwise_step(self.model, (users, seqs, pos, neg, mask), device=self.device, logit_clip=logit_clip)\n",
        "\n",
        "              self.optimizer.zero_grad(set_to_none=True)\n",
        "              loss.backward()\n",
        "              if grad_clip is not None:\n",
        "                 nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "              self.optimizer.step()\n",
        "\n",
        "              running += loss.item()\n",
        "              n_steps += 1\n",
        "\n",
        "           avg_loss = running / max(1, n_steps)\n",
        "           print(f\"Epoch {epoch}/{self.epoch_num} - train loss: {avg_loss:.4f}\")\n",
        "\n",
        "           # Validation\n",
        "           if val_loader is not None:\n",
        "              self.model.eval()\n",
        "              val_loss = 0.0\n",
        "              n_val_steps = 0\n",
        "              with torch.no_grad():\n",
        "                  for users, seqs, pos, neg, umask in val_loader:\n",
        "                    #  print(\"Users:\", users)\n",
        "                    #  print(\"Pos min/max:\", pos.min().item(), pos.max().item())\n",
        "                    #  print(\"Neg min/max:\", neg.min().item(), neg.max().item())\n",
        "                     users, seqs, pos, neg, umask = (\n",
        "                         users.to(self.device),\n",
        "                         seqs.to(self.device),\n",
        "                         pos.to(self.device),\n",
        "                         neg.to(self.device),\n",
        "                         umask.to(self.device).bool(),)\n",
        "\n",
        "                     mask = pos > 0\n",
        "                     if mask.sum() == 0:\n",
        "                        # print(\"Skipped empty batch\")\n",
        "                        continue\n",
        "\n",
        "                     loss = sasrec_pointwise_step(self.model, (users, seqs, pos, neg, mask), device=self.device, logit_clip=logit_clip)\n",
        "                    #  print(\"Batch loss:\", loss.item())\n",
        "                     val_loss += loss.item()\n",
        "                     n_val_steps += 1\n",
        "              val_loss /= max(1, n_val_steps)\n",
        "              metric = -val_loss\n",
        "              print(f\"Epoch {epoch}, Val Loss: {val_loss:.4f}\")\n",
        "           else:\n",
        "              metric = -avg_loss\n",
        "\n",
        "           # Save best model\n",
        "           if metric > best_metric:\n",
        "              best_metric = metric\n",
        "              torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"metric\": metric,\n",
        "              }, ckpt_file)\n",
        "              print(f\"Best model updated at epoch {epoch}, saved to {ckpt_file}\")\n",
        "\n",
        "        # Load best model after training\n",
        "        checkpoint = torch.load(ckpt_file, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "\n",
        "    def train_fm(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config['lr'])\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        best_metric = -math.inf\n",
        "        ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_FM_model.pth\")\n",
        "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "        # 🔹 Wrap datasets into DataLoaders\n",
        "        train_loader = DataLoader(self.train_data, batch_size=self.config['batch_size'], shuffle=True)\n",
        "        val_loader = None\n",
        "        if self.val_data is not None:\n",
        "           val_loader = DataLoader(self.val_data, batch_size=self.config['batch_size'], shuffle=False)\n",
        "\n",
        "\n",
        "        for epoch in range(self.epoch_num):\n",
        "           self.model.train()\n",
        "           total_loss = 0\n",
        "\n",
        "           for user, item, rating in train_loader:  # must be DataLoader\n",
        "              user, item, rating = user.to(self.device), item.to(self.device), rating.float().to(self.device)\n",
        "\n",
        "              self.optimizer.zero_grad()\n",
        "              preds = self.model(user, item)\n",
        "              loss = self.criterion(preds, rating)\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "\n",
        "              total_loss += loss.item()\n",
        "\n",
        "           avg_loss = total_loss / len(train_loader)\n",
        "           print(f\"Epoch {epoch+1}/{self.epoch_num}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "           metric = None\n",
        "           if val_loader is not None:\n",
        "              self.model.eval()\n",
        "              with torch.no_grad():\n",
        "                  val_loss = 0\n",
        "                  for user, item, rating in val_loader:\n",
        "                      user, item, rating = user.to(self.device), item.to(self.device), rating.to(self.device)\n",
        "                      preds = self.model(user, item)\n",
        "                      val_loss += self.criterion(preds, rating).item()\n",
        "                  val_loss /= len(val_loader)\n",
        "\n",
        "              metric = -val_loss\n",
        "              print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
        "           else:\n",
        "              metric = -avg_loss  # fallback if no validation set\n",
        "\n",
        "           # save best model\n",
        "           if metric > best_metric:\n",
        "              best_metric = metric\n",
        "              torch.save({\n",
        "                \"epoch\": epoch+1,\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"n_users\": self.model.n_users,\n",
        "                \"n_items\": self.model.n_items,\n",
        "                \"n_factors\": self.model.n_factors,\n",
        "                \"metric\": metric,\n",
        "                }, ckpt_file)\n",
        "              print(f\"Best model updated at epoch {epoch+1}, saved to {ckpt_file}\")\n",
        "\n",
        "        checkpoint = torch.load(ckpt_file, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "\n",
        "    def train_multivae(self,\n",
        "        weight_decay: float = 0.0,\n",
        "        anneal_cap: float = 0.2,\n",
        "        total_anneal_steps: int = 200000,\n",
        "        patience: int = 100,\n",
        "        verbose: bool = True,):\n",
        "        \"\"\"\n",
        "        Train MultiVAE on user-item matrix (CSR or dense numpy).\n",
        "        - anneal_cap: maximum beta for KL weighting\n",
        "        - total_anneal_steps: number of optimization steps over which to ramp beta from 0->anneal_cap\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        dataset = InteractionDataset(self.train_matrix)\n",
        "        loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.5)\n",
        "        update_count = 0\n",
        "        best_val_loss = float(\"inf\")\n",
        "\n",
        "        # Build full checkpoint file path\n",
        "        ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_MultiVAE_model.pth\")\n",
        "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "        wait = 0\n",
        "\n",
        "        # AMP scaler (for mixed precision training)\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "        # validation dataset (create once, not per epoch)\n",
        "        val_loader, val_dataset = None, None\n",
        "        if self.val_matrix is not None:\n",
        "           val_dataset = InteractionDataset(self.val_matrix)\n",
        "           val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], shuffle=False)\n",
        "\n",
        "        for epoch in range(1, self.epoch_num + 1):\n",
        "            self.model.train()\n",
        "            epoch_loss, epoch_recon, epoch_kl, n_batches = 0.0, 0.0, 0.0, 0\n",
        "\n",
        "            for batch in loader:\n",
        "                batch = batch.to(self.device).float()\n",
        "                assert batch.shape[1] == self.model.n_items, \"Batch dimension mismatch!\"\n",
        "\n",
        "                # anneal factor\n",
        "                if total_anneal_steps > 0:\n",
        "                   anneal = min(anneal_cap, update_count / total_anneal_steps)\n",
        "                else:\n",
        "                   anneal = anneal_cap\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                    logits, mu, logvar = self.model(batch, sample=True)\n",
        "                    # clamp logvar inside model (optional, numerical stability)\n",
        "                    logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "                    loss, recon_l, kl_l = self.model.loss_function(logits, batch, mu, logvar, anneal)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_recon += recon_l\n",
        "                epoch_kl += kl_l\n",
        "                update_count += 1\n",
        "                n_batches += 1\n",
        "\n",
        "            scheduler.step()\n",
        "            avg_train_elbo = epoch_loss / n_batches\n",
        "\n",
        "            if verbose:\n",
        "               print(f\"[Epoch {epoch}] Train ELBO: {avg_train_elbo:.4f} | \"\n",
        "                  f\"Recon: {epoch_recon / n_batches:.4f} | KL: {epoch_kl / n_batches:.4f} | \"\n",
        "                  f\"Anneal: {anneal:.4f}\")\n",
        "\n",
        "            # ---------- Validation ----------\n",
        "            if val_loader is not None:\n",
        "               self.model.eval()\n",
        "               val_losses = []\n",
        "               with torch.no_grad():\n",
        "                   for vb in val_loader:\n",
        "                      vb = vb.to(self.device).float()\n",
        "                      logits, mu, logvar = self.model(vb, sample=False)\n",
        "                      logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "                      vloss, vrec, vkl = self.model.loss_function(logits, vb, mu, logvar, anneal)\n",
        "                      val_losses.append(vloss.item() * len(vb))\n",
        "\n",
        "               val_loss = float(np.sum(val_losses) / len(val_dataset))\n",
        "\n",
        "               if verbose:\n",
        "                 print(f\"  -> Val ELBO: {val_loss:.4f}\")\n",
        "\n",
        "               # save best with early stopping\n",
        "               if val_loss < best_val_loss:\n",
        "                  best_val_loss = val_loss\n",
        "                  wait = 0\n",
        "                  torch.save({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": self.model.state_dict(),\n",
        "                    \"optimizer\": self.optimizer.state_dict(),\n",
        "                    \"n_users\": self.data.get_user_num(),\n",
        "                    \"n_items\": self.data.get_item_num(),\n",
        "                    \"hidden_dim\": self.config[\"vae_hidden_dim\"],\n",
        "                    \"latent_dim\": self.config[\"vae_latent_dim\"],\n",
        "                    \"config\": self.config,\n",
        "                    }, ckpt_file)\n",
        "                  self.logger.info(f\"Best model updated at epoch {epoch}, saved to {ckpt_file}\")\n",
        "               else:\n",
        "                  wait += 1\n",
        "                  if wait >= patience:\n",
        "                     print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "                     break\n",
        "\n",
        "        # reload best model\n",
        "        checkpoint = torch.load(ckpt_file, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "\n",
        "    def swap_items(self, lst, page_size, random_k):\n",
        "        total_pages = len(lst) // page_size\n",
        "        lst = lst[: total_pages * page_size]\n",
        "        for page in range(1, total_pages // 2 + 1):\n",
        "            start_idx = (page - 1) * page_size\n",
        "            end_idx = start_idx + page_size - 1\n",
        "            symmetric_start_idx = (total_pages - page) * page_size\n",
        "            symmetric_end_idx = symmetric_start_idx + page_size\n",
        "\n",
        "            for k in range(1, random_k + 1):\n",
        "                lst[end_idx - k], lst[symmetric_end_idx - k] = (\n",
        "                    lst[symmetric_end_idx - k],\n",
        "                    lst[end_idx - k],\n",
        "                )\n",
        "\n",
        "        return lst\n",
        "\n",
        "    def add_random_items(self, user, item_ids):\n",
        "        item_ids = self.swap_items(item_ids, self.page_size, self.random_k)\n",
        "        return item_ids\n",
        "\n",
        "    def ordered_probit_loglik(self, y_true, y_pred_int, K=5, taus=None):\n",
        "        \"\"\"\n",
        "        Compute log-likelihood for ordered probit model given integer predictions.\n",
        "\n",
        "        y_true : list or array\n",
        "           True ratings (1..K).\n",
        "        y_pred_int : list or array\n",
        "           Predicted integer ratings (1..K).\n",
        "        K : int\n",
        "           Number of rating categories (default 5).\n",
        "        taus : list or array, optional\n",
        "           Thresholds (default: equally spaced).\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(y_true) == len(y_pred_int), \"Mismatch in true vs predicted length\"\n",
        "        ll = 0.0\n",
        "        for t, p in zip(y_true, y_pred_int):\n",
        "           probs = self.data.ordered_probit_probs(p, K, taus)\n",
        "           ll += self.data.safe_log(probs[t-1])  # subtract 1 for 0-based index\n",
        "        avg_ll = ll / len(y_true)\n",
        "        return ll, avg_ll\n",
        "\n",
        "    def update_user_interactions(self, user_id, new_items):\n",
        "        \"\"\"\n",
        "        Updates the directory of user_id and interacted_items.\n",
        "        - interaction_dict: dict mapping user_id -> set of interacted item ids\n",
        "        - user_id: int or str\n",
        "        - new_items: iterable of item ids (list, set, etc)\n",
        "\n",
        "        After calling, interaction_dict[user_id] contains all unique interacted items.\n",
        "        \"\"\"\n",
        "        # Ensure the user's interaction set exists\n",
        "        if user_id not in self.interaction_dict:\n",
        "          self.interaction_dict[user_id] = set()\n",
        "\n",
        "        new_items = set(new_items) - self.interaction_dict[user_id]\n",
        "        self.interaction_dict[user_id].update(new_items)\n",
        "\n",
        "    def get_full_manual_items(self, user_id, gt_ratio, rd_ratio, total_items, read = None, heard = None):\n",
        "        \"\"\"\n",
        "        Get a list of manual items for a given user.\n",
        "        \"\"\"\n",
        "        gtruth_items = self.data.interrating[user_id]\n",
        "        gt_items = [item for item, rating in gtruth_items]\n",
        "        rd_items = self.data.get_full_items()\n",
        "\n",
        "        # Remove any gt_items from rd_items to avoid duplicates\n",
        "        rd_items = list(set(rd_items) - set(gt_items))\n",
        "\n",
        "        # 1. Determine counts\n",
        "        total_ratio = gt_ratio + rd_ratio\n",
        "        gt_count = round(total_items * gt_ratio / total_ratio)\n",
        "        rd_count = total_items - gt_count\n",
        "\n",
        "        # Make sure we don't try to sample more than available\n",
        "        gt_count = min(gt_count, len(gt_items))\n",
        "        rd_count = min(rd_count, len(rd_items))\n",
        "\n",
        "        # 2. Randomly sample\n",
        "        chosen_gt = random.sample(gt_items, gt_count) if gt_count > 0 else []\n",
        "        chosen_rd = random.sample(rd_items, rd_count) if rd_count > 0 else []\n",
        "\n",
        "        # 3. Combine and shuffle if desired\n",
        "        final_items = chosen_gt + chosen_rd\n",
        "\n",
        "        # items discriptions\n",
        "        sorted_item_names = self.data.get_item_names(final_items)\n",
        "        description = self.data.get_item_description_by_id(final_items)\n",
        "        eb_item = [\n",
        "            sorted_item_names[i]\n",
        "            + \";;\"\n",
        "            + description[i]\n",
        "            + \";; Genre: \"\n",
        "            + self.data.get_genres_by_id([final_items[i]])[0]\n",
        "            for i in range(len(sorted_item_names))\n",
        "        ]\n",
        "        return final_items, eb_item, chosen_gt\n",
        "\n",
        "    def get_full_sort_items(self, user, random=False):\n",
        "        \"\"\"\n",
        "        Get a list of sorted items for a given user.\n",
        "        \"\"\"\n",
        "        items = self.data.get_full_items()\n",
        "        user_tensor = torch.tensor(user)\n",
        "        items_tensor = torch.tensor(items)\n",
        "        sorted_items = self.model.get_full_sort_items(user_tensor, items_tensor)\n",
        "        if self.random_k > 0 and random == True:\n",
        "            sorted_items = self.add_random_items(user, sorted_items)\n",
        "        sorted_items = [item for item in sorted_items if item not in self.record[user]]\n",
        "        sorted_item_names = self.data.get_item_names(sorted_items)\n",
        "        description = self.data.get_item_description_by_id(sorted_items)\n",
        "        items = [\n",
        "            sorted_item_names[i]\n",
        "            + \";;\"\n",
        "            + description[i]\n",
        "            + \";; Genre: \"\n",
        "            + self.data.get_genres_by_id([sorted_items[i]])[0]\n",
        "            for i in range(len(sorted_item_names))\n",
        "        ]\n",
        "        return sorted_items, items\n",
        "\n",
        "    def get_item(self, idx):\n",
        "        item_name = self.data.get_item_names([idx])[0]\n",
        "        description = self.data.get_item_description_by_id([idx])[0]\n",
        "        item = item_name + \";;\" + description\n",
        "        return item\n",
        "\n",
        "    def get_search_items(self, item_name):\n",
        "        return self.data.search_items(item_name)\n",
        "\n",
        "    def get_inter_num(self):\n",
        "        return self.inter_num\n",
        "\n",
        "    def update_history_by_name(self, user_id, item_names):\n",
        "        \"\"\"\n",
        "        Update the history of a given user.\n",
        "        \"\"\"\n",
        "        item_names = [item_name.strip(\" <>'\\\"\") for item_name in item_names]\n",
        "        item_ids = self.data.get_item_ids(item_names)\n",
        "        self.record[user_id].extend(item_ids)\n",
        "\n",
        "    def update_history_by_id(self, user_id, item_ids):\n",
        "        \"\"\"\n",
        "        Update the history of a given user.\n",
        "        \"\"\"\n",
        "        self.record[user_id].extend(item_ids)\n",
        "\n",
        "    def update_positive(self, user_id, item_names):\n",
        "        \"\"\"\n",
        "        Update the positive history of a given user.\n",
        "        \"\"\"\n",
        "        item_ids = self.data.get_item_ids(item_names)\n",
        "        if len(item_ids) == 0:\n",
        "            return\n",
        "        self.positive[user_id].extend(item_ids)\n",
        "        self.inter_num += len(item_ids)\n",
        "\n",
        "    def update_positive_by_id(self, user_id, item_id):\n",
        "        \"\"\"\n",
        "        Update the history of a given user.\n",
        "        \"\"\"\n",
        "        self.positive[user_id].append(item_id)\n",
        "\n",
        "    def save_interaction(self):\n",
        "        \"\"\"\n",
        "        Save the interaction history to a csv file.\n",
        "        \"\"\"\n",
        "        inters = []\n",
        "        users = self.data.get_full_users()\n",
        "        for user in users:\n",
        "            for item in self.positive[user]:\n",
        "                new_row = {\"user_id\": user, \"item_id\": item, \"rating\": 1}\n",
        "                inters.append(new_row)\n",
        "\n",
        "            for item in self.record[user]:\n",
        "                if item in self.positive[user]:\n",
        "                    continue\n",
        "                new_row = {\"user_id\": user, \"item_id\": item, \"rating\": 0}\n",
        "                inters.append(new_row)\n",
        "\n",
        "        df = pd.DataFrame(inters)\n",
        "        df.to_csv(\n",
        "            self.config[\"interaction_path\"],\n",
        "            index=False,\n",
        "        )\n",
        "\n",
        "        self.inter_df = df\n",
        "\n",
        "    def add_train_data(self, user, item, label):\n",
        "        self.train_data.append((user, item, label))\n",
        "\n",
        "    def clear_train_data(self):\n",
        "        self.train_data = []\n",
        "\n",
        "    def add_user(self, user_id, N_expose, N_view, N_like, N_exit, S_sat):\n",
        "        self.user_data[\"user\"].append(user_id)\n",
        "        self.user_data[\"N_expose\"].append(N_expose)\n",
        "        self.user_data[\"N_view\"].append(N_view)\n",
        "        self.user_data[\"N_like\"].append(N_like)\n",
        "        self.user_data[\"N_exit\"].append(N_exit)\n",
        "        self.user_data[\"S_sat\"].append(S_sat)\n",
        "\n",
        "    def add_review(self, user_id, rating, feelings):\n",
        "        self.rating_feeling[\"User\"].append(user_id)\n",
        "        self.rating_feeling[\"Rating\"].append(rating)\n",
        "        self.rating_feeling[\"Feelings\"].append(feelings)\n",
        "\n",
        "    def satisfaction_metrics(self):\n",
        "        sm_df = pd.DataFrame(self.user_data)\n",
        "        if len(sm_df) == 0:\n",
        "           return None  # no data yet\n",
        "\n",
        "        metrics = {}\n",
        "        sm_df[\"view_ratio\"] = sm_df[\"N_view\"] / sm_df[\"N_expose\"]\n",
        "        sm_df[\"like_ratio\"] = sm_df[\"N_like\"] / sm_df[\"N_expose\"]\n",
        "\n",
        "        metrics[\"P_view\"] = sm_df[\"view_ratio\"].mean()\n",
        "        metrics[\"N_like\"] = sm_df[\"N_like\"].mean()\n",
        "        metrics[\"P_like\"] = sm_df[\"like_ratio\"].mean()\n",
        "        metrics[\"N_exit\"] = sm_df[\"N_exit\"].mean()\n",
        "        metrics[\"S_sat\"] = sm_df[\"S_sat\"].mean()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def get_entropy(\n",
        "        self,\n",
        "    ):\n",
        "        tot_entropy = 0\n",
        "        for user in self.record.keys():\n",
        "            inters = self.record[user]\n",
        "            genres = self.data.get_genres_by_id(inters)\n",
        "            entropy = calculate_entropy(genres)\n",
        "            tot_entropy += entropy\n",
        "\n",
        "        return tot_entropy / len(self.record.keys())\n",
        "\n",
        "    def check_train_data(self):\n",
        "        \"\"\"\n",
        "        Print or inspect the training data.\n",
        "        \"\"\"\n",
        "        print(\"Training Data:\")\n",
        "        for user, item, label in self.train_data:\n",
        "            print(f\"User: {user}, Item: {item}, Label: {label}\")\n",
        "\n",
        "    def create_train_data(self):\n",
        "        \"\"\"\n",
        "        Create a training dataset with random samples.\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of samples to generate.\n",
        "        \"\"\"\n",
        "        self.clear_train_data()  # Clear existing training data\n",
        "        all_data = self.data.interrating  # You need to implement this or use available interaction data\n",
        "\n",
        "        # Convert dict to list of (user, item, label)\n",
        "        # triplets = []\n",
        "        for user, interactions in all_data.items():\n",
        "            for item, rating in interactions:\n",
        "                self.add_train_data(user, item, float(rating))\n",
        "                # triplets.append((user, item, float(rating)))  # keep exact rating\n",
        "\n",
        "        # Split 80% train, 20% temp (to further split into val/test)\n",
        "        self.train_data, self.temp_data = train_test_split(self.train_data, test_size=0.2, random_state=2025)\n",
        "\n",
        "        # Split temp into 10% val and 10% test (from the total dataset)\n",
        "        self.val_data, self.test_data = train_test_split(self.temp_data, test_size=0.5, random_state=2025)\n",
        "\n",
        "        train_users = max([u for u, i, r in self.train_data]) + 1\n",
        "        train_items = max([i for u, i, r in self.train_data]) + 1\n",
        "\n",
        "        val_users = max([u for u, i, r in self.val_data]) + 1\n",
        "        val_items = max([i for u, i, r in self.val_data]) + 1\n",
        "\n",
        "        test_users = max([u for u, i, r in self.test_data]) + 1\n",
        "        test_items = max([i for u, i, r in self.test_data]) + 1\n",
        "\n",
        "        n_items_global = int(self.data.get_item_num())\n",
        "\n",
        "        # Initialize user-item matrix\n",
        "        self.train_matrix = np.zeros((train_users, n_items_global), dtype=np.float32)\n",
        "        self.val_matrix = np.zeros((val_users, n_items_global), dtype=np.float32)\n",
        "        self.test_matrix = np.zeros((test_users, n_items_global), dtype=np.float32)\n",
        "\n",
        "        # Fill interactions safely\n",
        "        for u, i, r in self.train_data:\n",
        "           if i >= n_items_global: continue  # skip bad indices\n",
        "           self.train_matrix[u, i] = r\n",
        "\n",
        "        for u, i, r in self.val_data:\n",
        "           if i >= n_items_global: continue\n",
        "           self.val_matrix[u, i] = r\n",
        "\n",
        "        for u, i, r in self.test_data:\n",
        "           if i >= n_items_global: continue\n",
        "           self.test_matrix[u, i] = r\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_user_metrics(\n",
        "        self, user_id, sim_recommended, all_items, threshold = 3):\n",
        "        \"\"\"\n",
        "        Evaluate precision, recall, (optionally real) accuracy, and F1 for a single user.\n",
        "\n",
        "        Returns:\n",
        "            dict: { 'precision': float, 'recall': float, 'accuracy': float, 'f1': float }\n",
        "        \"\"\"\n",
        "\n",
        "        if user_id not in self.data.interrating:\n",
        "           return {'precision': 0, 'recall': 0, 'accuracy': 0, 'f1': 0}\n",
        "\n",
        "        ground_truth_pairs = self.data.interrating[user_id]\n",
        "        gt_relevant = set(item for item, rating in ground_truth_pairs if rating >= threshold and item in all_items)\n",
        "        sim_recommended = set(sim_recommended)\n",
        "        all_items = set(all_items)\n",
        "\n",
        "        TP = len(gt_relevant & sim_recommended)\n",
        "        FP = len(sim_recommended - gt_relevant)\n",
        "        FN = len(gt_relevant - sim_recommended)\n",
        "        TN = len(all_items - (gt_relevant | sim_recommended))\n",
        "\n",
        "        precision = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "        recall = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0\n",
        "        accuracy = (TP + TN) / len(all_items) if all_items else 0\n",
        "\n",
        "        print(\"precision:\", precision, \"recall:\", recall, \"accuracy:\", accuracy, \"f1:\", f1)\n",
        "        return precision, recall, accuracy, f1\n",
        "\n",
        "    def precisionandrecallk(\n",
        "        self, user_id, recommended, k):\n",
        "        if user_id not in self.data.interrating:\n",
        "           return {'precision_at_k': 0, 'recall_at_k': 0}\n",
        "\n",
        "        sim_recommended = list(dict.fromkeys(recommended))\n",
        "        ground_truth_pairs = self.data.interrating[user_id]\n",
        "        gt_relevant = set(item for item, rating in ground_truth_pairs if rating >= 3)\n",
        "        recommended_at_k = sim_recommended[:k]\n",
        "        hits = sum([1 for item in recommended_at_k if item in gt_relevant])\n",
        "        precision_at_k = hits / k\n",
        "        recall_at_k = hits / len(gt_relevant) if gt_relevant else 0\n",
        "        return precision_at_k, recall_at_k\n",
        "\n",
        "\n",
        "    def calculation_of_rating(self, user_id, item_names, book_rating):\n",
        "        item_ids = self.data.get_item_ids([item_names])\n",
        "        if user_id in self.data.interrating:\n",
        "           # Check for item in user's ratings\n",
        "           for (itm, rating) in self.data.interrating[user_id]:\n",
        "               if itm == item_ids[0]:\n",
        "                  return (rating, book_rating)\n",
        "\n",
        "        # If not found\n",
        "        return (0, book_rating)\n",
        "\n",
        "\n",
        "    def calc_mse_rmse_rating_percentages(self, rating_pairs):\n",
        "\n",
        "        print(\"Incoming rating_pairs:\", rating_pairs[:20])  # show first 20 pairs\n",
        "        print(\"Total pairs:\", len(rating_pairs))\n",
        "\n",
        "        # Remove pairs with zero in ground truth or predicted rating\n",
        "        filtered_pairs = [(gt, pred) for gt, pred in rating_pairs\n",
        "                          if int(gt) != 0]\n",
        "\n",
        "        print(\"After filtering:\", filtered_pairs[:20])\n",
        "        print(\"Remaining pairs:\", len(filtered_pairs))\n",
        "\n",
        "        if not filtered_pairs:\n",
        "           # No valid data after filtering\n",
        "           return None, None, {}, {}, None, None, None\n",
        "\n",
        "        # Convert ratings to int\n",
        "        gt = [int(gt) for gt, pred in filtered_pairs]\n",
        "        pred = [int(pred) for gt, pred in filtered_pairs]\n",
        "        mse = np.mean([(g - p) ** 2 for g, p in zip(gt, pred)])\n",
        "        rmse = np.sqrt(mse)\n",
        "        loglike, ob_loglike = self.ordered_probit_loglik(gt, pred)\n",
        "        rho, p_value = spearmanr(gt, pred)\n",
        "\n",
        "        gt_count = Counter(gt)\n",
        "        pred_count = Counter(pred)\n",
        "        total = len(filtered_pairs)\n",
        "\n",
        "        gt_pct = {r: gt_count.get(r, 0) / total * 100 for r in range(1, 6)}\n",
        "        pred_pct = {r: pred_count.get(r, 0) / total * 100 for r in range(1, 6)}\n",
        "        return mse, rmse, gt_pct, pred_pct, loglike, ob_loglike, rho\n",
        "\n",
        "\n",
        "    def test_recommendations(self, user_id):\n",
        "        # Get the full list of items\n",
        "        all_items = self.data.get_full_items()\n",
        "\n",
        "        # Convert the user ID to tensor\n",
        "        user_tensor = torch.tensor(user_id)\n",
        "\n",
        "        # Convert all items to tensor\n",
        "        items_tensor = torch.tensor(all_items)\n",
        "\n",
        "        # Get sorted items based on the model's prediction\n",
        "        sorted_items = self.model.get_full_sort_items(user_tensor, items_tensor)\n",
        "\n",
        "        # Filter out items that are already in the user's history\n",
        "        recommended_items = [item for item in sorted_items if item not in self.record[user_id]]\n",
        "\n",
        "        # Return the recommended items\n",
        "        return recommended_items\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        self.model.eval()\n",
        "        users = torch.tensor([x[0] for x in dataset])\n",
        "        items = torch.tensor([x[1] for x in dataset])\n",
        "        labels = torch.tensor([x[2] for x in dataset]).float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "             outputs = self.model(users, items)\n",
        "             loss = self.criterion(outputs, labels)\n",
        "        return loss.item()\n",
        "\n",
        "    def load_checkpoint(self, path=\"best_model.pth\", resume_training=False):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        if resume_training:\n",
        "           # Load optimizer state to resume training exactly where it left off\n",
        "           self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "           start_epoch = checkpoint[\"epoch\"]\n",
        "           self.logger.info(f\"Resuming training from epoch {start_epoch}\")\n",
        "           return start_epoch\n",
        "        else:\n",
        "           self.model.eval()  # set to eval mode for inference\n",
        "\n",
        "    def train_mf(self):\n",
        "        if len(self.train_data) == 0:\n",
        "            print(\"No training data!\")\n",
        "            return\n",
        "\n",
        "        users = [x[0] for x in self.train_data]\n",
        "        items = [x[1] for x in self.train_data]\n",
        "        labels = [x[2] for x in self.train_data]\n",
        "\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(users), torch.tensor(items), torch.tensor(labels))\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=self.config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "\n",
        "        # Build full checkpoint file path\n",
        "        ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_MF_model.pth\")\n",
        "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "        for epoch in range(self.epoch_num):\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for user, item, label in train_loader:\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(user, item)\n",
        "                loss = self.criterion(outputs, label.float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            val_loss = self.evaluate(self.val_data)  # Evaluate on validation set\n",
        "\n",
        "            self.logger.info(\n",
        "            f\"Epoch {epoch+1}/{self.epoch_num}, Train Loss: {epoch_loss/len(train_loader):.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Save checkpoint if validation improves\n",
        "            if val_loss < best_val_loss:\n",
        "               best_val_loss = val_loss\n",
        "               torch.save({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                \"val_loss\": val_loss,\n",
        "                }, ckpt_file)\n",
        "               self.logger.info(f\"Best model updated at epoch {epoch+1}, saved to {ckpt_file}\")\n",
        "\n",
        "        # At the end, reload the best weights for inference\n",
        "        checkpoint = torch.load(ckpt_file)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    def load_best_model(self):\n",
        "        if self.config['rec_model'] == 'MF':\n",
        "           ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_MF_model.pth\")\n",
        "        elif self.config['rec_model'] == 'MultiVAE':\n",
        "           ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_MultiVAE_model.pth\")\n",
        "        elif self.config['rec_model'] == 'LightGCN':\n",
        "           ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_lightGCN_model.pth\")\n",
        "        elif self.config['rec_model'] == 'FM':\n",
        "           ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_FM_model.pth\")\n",
        "        elif self.config['rec_model'] == 'SASRec':\n",
        "           ckpt_file = os.path.join(self.config['checkpoint_path'], \"best_SASRec_model.pth\")\n",
        "        else:\n",
        "           raise ValueError(f\"Unknown model type: {self.config['rec_model']}\")\n",
        "\n",
        "        # Build full checkpoint file path\n",
        "        checkpoint = torch.load(ckpt_file)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "        self.logger.info(f\"Loaded best model from {ckpt_file}\")\n",
        "\n",
        "    def get_rec_discription(self, final_items):\n",
        "        # items discriptions\n",
        "        sorted_item_names = self.data.get_item_names(final_items)\n",
        "        description = self.data.get_item_description_by_id(final_items)\n",
        "        eb_item = [\n",
        "            sorted_item_names[i]\n",
        "            + \";;\"\n",
        "            + description[i]\n",
        "            + \";; Genre: \"\n",
        "            + self.data.get_genres_by_id([final_items[i]])[0]\n",
        "            for i in range(len(sorted_item_names))\n",
        "        ]\n",
        "        return eb_item\n",
        "\n",
        "    def get_full_rankings(self, use_test=False, batch_size=512):\n",
        "        \"\"\"\n",
        "        Compute full rankings for all users in self.data.\n",
        "        - training items are pushed to the end\n",
        "        - optionally, ground-truth test items can be put on top\n",
        "        \"\"\"\n",
        "        if self.config['rec_model'] == 'MF':\n",
        "           n_users = self.data.get_user_num()\n",
        "           n_items = self.data.get_item_num()\n",
        "\n",
        "           item_embed = self.model.item_embedding.weight[:n_items, :]\n",
        "\n",
        "           self.full_rankings = np.zeros((n_users, n_items), dtype=int)\n",
        "\n",
        "           for user in range(n_users):\n",
        "              user_tensor = torch.tensor([user])\n",
        "              user_embed = self.model.user_embedding(user_tensor)\n",
        "\n",
        "              scores = torch.matmul(user_embed, item_embed.T).squeeze(0).detach().numpy()\n",
        "\n",
        "              # Only consider valid item indices\n",
        "              train_items = [x[1] for x in self.train_data if x[0] == user and x[1] < n_items]\n",
        "              scores[train_items] = -np.inf\n",
        "\n",
        "              self.full_rankings[user] = np.argsort(-scores)\n",
        "\n",
        "              # # Optionally move ground-truth test items on top\n",
        "              if use_test:\n",
        "                 test_items = [x[1] for x in self.test_data if x[0] == user and x[1] < n_items]\n",
        "                 for idx, item in enumerate(test_items):\n",
        "                    if item in self.full_rankings[user]:\n",
        "                       current_pos = np.where(self.full_rankings[user] == item)[0][0]\n",
        "                       self.full_rankings[user][idx], self.full_rankings[user][current_pos] = (\n",
        "                         self.full_rankings[user][current_pos],\n",
        "                         self.full_rankings[user][idx])\n",
        "\n",
        "        elif self.config['rec_model'] == 'LightGCN':\n",
        "            n_users = self.data.get_user_num()\n",
        "            n_items = self.data.get_item_num()\n",
        "\n",
        "            # === 1. Get all user/item embeddings from LightGCN ===\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                all_user_emb, all_item_emb = self.model.propagate()\n",
        "                # shapes: (n_users, embed_dim), (n_items, embed_dim)\n",
        "\n",
        "            self.full_rankings = np.zeros((n_users, n_items), dtype=int)\n",
        "\n",
        "            for user in range(n_users):\n",
        "               # Get user embedding\n",
        "               user_embed = all_user_emb[user].unsqueeze(0)   # (1, embed_dim)\n",
        "\n",
        "               # Compute scores for all items (dot product)\n",
        "               scores = torch.matmul(user_embed, all_item_emb.T).squeeze(0).cpu().numpy()\n",
        "\n",
        "               # Push training items to -inf\n",
        "               train_items = [x[1] for x in self.train_data if x[0] == user and x[1] < n_items]\n",
        "               scores[train_items] = -np.inf\n",
        "\n",
        "               # Sort descending\n",
        "               self.full_rankings[user] = np.argsort(-scores)\n",
        "\n",
        "               # # Optionally move ground-truth test items on top\n",
        "               if use_test:\n",
        "                  test_items = [x[1] for x in self.test_data if x[0] == user and x[1] < n_items]\n",
        "                  for idx, item in enumerate(test_items):\n",
        "                     if item in self.full_rankings[user]:\n",
        "                        current_pos = np.where(self.full_rankings[user] == item)[0][0]\n",
        "                        self.full_rankings[user][idx], self.full_rankings[user][current_pos] = (\n",
        "                           self.full_rankings[user][current_pos],\n",
        "                           self.full_rankings[user][idx])\n",
        "\n",
        "        elif self.config['rec_model'] == 'MultiVAE':\n",
        "            # n_users = self.data.get_user_num()\n",
        "            # n_items = self.data.get_item_num()\n",
        "            if use_test:\n",
        "               matrix = self.test_matrix\n",
        "            else:\n",
        "               matrix = self.train_matrix\n",
        "\n",
        "            n_users, n_items = matrix.shape\n",
        "\n",
        "            self.model.eval()\n",
        "            self.full_rankings = np.zeros((n_users, n_items), dtype=int)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for start in range(0, n_users, batch_size):\n",
        "                   end = min(start + batch_size, n_users)\n",
        "\n",
        "                   # === 1. Build user input batch (interaction vectors) ===\n",
        "                   batch_users = []\n",
        "                   for u in range(start, end):\n",
        "                      row = self.train_matrix[u]  # should return (n_items,) vector\n",
        "                      batch_users.append(row)\n",
        "                   batch_users = torch.tensor(batch_users, dtype=torch.float32).to(self.model.device)\n",
        "\n",
        "                   # === 2. Forward pass through MultiVAE ===\n",
        "                   logits, mu, logvar = self.model(batch_users)   # shape: (batch_size, n_items)\n",
        "                   scores = logits.cpu().numpy()\n",
        "\n",
        "                   # === 3. Postprocess each user in batch ===\n",
        "                   for i, u in enumerate(range(start, end)):\n",
        "                      user_scores = scores[i]\n",
        "\n",
        "                      train_items = [x[1] for x in self.train_data if x[0] == u and x[1] < n_items]\n",
        "                      user_scores[train_items] = -np.inf\n",
        "\n",
        "                      # Sort items by descending score\n",
        "                      self.full_rankings[u] = np.argsort(-user_scores)\n",
        "\n",
        "                      # Optionally move ground-truth test items on top\n",
        "                      if use_test:\n",
        "                         test_items = [x[1] for x in self.test_data if x[0] == u and x[1] < n_items]\n",
        "                         for idx, item in enumerate(test_items):\n",
        "                            if item in self.full_rankings[u]:\n",
        "                               current_pos = np.where(self.full_rankings[u] == item)[0][0]\n",
        "                               self.full_rankings[u][idx], self.full_rankings[u][current_pos] = (\n",
        "                                   self.full_rankings[u][current_pos],\n",
        "                                   self.full_rankings[u][idx])\n",
        "\n",
        "        elif self.config['rec_model'] == 'FM':\n",
        "            n_users = self.data.get_user_num()\n",
        "            n_items = self.data.get_item_num()\n",
        "\n",
        "            self.full_rankings = np.zeros((n_users, n_items), dtype=int)\n",
        "\n",
        "            self.model.eval()\n",
        "            device = next(self.model.parameters()).device\n",
        "\n",
        "            for user in range(n_users):\n",
        "               # Generate all item IDs\n",
        "               item_ids = torch.arange(n_items, device=device)\n",
        "               user_ids = torch.full((n_items,), user, dtype=torch.long, device=device)\n",
        "\n",
        "               # Compute scores using FM forward\n",
        "               with torch.no_grad():\n",
        "                  scores = self.model(user_ids, item_ids).cpu().numpy()\n",
        "\n",
        "               # Push training items to the end\n",
        "               train_items = [x[1] for x in self.train_data if x[0] == user and x[1] < n_items]\n",
        "               scores[train_items] = -np.inf\n",
        "\n",
        "               # Sort items by descending score\n",
        "               ranking = np.argsort(-scores)\n",
        "\n",
        "               # Optionally move ground-truth test items to the top\n",
        "               if use_test:\n",
        "                  test_items = [x[1] for x in self.test_data if x[0] == user and x[1] < n_items]\n",
        "                  for idx, item in enumerate(test_items):\n",
        "                     if item in ranking:\n",
        "                        current_pos = np.where(ranking == item)[0][0]\n",
        "                        ranking[idx], ranking[current_pos] = ranking[current_pos], ranking[idx]\n",
        "\n",
        "               self.full_rankings[user] = ranking\n",
        "\n",
        "        elif self.config['rec_model'] == 'SASRec':\n",
        "            n_users = self.data.get_user_num()\n",
        "            n_items = self.data.get_item_num()\n",
        "\n",
        "            self.full_rankings = np.zeros((n_users, n_items), dtype=int)\n",
        "            self.model.eval()\n",
        "            device = next(self.model.parameters()).device\n",
        "\n",
        "            with torch.no_grad():\n",
        "               for start in range(0, n_users, batch_size):\n",
        "                  end = min(start + batch_size, n_users)\n",
        "                  batch_users = list(range(start, end))\n",
        "\n",
        "                  # Build input sequences for batch\n",
        "                  batch_seqs = []\n",
        "                  for u in batch_users:\n",
        "                     # Get user interaction sequence from train_data\n",
        "                     user_items = [x[1] for x in self.train_data if x[0] == u]\n",
        "                     padded_seq = _pad_sequence(user_items, self.model.max_seq_len)\n",
        "                     batch_seqs.append(padded_seq)\n",
        "\n",
        "                  batch_seqs = torch.tensor(batch_seqs, dtype=torch.long, device=device)\n",
        "\n",
        "                  # Forward pass: get sequence embeddings\n",
        "                  seq_out = self.model(batch_seqs)  # (B, L, H)\n",
        "                  seq_out_last = seq_out[:, -1, :]  # use last position (B, H)\n",
        "\n",
        "                  # All item embeddings\n",
        "                  all_item_emb = self.model.item_embedding.weight[:n_items, :]  # (n_items, H)\n",
        "\n",
        "                  # Compute scores\n",
        "                  scores = torch.matmul(seq_out_last, all_item_emb.T)  # (B, n_items)\n",
        "                  scores = scores.cpu().numpy()\n",
        "\n",
        "                  # Mask training items\n",
        "                  for i, u in enumerate(batch_users):\n",
        "                     train_items = [x[1] for x in self.train_data if x[0] == u and x[1] < n_items]\n",
        "                     scores[i, train_items] = -np.inf  # push train items to the end\n",
        "\n",
        "                     ranking = np.argsort(-scores[i])  # full ranking by score (highest first)\n",
        "                     if use_test:\n",
        "                        test_items = [x[1] for x in self.test_data if x[0] == u and x[1] < n_items]\n",
        "                        # Keep only test items that appear in ranking\n",
        "                        test_items_in_ranking = [item for item in ranking if item in test_items]\n",
        "                        # Take at most 5 test items\n",
        "                        top_test_items = test_items_in_ranking[:5]\n",
        "                        # Remaining items (exclude the ones we forced to the top)\n",
        "                        other_items = [item for item in ranking if item not in top_test_items]\n",
        "\n",
        "                        # New ranking: top test items first, then the rest in score order\n",
        "                        ranking = np.array(top_test_items + other_items)\n",
        "                     # Store final ranking\n",
        "                     self.full_rankings[u] = ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swWHZ3JDDdAi"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import torch\n",
        "\n",
        "\n",
        "class BaseRLEnvironment():\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - max_step_per_episode\n",
        "        - initial_temper\n",
        "        '''\n",
        "        parser.add_argument('--max_step_per_episode', type=int, default=100, help='max number of iteration allowed in each episode')\n",
        "        parser.add_argument('--initial_temper', type=float, default=10, help='initial temper of users')\n",
        "        return parser\n",
        "\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.device = args.device\n",
        "        super().__init__()\n",
        "        self.max_step_per_episode = args.max_step_per_episode\n",
        "        self.initial_temper = args.initial_temper\n",
        "\n",
        "    def reset(self, params):\n",
        "        pass\n",
        "\n",
        "    def step(self, action):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def get_user_model(self, log_path, device, from_load = True):\n",
        "        infile = open(log_path, 'r')\n",
        "        class_args = eval(infile.readline()) # example: Namespace(model='KRMBUserResponse', reader='KRMBSeqReader')\n",
        "        model_args = eval(infile.readline()) # model parameters in Namespace\n",
        "        infile.close()\n",
        "        checkpoint = torch.load(model_args.model_path + \".checkpoint\", map_location=device)\n",
        "        reader_stats = checkpoint[\"reader_stats\"]\n",
        "        modelClass = eval('{0}.{0}'.format(class_args.model))\n",
        "        model = modelClass(model_args, reader_stats, device)\n",
        "        if from_load:\n",
        "            model.load_from_checkpoint(model_args.model_path, with_optimizer = False)\n",
        "        model = model.to(device)\n",
        "        return reader_stats, model, model_args\n",
        "\n",
        "    def get_reader(self, log_path):\n",
        "        infile = open(log_path, 'r')\n",
        "        class_args = eval(infile.readline()) # example: Namespace(model='KRMBUserResponse', reader='KRMBSeqReader')\n",
        "        training_args = eval(infile.readline()) # model parameters in Namespace\n",
        "        training_args.val_holdout_per_user = 0\n",
        "        training_args.test_holdout_per_user = 0\n",
        "        training_args.device = self.device\n",
        "        infile.close()\n",
        "        # readerClass = eval('{0}.{0}'.format(class_args.reader))\n",
        "        readerClass = eval(class_args.reader)\n",
        "        reader = readerClass(training_args)\n",
        "        return reader, training_args\n",
        "\n",
        "\n",
        "    def get_observation_from_batch(self, sample_batch):\n",
        "        '''\n",
        "        extract observation from the reader's sample batch\n",
        "        @input:\n",
        "        - sample_batch: {\n",
        "            'user_id': (B,)\n",
        "            'uf_{feature}': (B,F_dim(feature)), user features\n",
        "            'history': (B,max_H)\n",
        "            'history_length': (B,)\n",
        "            'history_if_{feature}': (B, max_H * F_dim(feature))\n",
        "            'history_{response}': (B, max_H)\n",
        "            ... user ground truth feedbacks are not included as observation\n",
        "        }\n",
        "        @output:\n",
        "        - observation: {'user_profile': {'user_id': (B,),\n",
        "                                         'uf_{feature_name}': (B, feature_dim)},\n",
        "                        'user_history': {'history': (B, max_H),\n",
        "                                         'history_if_{feature_name}': (B, max_H, feature_dim),\n",
        "                                         'history_{response}': (B, max_H),\n",
        "                                         'history_length': (B, )}}\n",
        "        '''\n",
        "        sample_batch = wrap_batch(sample_batch, device = self.device)\n",
        "        profile = {'user_id': sample_batch['user_id']}\n",
        "        for k,v in sample_batch.items():\n",
        "            if 'uf_' in k:\n",
        "                profile[k] = v\n",
        "        history = {'history': sample_batch['history']}\n",
        "        for k,v in sample_batch.items():\n",
        "            if 'history_' in k:\n",
        "                history[k] = v\n",
        "        return {'user_profile': profile, 'user_history': history}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCtmLOAeDM0-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from argparse import Namespace\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class KREnvironment_WholeSession_GPU(BaseRLEnvironment):\n",
        "    '''\n",
        "    KuaiRand simulated environment for consecutive list-wise recommendation\n",
        "    Main interface:\n",
        "    - parse_model_args: for hyperparameters\n",
        "    - reset: reset online environment, monitor, and corresponding initial observation\n",
        "    - step: action --> new observation, user feedbacks, and other updated information\n",
        "    - get_candidate_info: obtain the entire item candidate pool\n",
        "    Main Components:\n",
        "    - data reader: self.reader for user profile&history sampler\n",
        "    - user immediate response model: see self.get_response\n",
        "    - no user leave model: see self.get_leave_signal\n",
        "    - candidate item pool: self.candidate_ids, self.candidate_item_meta\n",
        "    - history monitor: self.env_history, not set up until self.reset\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - uirm_log_path\n",
        "        - slate_size\n",
        "        - episode_batch_size\n",
        "        - item_correlation\n",
        "        - single_response\n",
        "        - from BaseRLEnvironment\n",
        "            - max_step_per_episode\n",
        "            - initial_temper\n",
        "        '''\n",
        "        parser = BaseRLEnvironment.parse_model_args(parser)\n",
        "        parser.add_argument('--uirm_log_path', type=str, required=True,\n",
        "                            help='log path for pretrained user immediate response model')\n",
        "        parser.add_argument('--slate_size', type=int, required=6,\n",
        "                            help='number of item per recommendation slate')\n",
        "        parser.add_argument('--episode_batch_size', type=int, default=32,\n",
        "                            help='episode sample batch size')\n",
        "        parser.add_argument('--item_correlation', type=float, default=0,\n",
        "                            help='magnitude of item correlation')\n",
        "        parser.add_argument('--single_response', action='store_true',\n",
        "                            help='only include the first feedback as reward signal')\n",
        "        return parser\n",
        "\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(KREnvironment_WholeSession_GPU, self).__init__(args)\n",
        "        self.uirm_log_path = args.uirm_log_path\n",
        "        self.slate_size = args.slate_size\n",
        "        self.episode_batch_size = args.episode_batch_size\n",
        "        self.rho = args.item_correlation\n",
        "        self.single_response = args.single_response\n",
        "\n",
        "        # --- load logged model config ---\n",
        "        with open(args.uirm_log_path, 'r') as infile:\n",
        "            class_args = eval(infile.readline())\n",
        "            model_args = eval(infile.readline())\n",
        "        print(\"Environment arguments: \\n\" + str(model_args))\n",
        "        assert (class_args.reader in ['KRMBSeqReader', 'MLSeqReader']\n",
        "                and 'KRMBUserResponse' in class_args.model)\n",
        "\n",
        "        # --- load reader ---\n",
        "        print(\"Load user sequence reader\")\n",
        "        reader, reader_args = self.get_reader(args.uirm_log_path)\n",
        "        self.reader = reader\n",
        "        print(self.reader.get_statistics())\n",
        "\n",
        "        # --- load user response model ---\n",
        "        print(\"Load immediate user response model\")\n",
        "        uirm_stats, uirm_model, uirm_args = self.get_user_model(args.uirm_log_path, args.device)\n",
        "        self.immediate_response_stats = uirm_stats\n",
        "        self.immediate_response_model = uirm_model\n",
        "        self.max_hist_len = uirm_stats['max_seq_len']\n",
        "        self.response_types = uirm_stats['feedback_type']\n",
        "        self.response_dim = len(self.response_types)\n",
        "        self.response_weights = torch.tensor(\n",
        "            list(self.reader.get_response_weights().values()),\n",
        "            dtype=torch.float,\n",
        "            device=args.device\n",
        "        )\n",
        "        if args.single_response:\n",
        "            self.response_weights = torch.zeros_like(self.response_weights)\n",
        "            self.response_weights[0] = 1\n",
        "\n",
        "        print(\"Setup candidate item pool\")\n",
        "\n",
        "        # (n_item,)\n",
        "        self.candidate_iids = torch.tensor(\n",
        "            [reader.item_id_vocab[iid] for iid in reader.items],\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # build item meta from reader\n",
        "        candidate_meta = [reader.get_item_meta_data(iid) for iid in reader.items]\n",
        "        self.candidate_item_meta = {}\n",
        "        self.n_candidate = len(candidate_meta)\n",
        "\n",
        "        # NORMALIZE KEYS HERE: force them to start with \"if_\"\n",
        "        for k in candidate_meta[0]:\n",
        "            key = k if k.startswith(\"if_\") else f\"if_{k}\"\n",
        "            self.candidate_item_meta[key] = torch.FloatTensor(\n",
        "                np.concatenate([meta[k] for meta in candidate_meta])\n",
        "            ).view(self.n_candidate, -1).to(self.device)\n",
        "\n",
        "        # now we can safely call the model the same way the original code did\n",
        "        item_enc, _ = self.immediate_response_model.get_item_encoding(\n",
        "            self.candidate_iids,\n",
        "            {k[3:]: v for k, v in self.candidate_item_meta.items()},  # strip \"if_\"\n",
        "            1\n",
        "        )\n",
        "        self.candidate_item_encoding = item_enc.view(-1, self.immediate_response_model.enc_dim)\n",
        "\n",
        "        # spaces\n",
        "        self.gt_state_dim = self.immediate_response_model.state_dim\n",
        "        self.action_dim = self.slate_size\n",
        "        self.observation_space = self.reader.get_statistics()\n",
        "        self.action_space = self.n_candidate\n",
        "\n",
        "        self.immediate_response_model.to(args.device)\n",
        "        self.immediate_response_model.device = args.device\n",
        "\n",
        "    def get_candidate_info(self, feed_dict, all_item=True):\n",
        "        if all_item:\n",
        "            cand = {'item_id': self.candidate_iids}\n",
        "            cand.update(self.candidate_item_meta)   # all \"if_...\"\n",
        "            return cand\n",
        "        else:\n",
        "            cand = {'item_id': feed_dict['item_id']}\n",
        "            idx = feed_dict['item_id'] - 1\n",
        "            cand.update({k: v[idx] for k, v in self.candidate_item_meta.items()})\n",
        "            return cand\n",
        "\n",
        "    def reset(self, params={'empty_history': True}):\n",
        "        if 'empty_history' not in params:\n",
        "            params['empty_history'] = False\n",
        "        BS = params.get('batch_size', self.episode_batch_size)\n",
        "\n",
        "        self.batch_iter = iter(DataLoader(self.reader, batch_size=BS, shuffle=True,\n",
        "                                          pin_memory=True, num_workers=8))\n",
        "        sample_info = next(self.batch_iter)\n",
        "        self.sample_batch = self.get_observation_from_batch(sample_info)\n",
        "        self.current_observation = self.sample_batch\n",
        "        self.current_step = torch.zeros(self.episode_batch_size, device=self.device)\n",
        "        self.current_sample_head_in_batch = BS\n",
        "\n",
        "        self.current_temper = torch.ones(self.episode_batch_size, device=self.device) * self.initial_temper\n",
        "        self.current_sum_reward = torch.zeros(self.episode_batch_size, device=self.device)\n",
        "\n",
        "        self.env_history = {'step': [0.], 'leave': [], 'temper': [], 'coverage': [], 'ILD': []}\n",
        "\n",
        "        return deepcopy(self.current_observation)\n",
        "\n",
        "\n",
        "    def step(self, step_dict):\n",
        "        '''\n",
        "        users react to the recommendation action\n",
        "        @input:\n",
        "        - step_dict: {'action': (B, slate_size),\n",
        "                      'action_features': (B, slate_size, item_dim) }\n",
        "        @output:\n",
        "        - new_observation: {'user_profile': {'user_id': (B,),\n",
        "                                             'uf_{feature_name}': (B, feature_dim)},\n",
        "                            'user_history': {'history': (B, max_H),\n",
        "                                             'history_if_{feature_name}': (B, max_H, feature_dim),\n",
        "                                             'history_{response}': (B, max_H),\n",
        "                                             'history_length': (B, )}}\n",
        "        - response_dict: {'immediate_response': see self.get_response@output - response_dict,\n",
        "                          'done': (B,)}\n",
        "        - update_info: see self.update_observation@output - update_info\n",
        "        '''\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = step_dict['action']  # (B, slate_size)\n",
        "\n",
        "            # ----- build batch for user model -----\n",
        "            # notice we pass \"if_...\" keys here\n",
        "            batch = {\n",
        "                'item_id': self.candidate_iids[action],   # (B, slate_size)\n",
        "                **self.current_observation['user_profile'],\n",
        "                **self.current_observation['user_history'],\n",
        "                **{k: v[action] for k, v in self.candidate_item_meta.items()}  # all \"if_...\"\n",
        "            }\n",
        "\n",
        "            response_dict = self.immediate_response_model(batch)\n",
        "            response = response_dict['preds']  # (B, slate_size, n_feedback) named \"probs\" in your earlier code\n",
        "\n",
        "            # some versions call it 'probs', some return 'preds'\n",
        "            if 'probs' in response_dict:\n",
        "               behavior_scores = response_dict['probs']\n",
        "            else:\n",
        "               behavior_scores = response_dict['preds']   # (B, slate_size, n_feedback)\n",
        "\n",
        "            # sample / binarize feedback\n",
        "            sampled_feedback = (torch.sigmoid(behavior_scores) > 0.5).float()\n",
        "\n",
        "            # done + temper update uses immediate_response\n",
        "            done_mask = self.get_leave_signal(\n",
        "            None,\n",
        "            None,\n",
        "            {'immediate_response': sampled_feedback}\n",
        "            )\n",
        "\n",
        "            response_out = {\n",
        "                'immediate_response': (torch.sigmoid(behavior_scores) > 0.5).float(),\n",
        "                'done': done_mask,\n",
        "                'coverage': len(torch.unique(action)),\n",
        "                'ILD': 0.0,  # fill with your ILD if you need it\n",
        "            }\n",
        "\n",
        "            update_info = self.update_observation(None,\n",
        "                                                  action,\n",
        "                                                  response_out['immediate_response'],\n",
        "                                                  done_mask)\n",
        "\n",
        "            # env_history update: step, leave, temper, converage, ILD\n",
        "            self.current_step += 1\n",
        "            n_leave = done_mask.sum()\n",
        "            self.env_history['leave'].append(n_leave.item())\n",
        "            self.env_history['temper'].append(torch.mean(self.current_temper).item())\n",
        "            self.env_history['coverage'].append(response_out['coverage'])\n",
        "            self.env_history['ILD'].append(response_out['ILD'])\n",
        "\n",
        "        # when users left, new users come into the running batch\n",
        "        if n_leave > 0:\n",
        "            final_steps = self.current_step[done_mask].detach().cpu().numpy()\n",
        "            for fst in final_steps:\n",
        "                self.env_history['step'].append(fst)\n",
        "\n",
        "            if self.current_sample_head_in_batch + n_leave < self.episode_batch_size:\n",
        "                # reuse previous batch if there are sufficient samples for n_leave\n",
        "                head = self.current_sample_head_in_batch\n",
        "                tail = self.current_sample_head_in_batch + n_leave\n",
        "                for obs_key in ['user_profile', 'user_history']:\n",
        "                    for k, v in self.sample_batch[obs_key].items():\n",
        "                        src = v[head:tail]              # new users\n",
        "                        dst = self.current_observation[obs_key][k]\n",
        "                        # 🔧 if dst is flattened but src is 3D, flatten src to match\n",
        "                        if dst.dim() == 2 and src.dim() == 3:\n",
        "                            B, H, D = src.shape\n",
        "                            src = src.view(B, H * D)\n",
        "                        self.current_observation[obs_key][k][done_mask] = src\n",
        "                self.current_sample_head_in_batch += n_leave\n",
        "            else:\n",
        "                # sample new users to fill in the blank\n",
        "                sample_info = self.sample_new_batch_from_reader()\n",
        "                self.sample_batch = self.get_observation_from_batch(sample_info)\n",
        "                for obs_key in ['user_profile', 'user_history']:\n",
        "                    for k, v in self.sample_batch[obs_key].items():\n",
        "                        src = v[:n_leave]\n",
        "                        dst = self.current_observation[obs_key][k]\n",
        "                        # 🔧 same fix here\n",
        "                        if dst.dim() == 2 and src.dim() == 3:\n",
        "                            B, H, D = src.shape\n",
        "                            src = src.view(B, H * D)\n",
        "                        self.current_observation[obs_key][k][done_mask] = src\n",
        "                self.current_sample_head_in_batch = n_leave\n",
        "\n",
        "            self.current_step[done_mask] *= 0\n",
        "            self.current_temper[done_mask] *= 0\n",
        "            self.current_temper[done_mask] += self.initial_temper\n",
        "        else:\n",
        "            self.env_history['step'].append(self.env_history['step'][-1])\n",
        "\n",
        "        return deepcopy(self.current_observation), response_out, update_info\n",
        "\n",
        "\n",
        "    def get_response(self, step_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - step_dict: {'action': (B, slate_size)}\n",
        "        @output:\n",
        "        - response_dict: {'immediate_response': (B, slate_size, n_feedback),\n",
        "                          'user_state': (B, gt_state_dim),\n",
        "                          'coverage': scalar,\n",
        "                          'ILD': scalar}\n",
        "        '''\n",
        "        # actions (exposures), (B, slate_size), indices of self.candidate_iid\n",
        "        action = step_dict['action']\n",
        "        coverage = len(torch.unique(action))\n",
        "        B = self.episode_batch_size\n",
        "\n",
        "        ########################################\n",
        "        # This is where the action take effect #\n",
        "        # (B, action_dim, 1, enc_dim)\n",
        "        batch = {'item_id': self.candidate_iids[action]}\n",
        "        batch.update(self.current_observation['user_profile'])\n",
        "        batch.update(self.current_observation['user_history'])\n",
        "        batch.update({k:v[action] for k,v in self.candidate_item_meta.items()})\n",
        "        out_dict = self.immediate_response_model(batch)\n",
        "        ########################################\n",
        "\n",
        "        # (B, slate_size, n_feedback)\n",
        "        behavior_scores = out_dict['probs']\n",
        "\n",
        "        # (B, slate_size, item_dim)\n",
        "        item_enc = self.candidate_item_encoding[action].view(B, self.slate_size, -1)\n",
        "        item_enc_norm = F.normalize(item_enc, p = 2.0, dim = -1)\n",
        "        # (B, slate_size)\n",
        "        corr_factor = self.get_intra_slate_similarity(item_enc_norm)\n",
        "\n",
        "        # user response sampling\n",
        "        # (B, slate_size, n_feedback)\n",
        "        point_scores = torch.sigmoid(behavior_scores) - corr_factor.view(B, self.slate_size, 1) * self.rho\n",
        "        point_scores[point_scores < 0] = 0\n",
        "\n",
        "        # (B, slate_size, n_feedback)\n",
        "        response = torch.bernoulli(point_scores).detach()\n",
        "\n",
        "        return {'immediate_response': response,\n",
        "                'user_state': out_dict['state'],\n",
        "                'coverage': coverage,\n",
        "                'ILD': 1 - torch.mean(corr_factor).item()}\n",
        "\n",
        "    def get_ground_truth_user_state(self, profile, history):\n",
        "        batch_data = {}\n",
        "        batch_data.update(profile)\n",
        "        batch_data.update(history)\n",
        "        gt_state_dict = self.immediate_response_model.encode_state(batch_data, self.episode_batch_size)\n",
        "        gt_user_state = gt_state_dict['state'].view(self.episode_batch_size,1,self.gt_state_dim)\n",
        "        return gt_user_state\n",
        "\n",
        "    def get_intra_slate_similarity(self, action_item_encoding):\n",
        "        '''\n",
        "        @input:\n",
        "        - action_item_encoding: (B, slate_size, enc_dim)\n",
        "        @output:\n",
        "        - similarity: (B, slate_size)\n",
        "        '''\n",
        "        B, L, d = action_item_encoding.shape\n",
        "        # pairwise similarity in a slate (B, L, L)\n",
        "        pair_similarity = torch.mean(action_item_encoding.view(B,L,1,d) * action_item_encoding.view(B,1,L,d), dim = -1)\n",
        "        # similarity to slate average, (B, L)\n",
        "        point_similarity = torch.mean(pair_similarity, dim = -1)\n",
        "        return point_similarity\n",
        "\n",
        "    def get_leave_signal(self, user_state, action, response_dict):\n",
        "        '''\n",
        "        User leave model maintains the user temper, and a user leaves when the temper drops below 1.\n",
        "        @input:\n",
        "        - user_state: not used in this env\n",
        "        - action: not used in this env\n",
        "        - response_dict: (B, slate_size, n_feedback)\n",
        "        @process:\n",
        "        - update temper\n",
        "        @output:\n",
        "        - done_mask:\n",
        "        '''\n",
        "        # (B, slate_size, n_feedback)\n",
        "        point_reward = response_dict['immediate_response'] * self.response_weights.view(1,1,-1)\n",
        "        # (B, slate_size)\n",
        "        combined_reward = torch.sum(point_reward, dim = 2)\n",
        "        # (B, )\n",
        "        temper_boost = torch.mean(combined_reward, dim = 1)\n",
        "        # temper update for leave model\n",
        "        temper_update = temper_boost - 2\n",
        "        temper_update[temper_update > 0] = 0\n",
        "        temper_update[temper_update < -2] = -2\n",
        "        self.current_temper += temper_update\n",
        "        # leave signal\n",
        "        done_mask = self.current_temper < 1\n",
        "        return done_mask\n",
        "\n",
        "\n",
        "    def update_observation(self, user_state, action, response, done_mask, update_current=True):\n",
        "        # (B, slate_size), encoded item id\n",
        "        rec_list = self.candidate_iids[action]\n",
        "        old_history = self.current_observation['user_history']\n",
        "        max_H = self.max_hist_len\n",
        "        L = old_history['history_length'] + self.slate_size\n",
        "        L[L > max_H] = max_H\n",
        "        new_history = {\n",
        "            'history': torch.cat((old_history['history'], rec_list), dim=1)[:, -max_H:],\n",
        "            'history_length': L\n",
        "        }\n",
        "        # IMPORTANT: store history with \"history_if_...\" so model can read it\n",
        "        for k, candidate_meta_features in self.candidate_item_meta.items():\n",
        "            meta_features = candidate_meta_features[action]  # (B, slate_size, feat_dim)\n",
        "            prev_meta = old_history[f'history_{k}'].view(self.episode_batch_size, max_H, -1)\n",
        "            new_history[f'history_{k}'] = torch.cat((prev_meta, meta_features), dim=1)[:, -max_H:, :].view(\n",
        "                self.episode_batch_size, -1\n",
        "            )\n",
        "\n",
        "        # feedback histories (your original loop can stay)\n",
        "        for i, R in enumerate(self.immediate_response_model.feedback_types):\n",
        "            k = f'history_{R}'\n",
        "            new_history[k] = torch.cat((old_history[k], response[:, :, i]), dim=1)[:, -max_H:]\n",
        "\n",
        "        if update_current:\n",
        "            self.current_observation['user_history'] = new_history\n",
        "\n",
        "        return {\n",
        "            'slate': rec_list,\n",
        "            'updated_observation': {\n",
        "                'user_profile': deepcopy(self.current_observation['user_profile']),\n",
        "                'user_history': deepcopy(new_history)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def sample_new_batch_from_reader(self):\n",
        "        '''\n",
        "        @output\n",
        "        - sample_info: see BaseRLEnvironment.get_observation_from_batch@input - sample_batch\n",
        "        '''\n",
        "        new_sample_flag = False\n",
        "        try:\n",
        "            sample_info = next(self.batch_iter)\n",
        "            if sample_info['user_profile'].shape[0] != self.episode_batch_size:\n",
        "                new_sample_flag = True\n",
        "        except:\n",
        "            new_sample_flag = True\n",
        "        if new_sample_flag:\n",
        "            self.batch_iter = iter(DataLoader(self.reader, batch_size = self.episode_batch_size, shuffle = True,\n",
        "                                              pin_memory = True, num_workers = 8))\n",
        "            sample_info = next(self.batch_iter)\n",
        "        return sample_info\n",
        "\n",
        "    def stop(self):\n",
        "        self.batch_iter = None\n",
        "\n",
        "    def get_new_iterator(self, B):\n",
        "        return iter(DataLoader(self.reader, batch_size = B, shuffle = True,\n",
        "                               pin_memory = True, num_workers = 8))\n",
        "\n",
        "\n",
        "    def create_observation_buffer(self, buffer_size):\n",
        "        '''\n",
        "        @input:\n",
        "        - buffer_size: L, scalar\n",
        "        @output:\n",
        "        - observation: {'user_profile': {'user_id': (L,),\n",
        "                                         'uf_{feature_name}': (L, feature_dim)},\n",
        "                        'user_history': {'history': (L, max_H),\n",
        "                                         'history_if_{feature_name}': (L, max_H * feature_dim),\n",
        "                                         'history_{response}': (L, max_H),\n",
        "                                         'history_length': (L,)}}\n",
        "        '''\n",
        "        observation = {'user_profile': {'user_id': torch.zeros(buffer_size).to(torch.long).to(self.device)},\n",
        "                       'user_history': {'history': torch.zeros(buffer_size, self.max_hist_len).to(torch.long).to(self.device),\n",
        "                                        'history_length': torch.zeros(buffer_size).to(torch.long).to(self.device)}}\n",
        "        for f,f_dim in self.observation_space['user_feature_dims'].items():\n",
        "            observation['user_profile'][f'uf_{f}'] = torch.zeros(buffer_size, f_dim).to(torch.float).to(self.device)\n",
        "        for f,f_dim in self.observation_space['item_feature_dims'].items():\n",
        "            observation['user_history'][f'history_if_{f}'] = torch.zeros(buffer_size, f_dim * self.max_hist_len)\\\n",
        "                                                                                .to(torch.float).to(self.device)\n",
        "        for f in self.observation_space['feedback_type']:\n",
        "            observation['user_history'][f'history_{f}'] = torch.zeros(buffer_size, self.max_hist_len)\\\n",
        "                                                                                .to(torch.float).to(self.device)\n",
        "        return observation\n",
        "\n",
        "    def get_report(self, smoothness = 10):\n",
        "        return {k: np.mean(v[-smoothness:]) for k,v in self.env_history.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi5qVZ6BsUny"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import axes, axis\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BackboneUserEncoder(BaseModel):\n",
        "    '''\n",
        "    KuaiRand Multi-Behavior user response model\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - state_user_latent_dim\n",
        "        - state_item_latent_dim\n",
        "        - state_transformer_enc_dim\n",
        "        - state_transformer_n_head\n",
        "        - state_transformer_d_forward\n",
        "        - state_transformer_n_layer\n",
        "        - state_dropout_rate\n",
        "        - from BaseModel:\n",
        "            - model_path\n",
        "            - loss\n",
        "            - l2_coef\n",
        "        '''\n",
        "        parser = BaseModel.parse_model_args(parser)\n",
        "\n",
        "        parser.add_argument('--state_user_latent_dim', type=int, default=16,\n",
        "                            help='user latent embedding size')\n",
        "        parser.add_argument('--state_item_latent_dim', type=int, default=16,\n",
        "                            help='item latent embedding size')\n",
        "        parser.add_argument('--state_transformer_enc_dim', type=int, default=32,\n",
        "                            help='item encoding size')\n",
        "        parser.add_argument('--state_transformer_n_head', type=int, default=4,\n",
        "                            help='number of attention heads in transformer')\n",
        "        parser.add_argument('--state_transformer_d_forward', type=int, default=64,\n",
        "                            help='forward layer dimension in transformer')\n",
        "        parser.add_argument('--state_transformer_n_layer', type=int, default=2,\n",
        "                            help='number of encoder layers in transformer')\n",
        "        parser.add_argument('--state_dropout_rate', type=float, default=0.1,\n",
        "                            help='dropout rate in deep layers of state encoder')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, args, reader_stats):\n",
        "        self.user_latent_dim = args.state_user_latent_dim\n",
        "        self.item_latent_dim = args.state_item_latent_dim\n",
        "        self.enc_dim = args.state_transformer_enc_dim\n",
        "        self.state_dim = 3*self.enc_dim\n",
        "        self.attn_n_head = args.state_transformer_n_head\n",
        "        self.dropout_rate = args.state_dropout_rate\n",
        "        super().__init__(args, reader_stats, args.device)\n",
        "\n",
        "    def to(self, device):\n",
        "        new_self = super(BackboneUserEncoder, self).to(device)\n",
        "        new_self.attn_mask = new_self.attn_mask.to(device)\n",
        "        new_self.pos_emb_getter = new_self.pos_emb_getter.to(device)\n",
        "        return new_self\n",
        "\n",
        "    def _define_params(self, args, reader_stats):\n",
        "        stats = self.reader_stats\n",
        "\n",
        "        self.user_feature_dims = stats['user_feature_dims'] # {feature_name: dim}\n",
        "        self.item_feature_dims = stats['item_feature_dims'] # {feature_name: dim}\n",
        "\n",
        "        # user embedding\n",
        "        self.uIDEmb = nn.Embedding(stats['n_user']+1, args.state_user_latent_dim)\n",
        "        self.uFeatureEmb = {}\n",
        "        for f,dim in self.user_feature_dims.items():\n",
        "            embedding_module = nn.Linear(dim, args.state_user_latent_dim)\n",
        "            self.add_module(f'UFEmb_{f}', embedding_module)\n",
        "            self.uFeatureEmb[f] = embedding_module\n",
        "\n",
        "        # item embedding\n",
        "        self.iIDEmb = nn.Embedding(stats['n_item']+1, args.state_item_latent_dim)\n",
        "        self.iFeatureEmb = {}\n",
        "        for f,dim in self.item_feature_dims.items():\n",
        "            embedding_module = nn.Linear(dim, args.state_item_latent_dim)\n",
        "            self.add_module(f'IFEmb_{f}', embedding_module)\n",
        "            self.iFeatureEmb[f] = embedding_module\n",
        "\n",
        "        # feedback embedding\n",
        "        self.feedback_types = stats['feedback_type']\n",
        "        self.feedback_dim = stats['feedback_size']\n",
        "        self.feedbackEncoder = nn.Linear(self.feedback_dim, args.state_transformer_enc_dim)\n",
        "\n",
        "        # item embedding kernel encoder\n",
        "        self.itemEmbNorm = nn.LayerNorm(args.state_item_latent_dim)\n",
        "        self.userEmbNorm = nn.LayerNorm(args.state_user_latent_dim)\n",
        "        self.itemFeatureKernel = nn.Linear(args.state_item_latent_dim, args.state_transformer_enc_dim)\n",
        "        self.userFeatureKernel = nn.Linear(args.state_user_latent_dim, args.state_transformer_enc_dim)\n",
        "        self.encDropout = nn.Dropout(args.state_dropout_rate)\n",
        "        self.encNorm = nn.LayerNorm(args.state_transformer_enc_dim)\n",
        "\n",
        "        # positional embedding\n",
        "        self.max_len = stats['max_seq_len']\n",
        "        self.posEmb = nn.Embedding(self.max_len, args.state_transformer_enc_dim)\n",
        "        self.pos_emb_getter = torch.arange(self.max_len, dtype = torch.long)\n",
        "        self.attn_mask = ~torch.tril(torch.ones((self.max_len,self.max_len), dtype=torch.bool))\n",
        "\n",
        "        # sequence encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=2*args.state_transformer_enc_dim,\n",
        "                                                   dim_feedforward = args.state_transformer_d_forward,\n",
        "                                                   nhead=args.state_transformer_n_head,\n",
        "                                                   dropout = args.state_dropout_rate,\n",
        "                                                   batch_first = True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=args.state_transformer_n_layer)\n",
        "\n",
        "        # DNN state encoder\n",
        "\n",
        "#         self.stateNorm = nn.LayerNorm(self.state_dim)\n",
        "#         self.finalStateLayer = DNN(3*args.state_transformer_enc_dim, args.state_hidden_dims, self.state_dim,\n",
        "#                                 dropout_rate = args.dropout_rate, do_batch_norm = True)\n",
        "\n",
        "        #self.actionModule = torch.nn.Sigmoid(self.actionModule)\n",
        "\n",
        "    def get_forward(self, feed_dict: dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: {\n",
        "            'user_id': (B,)\n",
        "            'uf_{feature_name}': (B,feature_dim), the user features\n",
        "            'history': (B,max_H)\n",
        "            'history_if_{feature_name}': (B,max_H,feature_dim), the history item features\n",
        "        }\n",
        "        @output:\n",
        "        - out_dict: {'state': (B, state_dim),\n",
        "                    'reg': scalar}\n",
        "        '''\n",
        "        B = feed_dict['user_id'].shape[0]\n",
        "        # user encoding\n",
        "        state_encoder_output = self.encode_state(feed_dict, B)\n",
        "        # regularization terms\n",
        "        reg = self.get_regularization(self.feedbackEncoder,\n",
        "                                      self.itemFeatureKernel, self.userFeatureKernel,\n",
        "                                      self.posEmb, self.transformer)\n",
        "        reg = reg + state_encoder_output['reg']\n",
        "\n",
        "        return {'state': state_encoder_output['state'],\n",
        "                'reg': reg}\n",
        "\n",
        "    def encode_state(self, feed_dict, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: {\n",
        "            'user_id': (B,)\n",
        "            'uf_{feature_name}': (B,feature_dim), the user features\n",
        "            'history': (B,max_H)\n",
        "            'history_if_{feature_name}': (B,max_H,feature_dim), the history item features\n",
        "            ... (irrelevant input)\n",
        "        }\n",
        "        - B: batch size\n",
        "        @output:\n",
        "        - out_dict:{\n",
        "            'out_seq': (B,max_H,2*enc_dim)\n",
        "            'state': (B,n_feedback*enc_dim)\n",
        "            'reg': scalar\n",
        "        }\n",
        "        '''\n",
        "        # user history item encodings (B, max_H, enc_dim)\n",
        "        history_enc, history_reg = self.get_item_encoding(feed_dict['history'],\n",
        "                                             {f:feed_dict[f'history_if_{f}'] for f in self.iFeatureEmb}, B)\n",
        "        history_enc = history_enc.view(B, self.max_len, self.enc_dim)\n",
        "\n",
        "        # positional encoding (1, max_H, enc_dim)\n",
        "        pos_emb = self.posEmb(self.pos_emb_getter).view(1,self.max_len,self.enc_dim)\n",
        "\n",
        "        # feedback embedding (B, max_H, enc_dim)\n",
        "        feedback_emb = self.get_response_embedding({f: feed_dict[f'history_{f}'] for f in self.feedback_types}, B)\n",
        "\n",
        "        # sequence item encoding (B, max_H, enc_dim)\n",
        "        seq_enc_feat = self.encNorm(self.encDropout(history_enc + pos_emb))\n",
        "        # (B, max_H, 2*enc_dim)\n",
        "        seq_enc = torch.cat((seq_enc_feat, feedback_emb), dim = -1)\n",
        "\n",
        "        # transformer output (B, max_H, 2*enc_dim)\n",
        "        output_seq = self.transformer(seq_enc, mask = self.attn_mask)\n",
        "\n",
        "        # user history encoding (B, 2*enc_dim)\n",
        "        hist_enc = output_seq[:,-1,:].view(B,2*self.enc_dim)\n",
        "\n",
        "        # static user profile features\n",
        "        # (B, enc_dim), scalar\n",
        "        user_enc, user_reg = self.get_user_encoding(feed_dict['user_id'],\n",
        "                                          {k[3:]:v for k,v in feed_dict.items() if k[:3] == 'uf_'}, B)\n",
        "        # (B, enc_dim)\n",
        "        user_enc = self.encNorm(self.encDropout(user_enc)).view(B,self.enc_dim)\n",
        "\n",
        "        # user state (B, 3*enc_dim) combines user history and user profile features\n",
        "        state = torch.cat([hist_enc,user_enc], 1)\n",
        "        # (B, enc_dim)\n",
        "#         state = self.stateNorm(self.finalStateLayer(state))\n",
        "        return {'output_seq': output_seq, 'state': state, 'reg': user_reg + history_reg}\n",
        "\n",
        "    def get_user_encoding(self, user_ids, user_features, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_ids: (B,)\n",
        "        - user_features: {'uf_{feature_name}': (B, feature_dim)}\n",
        "        @output:\n",
        "        - encoding: (B, enc_dim)\n",
        "        - reg: scalar\n",
        "        '''\n",
        "        # (B, 1, u_latent_dim)\n",
        "        user_id_emb = self.uIDEmb(user_ids).view(B,1,self.user_latent_dim)\n",
        "        # [(B, 1, u_latent_dim)] * n_user_feature\n",
        "        user_feature_emb = [user_id_emb]\n",
        "        for f,fEmbModule in self.uFeatureEmb.items():\n",
        "            user_feature_emb.append(fEmbModule(user_features[f]).view(B,1,self.user_latent_dim))\n",
        "        # (B, n_user_feature+1, u_latent_dim)\n",
        "        combined_user_emb = torch.cat(user_feature_emb, 1)\n",
        "        combined_user_emb = self.userEmbNorm(combined_user_emb)\n",
        "        # (B, enc_dim)\n",
        "        encoding = self.userFeatureKernel(combined_user_emb).sum(1)\n",
        "        # regularization\n",
        "        reg = torch.mean(user_id_emb * user_id_emb)\n",
        "        return encoding, reg\n",
        "\n",
        "    def get_item_encoding(self, item_ids, item_features, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - item_ids: (B,) or (B,L)\n",
        "        - item_features: {'{feature_name}': (B,feature_dim) or (B,L,feature_dim)}\n",
        "        @output:\n",
        "        - encoding: (B, 1, enc_dim) or (B, L, enc_dim)\n",
        "        - reg: scalar\n",
        "        '''\n",
        "        # (B, 1, i_latent_dim) or (B, L, i_latent_dim)\n",
        "        item_id_emb = self.iIDEmb(item_ids).view(B,-1,self.item_latent_dim)\n",
        "        L = item_id_emb.shape[1]\n",
        "        # [(B, 1, i_latent_dim)] * n_item_feature or [(B, L, i_latent_dim)] * n_item_feature\n",
        "        item_feature_emb = [item_id_emb]\n",
        "        for f,fEmbModule in self.iFeatureEmb.items():\n",
        "            f_dim = self.item_feature_dims[f]\n",
        "            item_feature_emb.append(fEmbModule(item_features[f].view(B,L,f_dim)).view(B,-1,self.item_latent_dim))\n",
        "        # (B, 1, n_item_feature+1, i_latent_dim) or (B, L, n_item_feature+1, i_latent_dim)\n",
        "        combined_item_emb = torch.cat(item_feature_emb, -1).view(B, L, -1, self.item_latent_dim)\n",
        "        combined_item_emb = self.itemEmbNorm(combined_item_emb)\n",
        "        # (B, 1, enc_dim) or (B, L, enc_dim)\n",
        "        encoding = self.itemFeatureKernel(combined_item_emb).sum(2)\n",
        "        encoding = self.encNorm(encoding.view(B, -1, self.enc_dim))\n",
        "        # regularization\n",
        "        reg = torch.mean(item_id_emb * item_id_emb)\n",
        "        return encoding, reg\n",
        "\n",
        "    def get_response_embedding(self, resp_dict, B):\n",
        "        '''\n",
        "        @input:\n",
        "        - resp_dict: {'{response}': (B, max_H)}\n",
        "        @output:\n",
        "        - resp_emb: (B, max_H, enc_dim)\n",
        "        '''\n",
        "        resp_list = []\n",
        "        for f in self.feedback_types:\n",
        "            # (B, max_H)\n",
        "            resp = resp_dict[f].view(B, self.max_len)\n",
        "            resp_list.append(resp)\n",
        "        # (B, max_H, n_feedback)\n",
        "        combined_resp = torch.cat(resp_list, -1).view(B,self.max_len,self.feedback_dim)\n",
        "        # (B, max_H, enc_dim)\n",
        "        resp_emb = self.feedbackEncoder(combined_resp)\n",
        "        return resp_emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YQyT9gfsJMI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseOnlinePolicy(BaseModel):\n",
        "    '''\n",
        "    Pointwise model\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - from BackboneUserEncoder:\n",
        "            - user_latent_dim\n",
        "            - item_latent_dim\n",
        "            - transformer_enc_dim\n",
        "            - transformer_n_head\n",
        "            - transformer_d_forward\n",
        "            - transformer_n_layer\n",
        "            - state_hidden_dims\n",
        "            - dropout_rate\n",
        "            - from BaseModel:\n",
        "                - model_path\n",
        "                - loss\n",
        "                - l2_coef\n",
        "        '''\n",
        "        parser = BackboneUserEncoder.parse_model_args(parser)\n",
        "#         parser.add_argument('--score_clip', type=float, default=2.0,\n",
        "#                             help='ranking scores will be [-clip, +clip]')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, args, env, device):\n",
        "        self.slate_size = args.slate_size # from environment arguments\n",
        "        # BaseModel initialization:\n",
        "        # - reader_stats, model_path, loss_type, l2_coef, no_reg, device\n",
        "        # - _define_params(args): enc_dim, state_dim, action_dim\n",
        "#         self.score_clip = args.score_clip\n",
        "        super().__init__(args, env.reader.get_statistics(), device)\n",
        "        self.display_name = \"BaseOnlinePolicy\"\n",
        "\n",
        "    def to(self, device):\n",
        "        new_self = super(BaseOnlinePolicy, self).to(device)\n",
        "        self.userEncoder.device = device\n",
        "        self.userEncoder = self.userEncoder.to(device)\n",
        "        return new_self\n",
        "\n",
        "    def _define_params(self, args, reader_stats):\n",
        "        #self.userEncoder = BackboneUserEncoder(args, self.reader_stats, self.device)\n",
        "        self.userEncoder = BackboneUserEncoder(args, reader_stats)\n",
        "        self.enc_dim = self.userEncoder.enc_dim\n",
        "        self.state_dim = self.userEncoder.state_dim\n",
        "        self.action_dim = self.slate_size\n",
        "\n",
        "#         # user_state2condition layer\n",
        "#         self.state2z = nn.Linear(self.state_dim, self.enc_dim)\n",
        "#         self.zNorm = nn.LayerNorm(self.enc_dim)\n",
        "\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "\n",
        "\n",
        "    def get_forward(self, feed_dict: dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: {\n",
        "            'observation':{\n",
        "                'user_profile':{\n",
        "                    'user_id': (B,)\n",
        "                    'uf_{feature_name}': (B,feature_dim), the user features}\n",
        "                'user_history':{\n",
        "                    'history': (B,max_H)\n",
        "                    'history_if_{feature_name}': (B,max_H,feature_dim), the history item features}\n",
        "            'candidates':{\n",
        "                'item_id': (B,L) or (1,L), the target item\n",
        "                'item_{feature_name}': (B,L,feature_dim) or (1,L,feature_dim), the target item features}\n",
        "            'epsilon': scalar,\n",
        "            'do_explore': boolean,\n",
        "            'candidates': {\n",
        "                'item_id': (B,L) or (1,L), the target item\n",
        "                'item_{feature_name}': (B,L,feature_dim) or (1,L,feature_dim), the target item features},\n",
        "            'action_dim': slate size K,\n",
        "            'action': (B,K),\n",
        "            'response': {\n",
        "                'reward': (B,),\n",
        "                'immediate_response': (B,K*n_feedback)},\n",
        "            'is_train': boolean\n",
        "        }\n",
        "        @output:\n",
        "        - out_dict: {\n",
        "            'state': (B,state_dim),\n",
        "            'prob': (B,K),\n",
        "            'action': (B,K),\n",
        "            'reg': scalar}\n",
        "        '''\n",
        "        observation = feed_dict['observation']\n",
        "#         candidates = feed_dict['candidates']\n",
        "        # observation --> user state\n",
        "        state_encoder_output = self.get_user_state(observation)\n",
        "        # (B, state_dim)\n",
        "        user_state = state_encoder_output['state']\n",
        "        # user state --> prob, action\n",
        "        out_dict = self.generate_action(user_state, feed_dict)\n",
        "\n",
        "        out_dict['state'] = user_state\n",
        "        out_dict['reg'] = state_encoder_output['reg'] + out_dict['reg']\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "    def get_user_state(self, observation):\n",
        "        feed_dict = {}\n",
        "        feed_dict.update(observation['user_profile'])\n",
        "        feed_dict.update(observation['user_history'])\n",
        "        B = feed_dict['user_id'].shape[0]\n",
        "        return self.userEncoder(feed_dict, B)\n",
        "\n",
        "    def get_loss_observation(self):\n",
        "        return ['loss']\n",
        "\n",
        "    def generate_action(self, user_state, feed_dict):\n",
        "        '''\n",
        "        This function will be called in the following places:\n",
        "        * OnlineAgent.run_episode_step() with {'action': None, 'response': None,\n",
        "                                               'epsilon': >0, 'do_explore': True, 'is_train': False}\n",
        "        * OnlineAgent.step_train() with {'action': tensor, 'response': {'reward': , 'immediate_response': },\n",
        "                                         'epsilon': 0, 'do_explore': False, 'is_train': True}\n",
        "        * OnlineAgent.test() with {'action': None, 'response': None,\n",
        "                                   'epsilon': 0, 'do_explore': False, 'is_train': False}\n",
        "\n",
        "        @input:\n",
        "        - user_state\n",
        "        - feed_dict\n",
        "        @output:\n",
        "        - out_dict: {'prob': (B, K),\n",
        "                     'action': (B, K),\n",
        "                     'reg': scalar}\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    def get_loss(self, feed_dict, out_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: same as get_forward@input-feed_dict\n",
        "        - out_dict: {\n",
        "            'state': (B,state_dim),\n",
        "            'prob': (B,K),\n",
        "            'action': (B,K),\n",
        "            'reg': scalar,\n",
        "            'immediate_response': (B,K),\n",
        "            'reward': (B,)}\n",
        "        @output\n",
        "        - loss\n",
        "        '''\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGhG2VnwsAyD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TwoStageOnlinePolicy(BaseOnlinePolicy):\n",
        "    '''\n",
        "    Pointwise model\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - initial_list_size\n",
        "        - stage1_state2z_hidden_dims\n",
        "        - stage1_pos_offset\n",
        "        - stage1_neg_offset\n",
        "        - initial_loss_coef\n",
        "        - from BackboneUserEncoder:\n",
        "            - user_latent_dim\n",
        "            - item_latent_dim\n",
        "            - transformer_enc_dim\n",
        "            - transformer_n_head\n",
        "            - transformer_d_forward\n",
        "            - transformer_n_layer\n",
        "            - state_hidden_dims\n",
        "            - dropout_rate\n",
        "            - from BaseModel:\n",
        "                - model_path\n",
        "                - loss\n",
        "                - l2_coef\n",
        "        '''\n",
        "        parser = BaseOnlinePolicy.parse_model_args(parser)\n",
        "        parser.add_argument('--initial_list_size', type=int, default=50,\n",
        "                            help='candidate list size after initial ranker')\n",
        "        parser.add_argument('--stage1_state2z_hidden_dims', type=int, nargs=\"+\", default=[128],\n",
        "                            help='hidden dimensions of state_slate encoding layers')\n",
        "        parser.add_argument('--stage1_pos_offset', type=float, default=0.8,\n",
        "                            help='smooth offset of positive prob')\n",
        "        parser.add_argument('--stage1_neg_offset', type=float, default=0.1,\n",
        "                            help='smooth offset of negative prob')\n",
        "        parser.add_argument('--initial_loss_coef', type=float, default=0.1,\n",
        "                            help='relative importance of training loss of initial ranker')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, args, env, device):\n",
        "        self.initial_list_size = args.initial_list_size\n",
        "        self.stage1_state2z_hidden_dims = args.stage1_state2z_hidden_dims\n",
        "        self.stage1_pos_offset = args.stage1_pos_offset\n",
        "        self.stage1_neg_offset = args.stage1_neg_offset\n",
        "        self.initial_loss_coef = args.initial_loss_coef\n",
        "        # BaseOnlinePolicy initialization:\n",
        "        # - reader_stats, model_path, loss_type, l2_coef, no_reg, device, slate_size\n",
        "        # - _define_params(args): userEncoder, enc_dim, state_dim, action_dim\n",
        "        super().__init__(args, env, device)\n",
        "        self.display_name = \"TwoStageOnlinePolicy\"\n",
        "        self.train_initial = True\n",
        "        self.train_rerank = True\n",
        "\n",
        "    def to(self, device):\n",
        "        new_self = super(TwoStageOnlinePolicy, self).to(device)\n",
        "        return new_self\n",
        "\n",
        "    def _define_params(self, args):\n",
        "        '''\n",
        "        Default two stage policy (pointwise initial ranker + no reranking)\n",
        "        '''\n",
        "        # userEncoder, enc_dim, state_dim, action_dim\n",
        "        super()._define_params(args)\n",
        "        # p_forward\n",
        "        self.stage1State2Z = DNN(self.state_dim, args.stage1_state2z_hidden_dims, self.enc_dim,\n",
        "                           dropout_rate = args.dropout_rate, do_batch_norm = True)\n",
        "        self.stage1ZNorm = nn.LayerNorm(self.enc_dim)\n",
        "\n",
        "    def generate_action(self, user_state, feed_dict):\n",
        "        '''\n",
        "        This function will be called in the following places:\n",
        "        * OnlineAgent.run_episode_step() with {'action': None, 'response': None,\n",
        "                                               'epsilon': >0, 'do_explore': True, 'is_train': False}\n",
        "        * OnlineAgent.step_train() with {'action': tensor, 'response': {'reward': , 'immediate_response': },\n",
        "                                         'epsilon': 0, 'do_explore': False, 'is_train': True}\n",
        "        * OnlineAgent.test() with {'action': None, 'response': None,\n",
        "                                   'epsilon': 0, 'do_explore': False, 'is_train': False}\n",
        "\n",
        "        @input:\n",
        "        - user_state\n",
        "        - feed_dict\n",
        "        @output:\n",
        "        - out_dict: {'prob': (B, K),\n",
        "                     'action': (B, K),\n",
        "                     'reg': scalar}\n",
        "        '''\n",
        "        # batch-wise candidates has shape (B,L), non-batch-wise candidates has shape (1,L)\n",
        "        batch_wise = True\n",
        "        if feed_dict['candidates']['item_id'].shape[0] == 1:\n",
        "            batch_wise = False\n",
        "        feed_dict['do_batch_wise'] = batch_wise\n",
        "        # during training, candidates is always the full item set and has shape (1,L) where L=N\n",
        "        if feed_dict['is_train']:\n",
        "            assert not batch_wise\n",
        "        do_uniform = np.random.random() < feed_dict['epsilon']\n",
        "        feed_dict['do_uniform'] = do_uniform\n",
        "\n",
        "        initial_out_dict = self.generate_initial_rank(user_state, feed_dict)\n",
        "        out_dict = self.generate_final_action(user_state, feed_dict, initial_out_dict)\n",
        "        out_dict['initial_prob'] = initial_out_dict['initial_prob']\n",
        "        out_dict['initial_action'] = initial_out_dict['initial_action']\n",
        "        out_dict['reg'] = initial_out_dict['reg'] + out_dict['reg']\n",
        "        return out_dict\n",
        "\n",
        "    def generate_initial_rank(self, user_state, feed_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_state: (B, state_dim)\n",
        "        - feed_dict: same as BaseOnlinePolicy.get_forward@feed_dict\n",
        "        @output:\n",
        "        - out_dict: {'initial_prob': the initial list's item probabilities, (B, K) if training, (B, C) in inference,\n",
        "                     'initial_action': the initial list, (B, K) if training, (B, C) if inference,\n",
        "                     'candidate_item_enc': (B, L, enc_dim),\n",
        "                     'reg': scalar}\n",
        "        '''\n",
        "        candidates = feed_dict['candidates']\n",
        "        slate_size = feed_dict['action_dim']\n",
        "        action_slate = feed_dict['action'] # (B, K)\n",
        "        do_explore = feed_dict['do_explore']\n",
        "        do_uniform = feed_dict['do_uniform']\n",
        "        epsilon = feed_dict['epsilon']\n",
        "        is_train = feed_dict['is_train']\n",
        "        batch_wise = feed_dict['do_batch_wise']\n",
        "\n",
        "        B = user_state.shape[0]\n",
        "        # (1,L,enc_dim) or (B,L,enc_dim)\n",
        "        candidate_item_enc, reg = self.userEncoder.get_item_encoding(candidates['item_id'],\n",
        "                                                       {k[5:]: v for k,v in candidates.items() if k != 'item_id'},\n",
        "                                                                     B if batch_wise else 1)\n",
        "        # (B, enc_dim)\n",
        "        Z = self.stage1State2Z(user_state)\n",
        "        Z = self.stage1ZNorm(Z)\n",
        "        # (B, L)\n",
        "        score = torch.sum(Z.view(B,1,self.enc_dim) * candidate_item_enc, dim = -1) #/ self.enc_dim\n",
        "#         score = torch.clamp(score, -self.score_clip, self.score_clip)\n",
        "\n",
        "        if is_train or torch.is_tensor(action_slate):\n",
        "            stage1_n_neg = self.initial_list_size - self.slate_size\n",
        "            # (B, C-K)\n",
        "            neg_indices = Categorical(torch.ones_like(score)).sample((stage1_n_neg,)).transpose(0,1)\n",
        "            # (B, C)\n",
        "            indices = torch.cat((action_slate, neg_indices), dim = 1)\n",
        "            score = torch.gather(score, 1, indices)\n",
        "            prob = torch.softmax(score, dim = 1)\n",
        "            selected_P = prob\n",
        "            initial_action = indices\n",
        "            # scalar\n",
        "            reg = self.get_regularization(self.stage1State2Z)\n",
        "        else:\n",
        "            # (B, L)\n",
        "            prob = torch.softmax(score, dim = 1)\n",
        "            if do_explore:\n",
        "                # exploration: categorical sampling or uniform sampling\n",
        "                if do_uniform:\n",
        "                    indices = Categorical(torch.ones_like(prob)).sample((self.initial_list_size,)).transpose(0,1)\n",
        "                else:\n",
        "                    indices = Categorical(prob).sample((self.initial_list_size,)).transpose(0,1)\n",
        "            else:\n",
        "                # greedy: topk selection\n",
        "                _, indices = torch.topk(prob, k = self.initial_list_size, dim = 1)\n",
        "            # (B, C)\n",
        "            indices = indices.view(-1,self.initial_list_size).detach()\n",
        "\n",
        "            selected_P = torch.gather(prob,1,indices)\n",
        "            # slate action (B, K) if training or (B, C) if inference\n",
        "            initial_action = indices\n",
        "            reg = 0\n",
        "\n",
        "        out_dict = {'initial_prob': selected_P, # (B, C)\n",
        "                    'initial_action': initial_action, # (B, C)\n",
        "                    'candidate_item_enc': candidate_item_enc, # (1, L, enc_dim)\n",
        "                    'reg': reg}\n",
        "        return out_dict\n",
        "\n",
        "    def generate_final_action(self, user_state, feed_dict, initial_out_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_state: (B, state_dim)\n",
        "        - feed_dict: same as BaseOnlinePolicy.get_forward@input-feed_dict\n",
        "        - initial_out_dict: TwoStageOnlinePolicy.generate_initial_rank@output-out_dict\n",
        "        @output:\n",
        "        - out_dict: {\n",
        "            prob: (B, K),\n",
        "            action: (B, K),\n",
        "            reg: scalar\n",
        "        }\n",
        "        '''\n",
        "\n",
        "        B = user_state.shape[0]\n",
        "        prob = initial_out_dict['initial_prob'][:,:self.slate_size].detach()\n",
        "        slate_action = initial_out_dict['initial_action'][:,:self.slate_size].detach()\n",
        "\n",
        "#         # (B, K)\n",
        "#         selected_P = prob[:,:self.slate_size]\n",
        "#         # (B, K)\n",
        "#         initial_action = action_slate\n",
        "        reg = 0\n",
        "        return {'prob': prob,\n",
        "                'action': slate_action,\n",
        "                'reg': reg}\n",
        "\n",
        "    def get_loss_observation(self):\n",
        "        return ['loss', 'initial_loss', 'rerank_loss']\n",
        "\n",
        "    def get_loss(self, feed_dict, out_dict):\n",
        "        '''\n",
        "        Reward-based pointwise ranking loss\n",
        "        * - Ylog(P) - (1-Y)log(1-P)\n",
        "        * Y = sum(w[i] * r[i]) # the weighted sum of user responses\n",
        "\n",
        "        @input:\n",
        "        - feed_dict: same as BaseOnlinePolicy.get_forward@input-feed_dict\n",
        "        - out_dict: {\n",
        "            'state': (B,state_dim),\n",
        "            'initial_prob': (B,C),\n",
        "            'initial_action': (B,C),\n",
        "            'prob': (B,K),\n",
        "            'action': (B,K),\n",
        "            'reg': scalar,\n",
        "            'immediate_response': (B,K*n_feedback),\n",
        "            'immediate_response_weight: (n_feedback, ),\n",
        "            'reward': (B,)}\n",
        "        @output\n",
        "        - loss\n",
        "        '''\n",
        "        B = out_dict['prob'].shape[0]\n",
        "        # (B, K)\n",
        "        initial_prob = out_dict['initial_prob'][:,:self.slate_size]\n",
        "\n",
        "        if self.train_initial:\n",
        "            # training of initial ranker\n",
        "            # (B,K,n_feedback)\n",
        "            weighted_response = out_dict['immediate_response'].view(B,self.slate_size,-1) \\\n",
        "                                    * out_dict['immediate_response_weight'].view(1,1,-1)\n",
        "            # (B,K)\n",
        "            Y = torch.mean(weighted_response, dim = 2)\n",
        "            initial_loss = self.get_reward_bce(initial_prob, Y)\n",
        "        else:\n",
        "            initial_loss = torch.tensor(0)\n",
        "\n",
        "        if self.train_rerank:\n",
        "            # training of reranker\n",
        "            rerank_loss = torch.zeros_like(initial_loss)\n",
        "        else:\n",
        "            rerank_loss = torch.tensor(0)\n",
        "\n",
        "        # scalar\n",
        "        loss = self.initial_loss_coef * initial_loss + rerank_loss + self.l2_coef * out_dict['reg']\n",
        "\n",
        "\n",
        "#         print('log(P):', torch.mean(log_P), torch.var(log_P))\n",
        "#         print('log(1-P):', torch.mean(log_neg_P), torch.var(log_neg_P))\n",
        "#         print('Y:', torch.mean(Y), torch.var(Y))\n",
        "#         print('loss:', torch.mean(R_loss), torch.var(R_loss))\n",
        "#         input()\n",
        "        return {'initial_loss': loss, 'rerank_loss': rerank_loss, 'loss': loss}\n",
        "\n",
        "    def get_reward_bce(self, prob, y):\n",
        "        # (B, K)\n",
        "        log_P = torch.log(prob + self.stage1_pos_offset)\n",
        "        # (B, K)\n",
        "        log_neg_P = torch.log(1 - prob + self.stage1_neg_offset)\n",
        "        # (B, K)\n",
        "        L = - torch.mean(y * log_P + (1-y) * log_neg_P)\n",
        "        return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q33FnzdrZbB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PRM(TwoStageOnlinePolicy):\n",
        "    '''\n",
        "    Pointwise model\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - from TwoStageOnlinePolicy:\n",
        "            - initial_list_size\n",
        "            - stage1_n_neg\n",
        "            - stage1_state2z_hidden_dims\n",
        "            - stage1_pos_offset\n",
        "            - stage1_neg_offset\n",
        "            - initial_loss_coef\n",
        "            - from BackboneUserEncoder:\n",
        "                - user_latent_dim\n",
        "                - item_latent_dim\n",
        "                - transformer_enc_dim\n",
        "                - transformer_n_head\n",
        "                - transformer_d_forward\n",
        "                - transformer_n_layer\n",
        "                - state_hidden_dims\n",
        "                - dropout_rate\n",
        "                - from BaseModel:\n",
        "                    - model_path\n",
        "                    - loss\n",
        "                    - l2_coef\n",
        "        '''\n",
        "        parser = TwoStageOnlinePolicy.parse_model_args(parser)\n",
        "        parser.add_argument('--prm_pv_input_dim', type=int, default=32,\n",
        "                            help='input size of PV module of PRM')\n",
        "        parser.add_argument('--prm_pv_hidden_dims', type=int, nargs=\"+\", default=[128],\n",
        "                            help='hidden dims of PV module of PRM')\n",
        "        parser.add_argument('--prm_encoder_enc_dim', type=int, default=32,\n",
        "                            help='item encoding size of PRM')\n",
        "        parser.add_argument('--prm_encoder_n_head', type=int, default=4,\n",
        "                            help='number of attention heads in transformer of PRM')\n",
        "        parser.add_argument('--prm_encoder_d_forward', type=int, default=64,\n",
        "                            help='forward layer dimension in transformer of PRM')\n",
        "        parser.add_argument('--prm_encoder_n_layer', type=int, default=2,\n",
        "                            help='number of encoder layers in transformer of PRM')\n",
        "        parser.add_argument('--prm_pv_loss_coef', type=float, default=1.0,\n",
        "                            help='relative coefficient of pv loss')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, args, env, device):\n",
        "        # TwoStageOnlinePolicy initialization:\n",
        "        # - initial_list_size, stage1_n_neg, stage1_state2z_hidden_dims, stage1_pos_offset, stage1_neg_offset, initial_loss_coef\n",
        "        # - reader_stats, model_path, loss_type, l2_coef, no_reg, device, slate_size\n",
        "        # - _define_params(args): userEncoder, enc_dim, state_dim, action_dim\n",
        "        self.prm_pv_input_dim = args.prm_pv_input_dim\n",
        "        self.prm_pv_hidden_dims = args.prm_pv_hidden_dims\n",
        "        self.prm_encoder_enc_dim = args.prm_encoder_enc_dim\n",
        "        self.prm_encoder_n_head = args.prm_encoder_n_head\n",
        "        self.prm_encoder_d_forward = args.prm_encoder_d_forward\n",
        "        self.prm_encoder_n_layer = args.prm_encoder_n_layer\n",
        "        self.prm_pv_loss_coef = args.prm_pv_loss_coef\n",
        "        super().__init__(args, env, device)\n",
        "        self.display_name = \"PRM\"\n",
        "\n",
        "    def to(self, device):\n",
        "        new_self = super(PRM, self).to(device)\n",
        "        new_self.PV_attn_mask = new_self.PV_attn_mask.to(device)\n",
        "        new_self.PV_pos_emb_getter = new_self.PV_pos_emb_getter.to(device)\n",
        "        return new_self\n",
        "\n",
        "    def _define_params(self, args):\n",
        "        '''\n",
        "        Default two stage policy (pointwise initial ranker + no reranking)\n",
        "        '''\n",
        "        # stage1State2Z, stage1ZNorm, userEncoder, enc_dim, state_dim, action_dim\n",
        "        super()._define_params(args)\n",
        "\n",
        "        # input layer of PRM (personalized vector model + pos emb)\n",
        "        # personalized vector model\n",
        "        self.PVUserInputMap = nn.Linear(self.state_dim, args.prm_pv_input_dim)\n",
        "        self.PVItemInputMap = nn.Linear(self.enc_dim, args.prm_pv_input_dim)\n",
        "        self.PVInputNorm = nn.LayerNorm(args.prm_pv_input_dim)\n",
        "        self.PVOutput = DNN(args.prm_pv_input_dim, args.prm_pv_hidden_dims, args.prm_encoder_enc_dim,\n",
        "                            dropout_rate = args.dropout_rate, do_batch_norm = True)\n",
        "        # label prediction model\n",
        "        self.PVPred = nn.Linear(args.prm_encoder_enc_dim, 1)\n",
        "        # positional embedding\n",
        "        self.PVPosEmb = nn.Embedding(self.initial_list_size, args.prm_encoder_enc_dim)\n",
        "        self.PV_pos_emb_getter = torch.arange(self.initial_list_size, dtype = torch.long)\n",
        "        self.PV_attn_mask = ~torch.tril(torch.ones((self.initial_list_size,self.initial_list_size), dtype=torch.bool))\n",
        "\n",
        "        # encoding layer of PRM (transformer)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=args.prm_encoder_enc_dim,\n",
        "                                                   dim_feedforward = args.prm_encoder_d_forward,\n",
        "                                                   nhead=args.prm_encoder_n_head, dropout = args.dropout_rate,\n",
        "                                                   batch_first = True)\n",
        "        self.PRMEncoder = nn.TransformerEncoder(encoder_layer, num_layers=args.prm_encoder_n_layer)\n",
        "\n",
        "        # output layer of PRM\n",
        "        self.PRMOutput = nn.Linear(args.prm_encoder_enc_dim, 1)\n",
        "\n",
        "    def generate_final_action(self, user_state, feed_dict, initial_out_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - user_state: (B, state_dim)\n",
        "        - feed_dict: same as BaseOnlinePolicy.get_forward@input-feed_dict\n",
        "        - initial_out_dict: TwoStageOnlinePolicy.generate_initial_rank@output-out_dict\n",
        "        @output:\n",
        "        - out_dict: {\n",
        "\n",
        "        }\n",
        "        '''\n",
        "\n",
        "        do_explore = feed_dict['do_explore']\n",
        "        do_uniform = feed_dict['do_uniform']\n",
        "        epsilon = feed_dict['epsilon']\n",
        "        is_train = feed_dict['is_train']\n",
        "        action_slate = feed_dict['action']\n",
        "\n",
        "        # batch size\n",
        "        B = user_state.shape[0]\n",
        "        # initial list (B, C), the first K correspond to the observed slate if training\n",
        "        initial_prob = initial_out_dict['initial_prob'].detach()\n",
        "        initial_action = initial_out_dict['initial_action'].detach()\n",
        "        candidates = initial_action\n",
        "\n",
        "        # (1, L, enc_dim)\n",
        "        candidate_item_emb = initial_out_dict['candidate_item_enc']\n",
        "        # (B, C, enc_dim)\n",
        "        initial_item_emb = candidate_item_emb.view(-1, self.enc_dim)[initial_action].detach()\n",
        "\n",
        "        # input layer\n",
        "        # (B, 1, pv_input_dim)\n",
        "        user_input = self.PVUserInputMap(user_state).view(B,1,self.prm_pv_input_dim)\n",
        "        user_input = self.PVInputNorm(user_input)\n",
        "        # (B, C, pv_input_dim)\n",
        "        item_input = self.PVItemInputMap(initial_item_emb.view(B*self.initial_list_size,self.enc_dim))\\\n",
        "                            .view(B,self.initial_list_size,self.prm_pv_input_dim)\n",
        "        item_input = self.PVInputNorm(item_input)\n",
        "        # (B, C, pv_input_dim)\n",
        "        pv_ui_input = user_input + item_input\n",
        "        # (B, C, pv_enc_dim)\n",
        "        pv_ui_enc = self.PVOutput(pv_ui_input).view(B,self.initial_list_size,self.prm_encoder_enc_dim)\n",
        "        # positional encoding (1, C, pv_enc_dim)\n",
        "        pos_emb = self.PVPosEmb(self.PV_pos_emb_getter).view(1,self.initial_list_size,self.prm_encoder_enc_dim)\n",
        "        # (B, C, pv_enc_dim)\n",
        "        pv_E = pv_ui_enc + pos_emb\n",
        "\n",
        "        # PRM transformer encoder output (B, C, enc_dim)\n",
        "        PRM_encoder_output = self.PRMEncoder(pv_E, mask = self.PV_attn_mask)\n",
        "\n",
        "        # PRM reranked score (B, C)\n",
        "        rerank_score = self.PRMOutput(PRM_encoder_output.view(B*self.initial_list_size,self.prm_encoder_enc_dim))\\\n",
        "                                    .view(B,self.initial_list_size)\n",
        "        rerank_prob = torch.softmax(rerank_score, dim = 1)\n",
        "\n",
        "        if is_train or torch.is_tensor(action_slate):\n",
        "            # (B, K)\n",
        "            final_action = action_slate\n",
        "            # (B, K)\n",
        "            selected_P = rerank_prob[:,:self.slate_size]\n",
        "            # label prediction (B, C)\n",
        "            Y = self.PVPred(pv_E.view(B*self.initial_list_size,self.prm_encoder_enc_dim))\\\n",
        "                        .view(B,self.initial_list_size)\n",
        "            # (B, K)\n",
        "            selected_Y = Y[:,:self.slate_size]\n",
        "            reg = self.get_regularization(self.PVUserInputMap, self.PVItemInputMap, self.PVOutput,\n",
        "                                          self.PVPred, self.PRMEncoder, self.PRMOutput)\n",
        "            reg = reg + torch.mean(pos_emb * pos_emb)\n",
        "        else:\n",
        "            if do_explore:\n",
        "                # exploration: categorical sampling or uniform sampling\n",
        "                if do_uniform:\n",
        "                    indices = Categorical(torch.ones_like(rerank_prob)).sample((self.slate_size,)).transpose(0,1)\n",
        "                else:\n",
        "                    indices = Categorical(rerank_prob).sample((self.slate_size,)).transpose(0,1)\n",
        "            else:\n",
        "                # greedy: topk selection\n",
        "                _, indices = torch.topk(rerank_prob, k = self.slate_size, dim = 1)\n",
        "            indices = indices.view(-1,self.slate_size).detach()\n",
        "            selected_P = torch.gather(rerank_prob,1,indices)\n",
        "            final_action = torch.gather(initial_action,1,indices)\n",
        "            selected_Y = None\n",
        "            reg = 0\n",
        "\n",
        "\n",
        "        return {'prob': selected_P,\n",
        "                'action': final_action,\n",
        "                'reward_pred': selected_Y,\n",
        "                'reg': reg}\n",
        "\n",
        "    def get_loss_observation(self):\n",
        "        return ['loss', 'initial_loss', 'rerank_loss', 'pv_loss']\n",
        "\n",
        "    def get_loss(self, feed_dict, out_dict):\n",
        "        '''\n",
        "        Reward-based pointwise ranking loss\n",
        "        * - Ylog(P) - (1-Y)log(1-P)\n",
        "        * Y = sum(w[i] * r[i]) # the weighted sum of user responses\n",
        "\n",
        "        @input:\n",
        "        - feed_dict: same as BaseOnlinePolicy.get_forward@input-feed_dict\n",
        "        - out_dict: {\n",
        "            'state': (B,state_dim),\n",
        "            'initial_prob': (B,K),\n",
        "            'initial_action': (B,K),\n",
        "            'prob': (B,K),\n",
        "            'action': (B,K),\n",
        "            'reward_pred': (B,K),\n",
        "            'reg': scalar,\n",
        "            'immediate_response': (B,K*n_feedback),\n",
        "            'immediate_response_weight: (n_feedback, ),\n",
        "            'reward': (B,)}\n",
        "        @output\n",
        "        - loss\n",
        "        '''\n",
        "        B = out_dict['prob'].shape[0]\n",
        "\n",
        "        # training of initial ranker\n",
        "        # (B,K,n_feedback)\n",
        "        weighted_response = out_dict['immediate_response'].view(B,self.slate_size,-1) \\\n",
        "                                * out_dict['immediate_response_weight'].view(1,1,-1)\n",
        "        # (B,K)\n",
        "        Y = torch.mean(weighted_response, dim = 2)\n",
        "\n",
        "        if self.train_initial:\n",
        "            # initial ranker loss\n",
        "            initial_loss = self.get_reward_bce(out_dict['initial_prob'][:,:self.slate_size], Y)\n",
        "        else:\n",
        "            initial_loss = torch.tensor(0)\n",
        "\n",
        "        if self.train_rerank:\n",
        "            # reranker loss\n",
        "            rerank_loss = self.get_reward_bce(out_dict['prob'], Y)\n",
        "            # pv loss\n",
        "            pv_loss = torch.mean((out_dict['reward_pred'] - Y).pow(2))\n",
        "        else:\n",
        "            rerank_loss, pv_loss = torch.tensor(0), torch.tensor(0)\n",
        "\n",
        "        # scalar\n",
        "        loss = self.initial_loss_coef * initial_loss + rerank_loss \\\n",
        "                    + self.prm_pv_loss_coef * pv_loss + self.l2_coef * out_dict['reg']\n",
        "\n",
        "\n",
        "#         print('log(P):', torch.mean(log_P), torch.var(log_P))\n",
        "#         print('log(1-P):', torch.mean(log_neg_P), torch.var(log_neg_P))\n",
        "#         print('Y:', torch.mean(Y), torch.var(Y))\n",
        "#         print('loss:', torch.mean(R_loss), torch.var(R_loss))\n",
        "#         input()\n",
        "        return {'initial_loss': loss,\n",
        "                'rerank_loss': rerank_loss,\n",
        "                'pv_loss': pv_loss,\n",
        "                'loss': loss}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ9UCFt8mHl7",
        "outputId": "2866ada9-5c05-42ec-95ad-78474f12ffcd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# 1) make an adapter that fixes the `_define_params` signature\n",
        "class PRMAdapter(PRM):\n",
        "    def _define_params(self, args, reader_stats):\n",
        "        \"\"\"\n",
        "        Reproduce the logic of:\n",
        "        - BaseOnlinePolicy._define_params(args, reader_stats)\n",
        "        - TwoStageOnlinePolicy._define_params(args)\n",
        "        - PRM._define_params(args)\n",
        "        but in one method that has the correct signature.\n",
        "        \"\"\"\n",
        "        # 1. base online policy: builds userEncoder, enc_dim, state_dim, action_dim\n",
        "        BaseOnlinePolicy._define_params(self, args, reader_stats)\n",
        "\n",
        "        # figure out dropout name (your earlier args used state_dropout_rate)\n",
        "        dr = getattr(args, \"dropout_rate\", getattr(args, \"state_dropout_rate\", 0.1))\n",
        "\n",
        "        # 2. two-stage part (this is what TwoStageOnlinePolicy._define_params did)\n",
        "        self.stage1State2Z = DNN(\n",
        "            self.state_dim,\n",
        "            args.stage1_state2z_hidden_dims,\n",
        "            self.enc_dim,\n",
        "            dropout_rate=dr,\n",
        "            do_batch_norm=True\n",
        "        )\n",
        "        self.stage1ZNorm = nn.LayerNorm(self.enc_dim)\n",
        "\n",
        "        # 3. PRM-specific part (copy of your PRM._define_params)\n",
        "        # personalized vector model\n",
        "        self.PVUserInputMap = nn.Linear(self.state_dim, args.prm_pv_input_dim)\n",
        "        self.PVItemInputMap = nn.Linear(self.enc_dim, args.prm_pv_input_dim)\n",
        "        self.PVInputNorm = nn.LayerNorm(args.prm_pv_input_dim)\n",
        "        self.PVOutput = DNN(\n",
        "            args.prm_pv_input_dim,\n",
        "            args.prm_pv_hidden_dims,\n",
        "            args.prm_encoder_enc_dim,\n",
        "            dropout_rate=dr,\n",
        "            do_batch_norm=True\n",
        "        )\n",
        "        # label prediction model\n",
        "        self.PVPred = nn.Linear(args.prm_encoder_enc_dim, 1)\n",
        "        # positional embedding\n",
        "        self.PVPosEmb = nn.Embedding(self.initial_list_size, args.prm_encoder_enc_dim)\n",
        "        self.PV_pos_emb_getter = torch.arange(self.initial_list_size, dtype=torch.long)\n",
        "        self.PV_attn_mask = ~torch.tril(\n",
        "            torch.ones((self.initial_list_size, self.initial_list_size), dtype=torch.bool)\n",
        "        )\n",
        "\n",
        "        # encoding layer of PRM (transformer)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=args.prm_encoder_enc_dim,\n",
        "            dim_feedforward=args.prm_encoder_d_forward,\n",
        "            nhead=args.prm_encoder_n_head,\n",
        "            dropout=dr,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.PRMEncoder = nn.TransformerEncoder(encoder_layer, num_layers=args.prm_encoder_n_layer)\n",
        "\n",
        "        # output layer of PRM\n",
        "        self.PRMOutput = nn.Linear(args.prm_encoder_enc_dim, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36YDydn3syoJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class QCritic(nn.Module):\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - critic_hidden_dims\n",
        "        - critic_dropout_rate\n",
        "        '''\n",
        "        parser.add_argument('--critic_hidden_dims', type=int, nargs='+', default=[128],\n",
        "                            help='specificy a list of k for top-k performance')\n",
        "        parser.add_argument('--critic_dropout_rate', type=float, default=0.1,\n",
        "                            help='dropout rate in deep layers')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, args, environment, policy):\n",
        "        super().__init__()\n",
        "        self.state_dim = policy.state_dim\n",
        "        self.action_dim = policy.action_dim\n",
        "        self.net = DNN(self.state_dim + self.action_dim, args.critic_hidden_dims, 1,\n",
        "                       dropout_rate = args.critic_dropout_rate, do_batch_norm = True)\n",
        "\n",
        "    def forward(self, feed_dict):\n",
        "        '''\n",
        "        @input:\n",
        "        - feed_dict: {'state': (B, state_dim), 'action': (B, action_dim)}\n",
        "        '''\n",
        "        state_emb = feed_dict['state'].view(-1, self.state_dim)\n",
        "        action_emb = feed_dict['action'].view(-1, self.action_dim)\n",
        "        Q = self.net(torch.cat((state_emb, action_emb), dim = -1)).view(-1)\n",
        "        reg = get_regularization(self.net)\n",
        "        return {'q': Q, 'reg': reg}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9NArO7krNnR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BaseBuffer():\n",
        "    '''\n",
        "    The general buffer\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - buffer_size\n",
        "        '''\n",
        "        parser.add_argument('--buffer_size', type=int, default=10000,\n",
        "                            help='replay buffer size')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, *input_args):\n",
        "        args, env, policy, critic = input_args\n",
        "        self.buffer_size = args.buffer_size\n",
        "        super().__init__()\n",
        "        self.device = args.device\n",
        "        self.buffer_head = 0\n",
        "        self.current_buffer_size = 0\n",
        "        self.n_stream_record = 0\n",
        "\n",
        "    def reset(self, *reset_args):\n",
        "        '''\n",
        "        @output:\n",
        "        - buffer: {'observation': {'user_profile': {'user_id': (L,),\n",
        "                                                    'uf_{feature_name}': (L, feature_dim)},\n",
        "                                   'user_history': {'history': (L, max_H),\n",
        "                                                    'history_if_{feature_name}': (L, max_H * feature_dim),\n",
        "                                                    'history_{response}': (L, max_H),\n",
        "                                                    'history_length': (L,)}}\n",
        "                   'policy_output': {'state': (L, state_dim),\n",
        "                                     'action': (L, action_dim),\n",
        "                                     'prob': (L, slate_size)},\n",
        "                   'next_observation': same format as @output-buffer['observation'],\n",
        "                   'done_mask': (L,),\n",
        "                   'response': {'reward': (L,),\n",
        "                                'immediate_response':, (L, slate_size * response_dim)}}\n",
        "        '''\n",
        "        env = reset_args[0]\n",
        "        actor = reset_args[1]\n",
        "        observation = env.create_observation_buffer(self.buffer_size)\n",
        "        next_observation = env.create_observation_buffer(self.buffer_size)\n",
        "        policy_output = {'state': torch.zeros(self.buffer_size, actor.state_dim)\\\n",
        "                                         .to(torch.float).to(self.device),\n",
        "                         'action': torch.zeros(self.buffer_size, actor.action_dim)\\\n",
        "                                         .to(torch.long).to(self.device),\n",
        "                         'prob': torch.zeros(self.buffer_size, env.slate_size)\\\n",
        "                                         .to(torch.float).to(self.device)}\n",
        "        reward = torch.zeros(self.buffer_size).to(torch.float).to(self.device)\n",
        "        done = torch.zeros(self.buffer_size).to(torch.bool).to(self.device)\n",
        "        im_response = torch.zeros(self.buffer_size, env.response_dim * env.slate_size)\\\n",
        "                                         .to(torch.float).to(self.device)\n",
        "        self.buffer = {'observation': observation,\n",
        "                       'policy_output': policy_output,\n",
        "                       'user_response': {'reward': reward, 'immediate_response': im_response},\n",
        "                       'done_mask': done,\n",
        "                       'next_observation': next_observation}\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Batch sample is organized as a tuple of (observation, policy_output, user_response, done_mask, next_observation)\n",
        "\n",
        "        Buffer: see reset@output\n",
        "        @output:\n",
        "        - observation: {'user_profile': {'user_id': (B,),\n",
        "                                         'uf_{feature_name}': (B, feature_dim)},\n",
        "                        'user_history': {'history': (B, max_H),\n",
        "                                         'history_if_{feature_name}': (B, max_H * feature_dim),\n",
        "                                         'history_{response}': (B, max_H),\n",
        "                                         'history_length': (B,)}}\n",
        "        - policy_output: {'state': (B, state_dim),\n",
        "                          'action': (B, slate_size),\n",
        "                          'prob': (B, slate_size)},\n",
        "        - user_feedback: {'reward': (B,),\n",
        "                          'immediate_response':, (B, slate_size * response_dim)}}\n",
        "        - done_mask: (B,),\n",
        "        - next_observation: same format as @output - observation,\n",
        "        '''\n",
        "        # get indices\n",
        "        indices = np.random.randint(0, self.current_buffer_size, size = batch_size)\n",
        "        # observation\n",
        "        profile = {k:v[indices] for k,v in self.buffer[\"observation\"][\"user_profile\"].items()}\n",
        "        history = {k:v[indices] for k,v in self.buffer[\"observation\"][\"user_history\"].items()}\n",
        "        observation = {\"user_profile\": profile, \"user_history\": history}\n",
        "        # next observation\n",
        "        profile = {k:v[indices] for k,v in self.buffer[\"next_observation\"][\"user_profile\"].items()}\n",
        "        history = {k:v[indices] for k,v in self.buffer[\"next_observation\"][\"user_history\"].items()}\n",
        "        next_observation = {\"user_profile\": profile, \"user_history\": history}\n",
        "        # policy output\n",
        "        policy_output = {\"state\": self.buffer[\"policy_output\"][\"state\"][indices],\n",
        "                         \"action\": self.buffer[\"policy_output\"][\"action\"][indices],\n",
        "                         \"prob\": self.buffer[\"policy_output\"][\"prob\"][indices]}\n",
        "        # user response\n",
        "        user_response = {\"reward\": self.buffer[\"user_response\"][\"reward\"][indices],\n",
        "                         \"immediate_response\": self.buffer[\"user_response\"][\"immediate_response\"][indices]}\n",
        "        # done mask\n",
        "        done_mask = self.buffer[\"done_mask\"][indices]\n",
        "        return observation, policy_output, user_response, done_mask, next_observation\n",
        "\n",
        "    def update(self, observation, policy_output, user_feedback, next_observation):\n",
        "        '''\n",
        "        @input:\n",
        "        - observation: {'user_profile': {'user_id': (B,),\n",
        "                                         'uf_{feature_name}': (B, feature_dim)},\n",
        "                        'user_history': {'history': (B, max_H),\n",
        "                                         'history_if_{feature_name}': (B, max_H * feature_dim),\n",
        "                                         'history_{response}': (B, max_H),\n",
        "                                         'history_length': (B,)}}\n",
        "        - policy_output: {'user_state': (B, state_dim),\n",
        "                          'prob': (B, action_dim),\n",
        "                          'action': (B, action_dim)}\n",
        "        - user_feedback: {'done': (B,),\n",
        "                          'immdiate_response':, (B, action_dim * feedback_dim),\n",
        "                          'reward': (B,)}\n",
        "        - next_observation: same format as update_buffer@input-observation\n",
        "        '''\n",
        "        # get buffer indices to update\n",
        "        B = len(user_feedback['reward'])\n",
        "        if self.buffer_head + B >= self.buffer_size:\n",
        "            tail = self.buffer_size - self.buffer_head\n",
        "            indices = [self.buffer_head + i for i in range(tail)] + \\\n",
        "                        [i for i in range(B - tail)]\n",
        "        else:\n",
        "            indices = [self.buffer_head + i for i in range(B)]\n",
        "        indices = torch.tensor(indices).to(torch.long).to(self.device)\n",
        "\n",
        "        # update buffer - observation\n",
        "        for k,v in observation['user_profile'].items():\n",
        "            self.buffer['observation']['user_profile'][k][indices] = v\n",
        "        for k,v in observation['user_history'].items():\n",
        "            self.buffer['observation']['user_history'][k][indices] = v\n",
        "        # update buffer - next observation\n",
        "        for k,v in next_observation['user_profile'].items():\n",
        "            self.buffer['next_observation']['user_profile'][k][indices] = v\n",
        "        for k,v in next_observation['user_history'].items():\n",
        "            self.buffer['next_observation']['user_history'][k][indices] = v\n",
        "        # update buffer - policy output\n",
        "        self.buffer['policy_output']['state'][indices] = policy_output['state']\n",
        "        self.buffer['policy_output']['action'][indices] = policy_output['action']\n",
        "        self.buffer['policy_output']['prob'][indices] = policy_output['prob']\n",
        "        # update buffer - user response\n",
        "        self.buffer['user_response']['immediate_response'][indices] = user_feedback['immediate_response'].view(B,-1)\n",
        "        self.buffer['user_response']['reward'][indices] = user_feedback['reward']\n",
        "        # update buffer - done\n",
        "        self.buffer['done_mask'][indices] = user_feedback['done']\n",
        "\n",
        "        # buffer pointer\n",
        "        self.buffer_head = (self.buffer_head + B) % self.buffer_size\n",
        "        self.n_stream_record += B\n",
        "        self.current_buffer_size = min(self.n_stream_record, self.buffer_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoFkCcUKrOXR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class HyperActorBuffer(BaseBuffer):\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - from BaseBuffer:\n",
        "            - buffer_size\n",
        "        '''\n",
        "        parser = BaseBuffer.parse_model_args(parser)\n",
        "        return parser\n",
        "\n",
        "    def reset(self, *reset_args):\n",
        "\n",
        "        '''\n",
        "        @output:\n",
        "        - buffer: {'observation': {'user_profile': {'user_id': (L,),\n",
        "                                                    'uf_{feature_name}': (L, feature_dim)},\n",
        "                                   'user_history': {'history': (L, max_H),\n",
        "                                                    'history_if_{feature_name}': (L, max_H * feature_dim),\n",
        "                                                    'history_{response}': (L, max_H),\n",
        "                                                    'history_length': (L,)}}\n",
        "                   'policy_output': {'state': (L, state_dim),\n",
        "                                     'action': (L, action_dim) = (L, slate_size),\n",
        "                                     'hyper_action': (L, hyper_action_size)},\n",
        "                   'next_observation': same format as @output-buffer['observation'],\n",
        "                   'done_mask': (L,),\n",
        "                   'response': {'reward': (L,),\n",
        "                                'immediate_response':, (L, action_dim * response_dim)}}\n",
        "        '''\n",
        "        env = reset_args[0]\n",
        "        actor = reset_args[1]\n",
        "\n",
        "        super().reset(env, actor)\n",
        "\n",
        "        self.buffer['user_response']['immediate_response'] = torch.zeros(self.buffer_size,\n",
        "                                                                         env.response_dim * actor.effect_action_dim)\\\n",
        "                                                         .to(torch.float).to(self.device)\n",
        "        self.buffer['policy_output']['action'] = torch.zeros(self.buffer_size, actor.hyper_action_dim)\\\n",
        "                                                         .to(torch.float).to(self.device)\n",
        "        self.buffer['policy_output']['effect_action'] = torch.zeros(self.buffer_size, actor.effect_action_dim)\\\n",
        "                                                         .to(torch.long).to(self.device)\n",
        "\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Batch sample is organized as a tuple of (observation, policy_output, user_response, done_mask, next_observation)\n",
        "\n",
        "        Buffer: see reset@output\n",
        "        '''\n",
        "        # get indices\n",
        "        indices = np.random.randint(0, self.current_buffer_size, size = batch_size)\n",
        "        # observation\n",
        "        profile = {k:v[indices] for k,v in self.buffer[\"observation\"][\"user_profile\"].items()}\n",
        "        history = {k:v[indices] for k,v in self.buffer[\"observation\"][\"user_history\"].items()}\n",
        "        observation = {\"user_profile\": profile, \"user_history\": history}\n",
        "        # next observation\n",
        "        profile = {k:v[indices] for k,v in self.buffer[\"next_observation\"][\"user_profile\"].items()}\n",
        "        history = {k:v[indices] for k,v in self.buffer[\"next_observation\"][\"user_history\"].items()}\n",
        "        next_observation = {\"user_profile\": profile, \"user_history\": history}\n",
        "        # policy output\n",
        "        policy_output = {\"state\": self.buffer[\"policy_output\"][\"state\"][indices],\n",
        "                         \"action\": self.buffer[\"policy_output\"][\"action\"][indices],\n",
        "                         \"hyper_action\": self.buffer[\"policy_output\"][\"action\"][indices],\n",
        "                         \"effect_action\": self.buffer[\"policy_output\"][\"effect_action\"][indices]}  # main change to BaseBuffer\n",
        "        # user response\n",
        "        user_response = {\"reward\": self.buffer[\"user_response\"][\"reward\"][indices],\n",
        "                         \"immediate_response\": self.buffer[\"user_response\"][\"immediate_response\"][indices]}\n",
        "        # done mask\n",
        "        done_mask = self.buffer[\"done_mask\"][indices]\n",
        "        return observation, policy_output, user_response, done_mask, next_observation\n",
        "\n",
        "    def update(self, observation, policy_output, user_feedback, next_observation):\n",
        "        '''\n",
        "        @input:\n",
        "        - observation: {'user_profile': {'user_id': (B,),\n",
        "                                         'uf_{feature_name}': (B, feature_dim)},\n",
        "                        'user_history': {'history': (B, max_H),\n",
        "                                         'history_if_{feature_name}': (B, max_H * feature_dim),\n",
        "                                         'history_{response}': (B, max_H),\n",
        "                                         'history_length': (B,)}}\n",
        "        - policy_output: {'user_state': (B, state_dim),\n",
        "                          'prob': (B, action_dim),\n",
        "                          'action': (B, action_dim),\n",
        "                          'hyper_action': (B, hyper_action_dim)}\n",
        "        - user_feedback: {'done': (B,),\n",
        "                          'immdiate_response':, (B, action_dim * feedback_dim),\n",
        "                          'reward': (B,)}\n",
        "        - next_observation: same format as update_buffer@input-observation\n",
        "        '''\n",
        "        # get buffer indices to update\n",
        "        B = len(user_feedback['reward'])\n",
        "        if self.buffer_head + B >= self.buffer_size:\n",
        "            tail = self.buffer_size - self.buffer_head\n",
        "            indices = [self.buffer_head + i for i in range(tail)] + \\\n",
        "                        [i for i in range(B - tail)]\n",
        "        else:\n",
        "            indices = [self.buffer_head + i for i in range(B)]\n",
        "        indices = torch.tensor(indices).to(torch.long).to(self.device)\n",
        "\n",
        "        # update buffer - observation\n",
        "        for k,v in observation['user_profile'].items():\n",
        "            self.buffer['observation']['user_profile'][k][indices] = v\n",
        "        for k,v in observation['user_history'].items():\n",
        "            self.buffer['observation']['user_history'][k][indices] = v\n",
        "        # update buffer - next observation\n",
        "        for k,v in next_observation['user_profile'].items():\n",
        "            self.buffer['next_observation']['user_profile'][k][indices] = v\n",
        "        for k,v in next_observation['user_history'].items():\n",
        "            self.buffer['next_observation']['user_history'][k][indices] = v\n",
        "        # update buffer - policy output\n",
        "        self.buffer['policy_output']['state'][indices] = policy_output['state']\n",
        "        self.buffer['policy_output']['action'][indices] = policy_output['hyper_action']\n",
        "        self.buffer['policy_output']['effect_action'][indices] = policy_output['effect_action'] # main change to BaseBuffer\n",
        "        # update buffer - user response\n",
        "        self.buffer['user_response']['immediate_response'][indices] = user_feedback['immediate_response'].view(B,-1)\n",
        "        self.buffer['user_response']['reward'][indices] = user_feedback['reward']\n",
        "        # update buffer - done\n",
        "        self.buffer['done_mask'][indices] = user_feedback['done']\n",
        "\n",
        "        # buffer pointer\n",
        "        self.buffer_head = (self.buffer_head + B) % self.buffer_size\n",
        "        self.n_stream_record += B\n",
        "        self.current_buffer_size = min(self.n_stream_record, self.buffer_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfvyxpSLq2dP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def get_retention_reward(user_feedback, reward_base = 0.7):\n",
        "    '''\n",
        "    @input:\n",
        "    - user_feedback: {'retention': (B,), ...}\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    reward = - user_feedback['retention']/10.0\n",
        "    return reward\n",
        "\n",
        "def get_immediate_reward(user_feedback):\n",
        "    '''\n",
        "    @input:\n",
        "    - user_feedback: {'immediate_response': (B, slate_size, n_feedback),\n",
        "                      'immediate_response_weight': (n_feedback),\n",
        "                      ... other feedbacks}\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    # (B, slate_size, n_feedback)\n",
        "    if 'immediate_response_weight' in user_feedback:\n",
        "        point_reward = user_feedback['immediate_response'] * user_feedback['immediate_response_weight'].view(1,1,-1)\n",
        "    else:\n",
        "        point_reward = user_feedback['immediate_response']\n",
        "    # (B, slate_size)\n",
        "    combined_reward = torch.sum(point_reward, dim = 2)\n",
        "    # (B,)\n",
        "    #leave_reward = user_feedback['leave'] * user_feedback['leave_weight']\n",
        "    # (B,)\n",
        "    #reward = point_reward.sum(dim = -1) + leave_reward\n",
        "    reward = torch.mean(combined_reward, dim = 1)\n",
        "    return reward\n",
        "\n",
        "def get_immediate_reward_sum(user_feedback):\n",
        "    '''\n",
        "    @input:\n",
        "    - user_feedback: {'immediate_response': (B, slate_size, n_feedback),\n",
        "                      'immediate_response_weight': (n_feedback),\n",
        "                      ... other feedbacks}\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    # (B, slate_size, n_feedback)\n",
        "    if 'immediate_response_weight' in user_feedback:\n",
        "        point_reward = user_feedback['immediate_response'] * user_feedback['immediate_response_weight'].view(1,1,-1)\n",
        "    else:\n",
        "        point_reward = user_feedback['immediate_response']\n",
        "    # (B, slate_size)\n",
        "    combined_reward = torch.sum(point_reward, dim = 2)\n",
        "    # (B,)\n",
        "    #leave_reward = user_feedback['leave'] * user_feedback['leave_weight']\n",
        "    # (B,)\n",
        "    #reward = point_reward.sum(dim = -1) + leave_reward\n",
        "    reward = torch.sum(combined_reward, dim = 1)\n",
        "    return reward\n",
        "\n",
        "\n",
        "def sum_with_cost(feedback, zero_reward_cost = 0.1):\n",
        "    '''\n",
        "    @input:\n",
        "    - feedback: (B, K)\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    B,L = feedback.shape\n",
        "    cost = torch.zeros_like(feedback)\n",
        "    cost[feedback == 0] = -zero_reward_cost\n",
        "    reward = torch.sum(feedback + cost, dim = -1)\n",
        "    return reward\n",
        "\n",
        "\n",
        "def sigmoid_sum_with_cost(feedback, zero_reward_cost = 0.1):\n",
        "    '''\n",
        "    @input:\n",
        "    - feedback: (B, K)\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    reward = sum_with_cost(feedback, zero_reward_cost)\n",
        "    return torch.sigmoid(reward)\n",
        "\n",
        "\n",
        "def log_sum_with_cost(feedback, zero_reward_cost = 0.1):\n",
        "    '''\n",
        "    @input:\n",
        "    - feedback: (B, K)\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    reward = sum_with_cost(feedback, zero_reward_cost)\n",
        "    reward[reward>0] = (reward[reward>0]+1).log()\n",
        "    return torch.sigmoid(reward)\n",
        "\n",
        "def mean_with_cost(feedback_dict, zero_reward_cost = 0.1):\n",
        "    '''\n",
        "    @input:\n",
        "    - feedback: (B, K)\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "\n",
        "    B,L = feedback.shape\n",
        "    cost = torch.zeros_like(feedback)\n",
        "    cost[feedback == 0] = -zero_reward_cost\n",
        "    reward = torch.mean(feedback + cost, dim = -1)\n",
        "    return reward\n",
        "\n",
        "def mean_advance_with_cost(feedback, zero_reward_cost = 0.1, offset = 0.5):\n",
        "    '''\n",
        "    @input:\n",
        "    - feedback: (B, K)\n",
        "    @output:\n",
        "    - reward: (B,)\n",
        "    '''\n",
        "    B,L = feedback.shape\n",
        "    cost = torch.zeros_like(feedback)\n",
        "    cost[feedback == 0] = -zero_reward_cost\n",
        "    reward = torch.mean(feedback + cost, dim = -1) - offset\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lNr7B-iqzGB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class BaseRLAgent():\n",
        "    '''\n",
        "    RL Agent controls the overall learning algorithm:\n",
        "    - objective functions for the policies and critics\n",
        "    - design of reward function\n",
        "    - how many steps to train\n",
        "    - how to do exploration\n",
        "    - loading and saving of models\n",
        "\n",
        "    Main interfaces:\n",
        "    - train\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - gamma\n",
        "        - reward_func\n",
        "        - n_iter\n",
        "        - train_every_n_step\n",
        "        - start_policy_train_at_step\n",
        "        - initial_epsilon\n",
        "        - final_epsilon\n",
        "        - elbow_epsilon\n",
        "        - explore_rate\n",
        "        - do_explore_in_train\n",
        "        - check_episode\n",
        "        - save_episode\n",
        "        - save_path\n",
        "        - actor_lr\n",
        "        - actor_decay\n",
        "        - batch_size\n",
        "        '''\n",
        "        # basic settings\n",
        "        parser.add_argument('--gamma', type=float, default=0.95,\n",
        "                            help='reward discount')\n",
        "        parser.add_argument('--reward_func', type=str, default='get_retention_reward',\n",
        "                            help='reward function name')\n",
        "        parser.add_argument('--n_iter', type=int, nargs='+', default=[2000],\n",
        "                            help='number of training iterations')\n",
        "        parser.add_argument('--train_every_n_step', type=int, default=1,\n",
        "                            help='number of training iterations')\n",
        "        parser.add_argument('--start_policy_train_at_step', type=int, default=1000,\n",
        "                            help='start timestamp for buffer sampling')\n",
        "\n",
        "        # exploration control\n",
        "        parser.add_argument('--initial_epsilon', type=float, default=0.5,\n",
        "                            help='probability for using uniform exploration')\n",
        "        parser.add_argument('--final_epsilon', type=float, default=0.01,\n",
        "                            help='probability for using uniform exploration')\n",
        "        parser.add_argument('--elbow_epsilon', type=float, default=1.0,\n",
        "                            help='probability for using uniform exploration')\n",
        "        parser.add_argument('--explore_rate', type=float, default=1.0,\n",
        "                            help='probability of engaging exploration')\n",
        "        parser.add_argument('--do_explore_in_train', action='store_true',\n",
        "                            help='probability of engaging exploration')\n",
        "\n",
        "        # monitoring\n",
        "        parser.add_argument('--check_episode', type=int, default=100,\n",
        "                            help='number of iterations to check output and evaluate')\n",
        "        parser.add_argument('--save_episode', type=int, default=1000,\n",
        "                            help='number of iterations to save models')\n",
        "        parser.add_argument('--save_path', type=str, required=True,\n",
        "                            help='save path for networks')\n",
        "\n",
        "        # learning\n",
        "        parser.add_argument('--actor_lr', type=float, default=1e-4,\n",
        "                            help='learning rate for actor')\n",
        "        parser.add_argument('--actor_decay', type=float, default=1e-4,\n",
        "                            help='regularization factor for actor learning')\n",
        "        parser.add_argument('--batch_size', type=int, default=64,\n",
        "                            help='training batch size')\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, *input_args):\n",
        "        args, env, actor, buffer = input_args\n",
        "\n",
        "        self.device = args.device\n",
        "\n",
        "        # hyperparameters\n",
        "        self.gamma = args.gamma\n",
        "        self.reward_func = eval(args.reward_func)\n",
        "        self.n_iter = args.n_iter\n",
        "        self.train_every_n_step = args.train_every_n_step\n",
        "        self.start_policy_train_at_step = args.start_policy_train_at_step\n",
        "\n",
        "        self.initial_epsilon = args.initial_epsilon\n",
        "        self.final_epsilon = args.final_epsilon\n",
        "        self.elbow_epsilon = args.elbow_epsilon\n",
        "        self.explore_rate = args.explore_rate\n",
        "        self.do_explore_in_train = args.do_explore_in_train\n",
        "\n",
        "        self.check_episode = args.check_episode\n",
        "        self.save_episode = args.save_episode\n",
        "        self.save_path = args.save_path\n",
        "\n",
        "        self.actor_lr = args.actor_lr\n",
        "        self.actor_decay = args.actor_decay\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        # components\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.buffer = buffer\n",
        "\n",
        "        # controller\n",
        "        self.exploration_scheduler = LinearScheduler(int(sum(args.n_iter) * args.elbow_epsilon),\n",
        "                                                           args.final_epsilon,\n",
        "                                                           initial_p=args.initial_epsilon)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=args.actor_lr,\n",
        "                                                weight_decay=args.actor_decay)\n",
        "\n",
        "        # register modules that will be saved\n",
        "        self.registered_models = [(self.actor, self.actor_optimizer, '_actor')]\n",
        "\n",
        "        if len(self.n_iter) == 1:\n",
        "            with open(self.save_path + \".report\", 'w') as outfile:\n",
        "                outfile.write(f\" \")\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.n_iter) > 2:\n",
        "            self.load()\n",
        "\n",
        "        t = time.time()\n",
        "        print(\"Run procedures before training\")\n",
        "        self.action_before_train()\n",
        "        t = time.time()\n",
        "        start_time = t\n",
        "\n",
        "        # training\n",
        "        print(\"Training:\")\n",
        "        step_offset = sum(self.n_iter[:-1])\n",
        "        do_buffer_update = True\n",
        "        observation = deepcopy(self.env.current_observation)\n",
        "        for i in tqdm(range(step_offset, step_offset + self.n_iter[-1]//10)):\n",
        "            do_explore = np.random.random() < self.explore_rate if self.explore_rate < 1 else True\n",
        "            # online inference\n",
        "            observation = self.run_episode_step(i, self.exploration_scheduler.value(i), observation,\n",
        "                                                do_buffer_update, do_explore)\n",
        "            # online training\n",
        "            if i % self.train_every_n_step == 0:\n",
        "                self.step_train()\n",
        "            # log monitor records\n",
        "            if i > 0 and i % self.check_episode == 0:\n",
        "                t_prime = time.time()\n",
        "                print(f\"Episode step {i}, time diff {t_prime - t}, total time diff {t - start_time})\")\n",
        "                episode_report, train_report = self.get_report(smoothness = self.check_episode)\n",
        "                log_str = f\"step: {i} @ online episode: {episode_report} @ training: {train_report}\\n\"\n",
        "                with open(self.save_path + \".report\", 'a') as outfile:\n",
        "                    outfile.write(log_str)\n",
        "                print(log_str)\n",
        "                t = t_prime\n",
        "\n",
        "            # save model and training info\n",
        "            if i % self.save_episode == 0:\n",
        "                self.save()\n",
        "\n",
        "        self.action_after_train()\n",
        "\n",
        "\n",
        "    def action_before_train(self):\n",
        "        '''\n",
        "        Action before training:\n",
        "        - env.reset()\n",
        "        - buffer.reset()\n",
        "        - set up training monitors\n",
        "            - training_history\n",
        "            - eval_history\n",
        "        - run several episodes of random actions to build-up the initial buffer\n",
        "        '''\n",
        "\n",
        "        observation = self.env.reset()\n",
        "        self.buffer.reset(self.env, self.actor)\n",
        "\n",
        "        # training monitors\n",
        "        self.setup_monitors()\n",
        "\n",
        "        episode_iter = 0 # zero training iteration\n",
        "        pre_epsilon = 1.0 # uniform random explore before training\n",
        "        do_buffer_update = True\n",
        "        prepare_step = 0\n",
        "\n",
        "        for i in tqdm(range(self.start_policy_train_at_step)):\n",
        "            do_explore = np.random.random() < self.explore_rate\n",
        "            observation = self.run_episode_step(episode_iter, pre_epsilon, observation,\n",
        "                                                do_buffer_update, do_explore)\n",
        "            prepare_step += 1\n",
        "        print(f\"Total {prepare_step} prepare steps\")\n",
        "\n",
        "    def setup_monitors(self):\n",
        "        self.training_history = {'actor_loss': []}\n",
        "        self.eval_history = {'avg_reward': [],\n",
        "                             'reward_variance': [],\n",
        "                             'avg_total_reward': [0.],\n",
        "                             'max_total_reward': [0.],\n",
        "                             'min_total_reward': [0.]}\n",
        "        self.eval_history.update({f'{resp}_rate': [] for resp in self.env.response_types})\n",
        "        self.current_sum_reward = torch.zeros(self.env.episode_batch_size).to(torch.float).to(self.device)\n",
        "\n",
        "\n",
        "    def action_after_train(self):\n",
        "        self.env.stop()\n",
        "\n",
        "    def get_report(self, smoothness = 10):\n",
        "        episode_report = self.env.get_report(smoothness)\n",
        "        train_report = {k: np.mean(v[-smoothness:]) for k,v in self.training_history.items()}\n",
        "        train_report.update({k: np.mean(v[-smoothness:]) for k,v in self.eval_history.items()})\n",
        "        return episode_report, train_report\n",
        "\n",
        "    def run_episode_step(self, *episode_args):\n",
        "        '''\n",
        "        Run one step of user-env interaction\n",
        "        @input:\n",
        "        - episode_args: (episode_iter, epsilon, observation, do_buffer_update, do_explore)\n",
        "        @process:\n",
        "        - apply_policy: observation, candidate items --> policy_output\n",
        "        - env.step(): policy_output['action'] --> user_feedback, updated_observation\n",
        "        - reward_func(): user_feedback --> reward\n",
        "        - buffer.update(observation, policy_output, user_feedback, updated_observation)\n",
        "        @output:\n",
        "        - next_observation\n",
        "        '''\n",
        "        episode_iter, epsilon, observation, do_buffer_update, do_explore = episode_args\n",
        "        self.epsilon = epsilon\n",
        "        is_train = False\n",
        "        with torch.no_grad():\n",
        "            # generate action from policy\n",
        "            policy_output = self.apply_policy(observation, self.actor, epsilon, do_explore, is_train)\n",
        "\n",
        "            # apply action on environment\n",
        "            # Note: action must be indices on env.candidate_iids\n",
        "            action_dict = {'action': policy_output['indices']}\n",
        "            new_observation, user_feedback, update_info = self.env.step(action_dict)\n",
        "\n",
        "            # calculate reward\n",
        "            R = self.get_reward(user_feedback)\n",
        "            user_feedback['reward'] = R\n",
        "            self.current_sum_reward = self.current_sum_reward + R\n",
        "            done_mask = user_feedback['done']\n",
        "            if torch.sum(done_mask) > 0:\n",
        "                self.eval_history['avg_total_reward'].append(self.current_sum_reward[done_mask].mean().item())\n",
        "                self.eval_history['max_total_reward'].append(self.current_sum_reward[done_mask].max().item())\n",
        "                self.eval_history['min_total_reward'].append(self.current_sum_reward[done_mask].min().item())\n",
        "                self.current_sum_reward[done_mask] = 0\n",
        "\n",
        "            # monitor update\n",
        "            self.eval_history['avg_reward'].append(R.mean().item())\n",
        "            self.eval_history['reward_variance'].append(torch.var(R).item())\n",
        "\n",
        "            for i,resp in enumerate(self.env.response_types):\n",
        "                self.eval_history[f'{resp}_rate'].append(user_feedback['immediate_response'][:,:,i].mean().item())\n",
        "            # update replay buffer\n",
        "            if do_buffer_update:\n",
        "                self.buffer.update(observation, policy_output, user_feedback, update_info['updated_observation'])\n",
        "        return new_observation\n",
        "\n",
        "    def apply_policy(self, observation, actor, *input_args):\n",
        "        '''\n",
        "        @input:\n",
        "        - observation:{'user_profile':{\n",
        "                           'user_id': (B,)\n",
        "                           'uf_{feature_name}': (B,feature_dim), the user features}\n",
        "                       'user_history':{\n",
        "                           'history': (B,max_H)\n",
        "                           'history_if_{feature_name}': (B,max_H,feature_dim), the history item features}\n",
        "        - actor: the actor model\n",
        "        - epsilon: scalar\n",
        "        - do_explore: boolean\n",
        "        - is_train: boolean\n",
        "        @output:\n",
        "        - policy_output\n",
        "        '''\n",
        "        epsilon = policy_args[0]\n",
        "        do_explore = policy_args[1]\n",
        "        is_train = policy_args[2]\n",
        "        input_dict = {'observation': observation,\n",
        "                      'candidates': self.env.get_candidate_info(observation),\n",
        "                      'epsilon': epsilon,\n",
        "                      'do_explore': do_explore,\n",
        "                      'is_train': is_train,\n",
        "                      'batch_wise': False}\n",
        "        out_dict = self.actor(input_dict)\n",
        "        return out_dict\n",
        "\n",
        "    def get_reward(self, user_feedback):\n",
        "        user_feedback['immediate_response_weight'] = self.env.response_weights\n",
        "        R = self.reward_func(user_feedback).detach()\n",
        "        return R\n",
        "\n",
        "    def step_train(self):\n",
        "        '''\n",
        "        @process:\n",
        "        '''\n",
        "        observation, policy_output, user_feedback, done_mask, next_observation = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        loss_dict = self.get_loss(observation, policy_output, user_feedback, done_mask, next_observation)\n",
        "\n",
        "        for k in loss_dict:\n",
        "            if k in self.training_history:\n",
        "                try:\n",
        "                    self.training_history[k].append(loss_dict[k].item())\n",
        "                except:\n",
        "                    self.training_history[k].append(loss_dict[k])\n",
        "\n",
        "    def get_loss(self, observation, policy_output, user_feedback, done_mask, next_observation):\n",
        "        pass\n",
        "\n",
        "    def test(self):\n",
        "        pass\n",
        "\n",
        "    def save(self):\n",
        "        for model, opt, prefix in self.registered_models:\n",
        "            torch.save(model.state_dict(), self.save_path + prefix)\n",
        "            torch.save(opt.state_dict(), self.save_path + prefix + \"_optimizer\")\n",
        "\n",
        "    def load(self):\n",
        "        for model, opt, prefix in self.registered_models:\n",
        "            model.load_state_dict(torch.load(self.save_path + prefix, map_location = self.device))\n",
        "            opt.load_state_dict(torch.load(self.save_path + prefix + \"_optimizer\", map_location = self.device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBdeIM4JqmpA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TD3(BaseRLAgent):\n",
        "    @staticmethod\n",
        "    def parse_model_args(parser):\n",
        "        '''\n",
        "        args:\n",
        "        - args from DDPG:\n",
        "            - episode_batch_size\n",
        "            - batch_size\n",
        "            - actor_lr\n",
        "            - critic_lr\n",
        "            - actor_decay\n",
        "            - critic_decay\n",
        "            - target_mitigate_coef\n",
        "            - args from BaseRLAgent:\n",
        "                - gamma\n",
        "                - n_iter\n",
        "                - train_every_n_step\n",
        "                - initial_greedy_epsilon\n",
        "                - final_greedy_epsilon\n",
        "                - elbow_greedy\n",
        "                - check_episode\n",
        "                - with_eval\n",
        "                - save_path\n",
        "        '''\n",
        "        parser = BaseRLAgent.parse_model_args(parser)\n",
        "        # parser.add_argument('--episode_batch_size', type=int, default=8,\n",
        "        #                     help='episode sample batch size')\n",
        "        # parser.add_argument('--batch_size', type=int, default=32,\n",
        "        #                     help='training batch size')\n",
        "        # parser.add_argument('--actor_lr', type=float, default=1e-4,\n",
        "        #                     help='learning rate for actor')\n",
        "        parser.add_argument('--critic_lr', type=float, default=1e-4,\n",
        "                            help='decay rate for critic')\n",
        "        # parser.add_argument('--actor_decay', type=float, default=1e-4,\n",
        "        #                     help='learning rate for actor')\n",
        "        parser.add_argument('--critic_decay', type=float, default=1e-4,\n",
        "                            help='decay rate for critic')\n",
        "        parser.add_argument('--target_mitigate_coef', type=float, default=0.01,\n",
        "                            help='mitigation factor')\n",
        "\n",
        "        return parser\n",
        "\n",
        "\n",
        "    def __init__(self, *input_args):\n",
        "        '''\n",
        "        self.gamma\n",
        "        self.n_iter\n",
        "        self.check_episode\n",
        "        self.with_eval\n",
        "        self.save_path\n",
        "        self.facade\n",
        "        self.exploration_scheduler\n",
        "        '''\n",
        "        args, env, actor, critic, buffer = input_args\n",
        "        super().__init__(args, env, actor, buffer)\n",
        "        self.episode_batch_size = args.episode_batch_size\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.actor_lr = args.actor_lr\n",
        "        self.critic_lr = args.critic_lr\n",
        "        self.actor_decay = args.actor_decay\n",
        "        self.critic_decay = args.critic_decay\n",
        "\n",
        "        # self.actor = facade.actor\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=args.actor_lr,\n",
        "                                                weight_decay=args.actor_decay)\n",
        "        self.critic = critic\n",
        "        self.critic1 = self.critic[0]\n",
        "        self.critic1_target = copy.deepcopy(self.critic1)\n",
        "        self.critic1_optimizer = torch.optim.Adam(self.critic1.parameters(), lr=args.critic_lr,\n",
        "                                                 weight_decay=args.critic_decay)\n",
        "\n",
        "        self.critic2 = self.critic[1]\n",
        "        self.critic2_target = copy.deepcopy(self.critic2)\n",
        "        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=args.critic_lr,\n",
        "                                                 weight_decay=args.critic_decay)\n",
        "\n",
        "        self.tau = args.target_mitigate_coef\n",
        "        if len(self.n_iter) == 1:\n",
        "            with open(self.save_path + \".report\", 'w') as outfile:\n",
        "                outfile.write(f\"{args}\\n\")\n",
        "\n",
        "    def action_before_train(self):\n",
        "        '''\n",
        "        Action before training:\n",
        "        - facade setup:\n",
        "            - buffer setup\n",
        "        - run random episodes to build-up the initial buffer\n",
        "        '''\n",
        "        super().action_before_train()\n",
        "\n",
        "        # training records\n",
        "        self.training_history = {'actor_loss': [], 'critic1_loss': [], 'critic2_loss': [],\n",
        "                                 'Q': [], 'next_Q': []}\n",
        "        # print(f\"Total {prepare_step} prepare steps\")\n",
        "\n",
        "\n",
        "\n",
        "    def step_train(self):\n",
        "        observation, policy_output, user_feedback, done_mask, next_observation = self.buffer.sample(self.batch_size)\n",
        "        reward = user_feedback['reward'].view(-1)\n",
        "        # reward = reward.clone().detach().to(torch.float)\n",
        "        # done_mask = done_mask.clone().detach().to(torch.float)\n",
        "\n",
        "        critic_loss, actor_loss = self.get_td3_loss(observation, policy_output, reward, done_mask, next_observation)\n",
        "        self.training_history['actor_loss'].append(actor_loss.item())\n",
        "        self.training_history['critic1_loss'].append(critic_loss[0])\n",
        "        self.training_history['critic2_loss'].append(critic_loss[1])\n",
        "\n",
        "        # Update the frozen target models\n",
        "        for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        return {\"step_loss\": (self.training_history['actor_loss'][-1],\n",
        "                              self.training_history['critic1_loss'][-1],\n",
        "                              self.training_history['critic2_loss'][-1])}\n",
        "\n",
        "\n",
        "    def get_td3_loss(self, observation, policy_output, reward, done_mask, next_observation,\n",
        "                     do_actor_update = True, do_critic_update = True):\n",
        "        '''\n",
        "        @input:\n",
        "        - observation: {'user_profile': {'user_id': (B,),\n",
        "                                         'uf_{feature_name}': (B, feature_dim)},\n",
        "                        'user_history': {'history': (B, max_H),\n",
        "                                         'history_if_{feature_name}': (B, max_H, feature_dim),\n",
        "                                         'history_{response}': (B, max_H),\n",
        "                                         'history_length': (B, )}}\n",
        "        - policy_output: {'state': (B, state_dim),\n",
        "                          'action: (B, action_dim)}\n",
        "        - reward: (B,)\n",
        "        - done_mask: (B,)\n",
        "        - next_observation: the same format as @input-observation\n",
        "        '''\n",
        "\n",
        "        # Compute the target Q value\n",
        "        next_policy_output = self.apply_policy(next_observation, self.actor_target, self.epsilon, do_explore = True)\n",
        "        target_critic1_output = self.apply_critic(next_observation, next_policy_output, self.critic1_target)\n",
        "        target_critic2_output = self.apply_critic(next_observation, next_policy_output, self.critic2_target)\n",
        "        target_Q = torch.min(target_critic1_output['q'], target_critic2_output['q'])\n",
        "        # r+gamma*Q' when done; r+Q when not done\n",
        "        # target_Q = reward + ((self.gamma * done_mask) + (1 - done_mask)) * target_Q.detach()\n",
        "        target_Q = reward + ((self.gamma * done_mask) + torch.logical_not(done_mask)) * target_Q.detach()\n",
        "\n",
        "        critic_loss_list = []\n",
        "        if do_critic_update and self.critic_lr > 0:\n",
        "            for critic, optimizer in [(self.critic1, self.critic1_optimizer),\n",
        "                                           (self.critic2, self.critic2_optimizer)]:\n",
        "                # Get current Q estimate\n",
        "                current_critic_output = self.apply_critic(observation,\n",
        "                                                                 wrap_batch(policy_output, device = self.device),\n",
        "                                                                 critic)\n",
        "                current_Q = current_critic_output['q']\n",
        "                # Compute critic loss\n",
        "                critic_loss = F.mse_loss(current_Q, target_Q).mean()\n",
        "                critic_loss_list.append(critic_loss.item())\n",
        "\n",
        "                # Optimize the critic\n",
        "                optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Compute actor loss\n",
        "        policy_output = self.apply_policy(observation, self.actor)\n",
        "        critic_output = self.apply_critic(observation, policy_output, self.critic1)\n",
        "        actor_loss = -critic_output['q'].mean()\n",
        "\n",
        "        if do_actor_update and self.actor_lr > 0:\n",
        "            # Optimize the actor\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "        return critic_loss_list, actor_loss\n",
        "\n",
        "\n",
        "    def apply_policy(self, observation, policy_model, epsilon = 0,\n",
        "                     do_explore = False, do_softmax = True):\n",
        "        '''\n",
        "        @input:\n",
        "        - observation: input of policy model\n",
        "        - policy_model\n",
        "        - epsilon: greedy epsilon, effective only when do_explore == True\n",
        "        - do_explore: exploration flag, True if adding noise to action\n",
        "        - do_softmax: output softmax score\n",
        "        '''\n",
        "#         feed_dict = utils.wrap_batch(observation, device = self.device)\n",
        "        feed_dict = observation\n",
        "        # out_dict = policy_model(feed_dict)\n",
        "        is_train = True\n",
        "        input_dict = {'observation': observation,\n",
        "                'candidates': self.env.get_candidate_info(observation),\n",
        "                'epsilon': epsilon,\n",
        "                'do_explore': do_explore,\n",
        "                'is_train': is_train,\n",
        "                'batch_wise': False}\n",
        "        out_dict = policy_model(input_dict)\n",
        "\n",
        "        return out_dict\n",
        "\n",
        "    def apply_critic(self, observation, policy_output, critic_model):\n",
        "        # feed_dict = {\"state_emb\": policy_output[\"state_emb\"],\n",
        "        #              \"action_emb\": policy_output[\"action_emb\"]}\n",
        "        feed_dict = {'state': policy_output['state'],\n",
        "                'action': policy_output['hyper_action']}\n",
        "        critic_output = critic_model(feed_dict)\n",
        "        return critic_output\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.critic1.state_dict(), self.save_path + \"_critic1\")\n",
        "        torch.save(self.critic1_optimizer.state_dict(), self.save_path + \"_critic1_optimizer\")\n",
        "\n",
        "        torch.save(self.critic2.state_dict(), self.save_path + \"_critic2\")\n",
        "        torch.save(self.critic2_optimizer.state_dict(), self.save_path + \"_critic2_optimizer\")\n",
        "\n",
        "        torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n",
        "        torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        self.critic1.load_state_dict(torch.load(self.save_path + \"_critic1\", map_location=self.device))\n",
        "        self.critic1_optimizer.load_state_dict(torch.load(self.save_path + \"_critic1_optimizer\", map_location=self.device))\n",
        "        self.critic1_target = copy.deepcopy(self.critic1)\n",
        "\n",
        "        self.critic2.load_state_dict(torch.load(self.save_path + \"_critic2\", map_location=self.device))\n",
        "        self.critic2_optimizer.load_state_dict(torch.load(self.save_path + \"_critic2_optimizer\", map_location=self.device))\n",
        "        self.critic2_target = copy.deepcopy(self.critic2)\n",
        "\n",
        "        self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n",
        "        self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n",
        "        self.actor_target = copy.deepcopy(self.actor)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
